
問題2-1
未回答
Your machine learning project involves identifying fraudulent transactions in a dataset. Which Spark ML algorithm is suitable for anomaly detection tasks, where the goal is to identify rare and abnormal instances?
Linear Regression
Decision Trees
Naive Bayes
正解
Isolation Forest
全体的な説明
Correct Answer:
Isolation Forest
Explanation:
Isolation Forest is a Spark ML algorithm specifically designed for anomaly detection. It excels at identifying rare or abnormal instances (e.g., fraudulent transactions) by isolating anomalies in the data rather than profiling normal behavior.
Key Features of Isolation Forest:
✅ Anomaly Detection:
Works well for imbalanced datasets (few fraud cases vs. many normal transactions).
Uses random partitioning to isolate anomalies faster than normal points.
✅ Scalability:
Efficient for high-dimensional data (common in fraud detection).
✅ Spark ML Integration:
Available in pyspark.ml.feature.IsolationForest.
Example Code:
from pyspark.ml.feature import IsolationForest
# Initialize Isolation Forest
isolation_forest = IsolationForest(
    numTrees=100,  # Number of trees
    contamination=0.01,  # Estimated fraction of anomalies
    featuresCol="features",
    predictionCol="prediction"
)
# Train and predict
model = isolation_forest.fit(train_data)
predictions = model.transform(test_data)
# Anomalies are marked as 1 (normal: 0)
predictions.filter(predictions.prediction == 1).show()
Why Other Options Are Incorrect:
Linear Regression:
Predicts continuous values, not anomalies.
Decision Trees:
Used for classification/regression, not optimized for anomaly detection.
Naive Bayes:
A probabilistic classifier, ineffective for imbalanced anomaly detection.
Key Takeaway:
For fraud detection (rare events), use Isolation Forest due to its:
Efficiency in isolating anomalies.
Compatibility with Spark’s distributed computing.
ドメイン
Spark ML Algorithms

問題2-2
未回答
What method can be used to view the notebook that executed an MLflow run?
Open the model.pkl artifact on the MLflow run page
Click the "Models" link corresponding to the run on the MLflow experiment page
Open the MLmodel artifact on the MLflow run page
Click the "Start Time" link corresponding to the run on the MLflow experiment page
正解
Click the "Source" link in the row corresponding to the run in the MLflow experiment
page
全体的な説明
Correct Answer:
Click the "Source" link in the row corresponding to the run in the MLflow experiment page
Explanation:
To view the notebook (or script) that executed an MLflow run:
Navigate to the MLflow Experiment page in Databricks.
Locate the specific run in the runs table.
Click the "Source" link in the run's row. This redirects you to the notebook/job that initiated the run.
Why This Works:
The "Source" field in MLflow tracks the origin of the run (e.g., notebook path, job ID).
Clicking it opens the exact notebook version used, including commit history (if linked to Git).
Why Other Options Are Incorrect:
Open the model.pkl artifact:
This file stores the serialized model, not the notebook.
Click the "Models" link:
Redirects to the Model Registry, not the source code.
Open the MLmodel artifact:
Contains model metadata (e.g., flavors, dependencies), not the notebook.
Click the "Start Time" link:
Shows run duration/timestamps, not the source.
Key Takeaway:
The "Source" link is the direct way to access the notebook/script behind an MLflow run. This is critical for:
✅ Reproducibility: Verify the exact code used.
✅ Debugging: Trace errors to their origin.
Example Workflow:
Train model → Log to MLflow.
Check "Source" link to revisit the notebook.
ドメイン
Databricks ML

問題2-3
未回答
A machine learning engineer uses the following code block to scale the inference of a single-node model on a Spark DataFrame with one million records:
 @pandas_udf("double")
 def predict(iterator: Iterator[pd.DataFrame])-> Iterator[pd.Series]:
 model_path = f"runs://(run.info.run_id)/model"
 model = mlflow.sklearn.load_model(model_path)
 for features in iterator:
 pdf = pd.concat(features, axis=1)
 yield pd.Series(model.predict(pdf))
Assuming the default Spark configuration is in place, what is the advantage of using an Iterator?
Choose only ONE best answer.
The model will be restricted to a single executor, preventing the data from being distributed.
The data will be restricted to a single executor, preventing the model from being loaded multiple times.
The data will be distributed across multiple executors during the inference process.
There's no benefit to including an Iterator as the input or output
正解
The model only needs to be loaded once per executor rather than once per batch during the inference process.
全体的な説明
Correct Answer:
✅ The model only needs to be loaded once per executor rather than once per batch during the inference process.
Explanation:
Using an Iterator in a Pandas UDF (@pandas_udf) improves efficiency by ensuring the model is loaded only once per executor instead of once per batch of data.
Why is this important?
Without an Iterator, the model would reload for every batch of data processed by Spark, leading to high computational overhead.
With an Iterator, the model loads once per executor, and the function processes multiple batches without reloading the model.
How the Iterator Optimizes Performance
from typing import Iterator
import pandas as pd
import mlflow
from pyspark.sql.functions import pandas_udf
@pandas_udf("double")
def predict(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:
    # Load the model once per executor
    model_path = f"runs://(run.info.run_id)/model"
    model = mlflow.sklearn.load_model(model_path)
    # Iterate through incoming data batches
    for features in iterator:
        pdf = pd.concat(features, axis=1)
        yield pd.Series(model.predict(pdf))
Iterator[pd.DataFrame] processes multiple batches per executor, rather than calling the function separately for each batch.
mlflow.sklearn.load_model(model_path) loads the model only once per executor, improving efficiency.
Fewer I/O and computation costs compared to loading the model for every mini-batch.
Why Other Options Are Incorrect?
"The model will be restricted to a single executor, preventing the data from being distributed."
Incorrect, because data remains distributed across multiple Spark executors.
The Iterator optimizes model loading per executor, but does not affect data distribution.
"The data will be restricted to a single executor, preventing the model from being loaded multiple times."
Incorrect, because data is still processed across multiple executors.
The Iterator only affects how model loading is handled, not data partitioning.
"The data will be distributed across multiple executors during the inference process."
Partially correct, but this happens by default in Spark.
The Iterator does not change data distribution; it optimizes model loading per executor.
"There's no benefit to including an Iterator as the input or output."
Incorrect, because the Iterator reduces unnecessary model reloading, which significantly improves performance.
Without an Iterator, the model would reload for every batch, slowing down inference.
Final Answer:
✅ Using an Iterator ensures the model is only loaded once per executor rather than once per batch, improving inference efficiency in Spark ML
ドメイン
Spark ML

問題2-4
未回答
You are working with a time series dataset and want to specify the column that represents time for forecasting. Which parameter should you use?
正解
time_col
target_col
max_trials
exclude_cols
全体的な説明
Correct Answer:
time_col
Explanation:
When working with time series forecasting in Databricks AutoML or other ML frameworks, the time_col parameter is used to specify the column in your dataset that represents the time component (e.g., dates or timestamps). This ensures the model correctly interprets the temporal structure of the data.
Example (Databricks AutoML):
from databricks import automl
# Specify the time column for forecasting
summary = automl.forecast(
    dataset=df,
    time_col="date",  # Column with timestamps
    target_col="sales",  # Column to predict
    horizon=7  # Forecast 7 periods ahead
)
Why Other Options Are Incorrect:
target_col:
Specifies the column to predict (e.g., sales), not the time column.
max_trials:
Controls the number of hyperparameter tuning trials, unrelated to time series.
exclude_cols:
Lists columns to ignore during training, not to identify time.
Key Takeaway:
For time series forecasting:
✅ Use time_col to define the timestamp column.
✅ Use target_col for the variable to predict.
Note: This parameter is critical for:
Splitting data into training/validation sets chronologically.
Generating future time points for predictions (horizon).
ドメイン
AutoML

問題2-5
未回答
A data scientist is using Databricks for a time-series forecasting project. The dataset contains irregular time intervals, and they want to handle missing time points effectively.
What Databricks feature or library would be useful in this scenario?
MLlib CrossValidator.
Databricks Delta.
正解
Databricks Time Series Library.
MLflow Tracking.
全体的な説明
The best option for handling missing time points effectively in time-series forecasting with irregular intervals is:
C. Databricks Time Series Library.
Here's why:
Databricks Time Series Library is specifically designed for time-series analysis and offers functionalities to handle missing data. It can:
Impute missing values using various techniques like linear interpolation, forward filling, or statistical methods.
Handle gaps and irregular intervals efficiently, providing accurate forecasts even with non-uniform data.
It integrates seamlessly with Spark DataFrames and other Databricks features, allowing easy processing and manipulation of time-series data.
The library also includes functionalities for forecasting, anomaly detection, and trend analysis, making it a comprehensive solution for the data scientist's project.
Let's analyze the other options:
MLlib CrossValidator: While useful for evaluating model performance, it doesn't directly address missing data issues.
Databricks Delta: This is a storage format for data lakes, primarily focusing on reliability and versioning. It can't handle missing values in the way the Time Series Library does.
MLflow Tracking: This is primarily for logging and managing ML experiments, not specifically for data manipulation or missing value handling.
Therefore, Databricks Time Series Library is the most relevant and effective option for the data scientist's time-series forecasting project with irregular intervals and missing data.
Here's a diagram to illustrate why Databricks Time Series Library is the best choice for handling missing time points in time-series forecasting with irregular intervals:
Databricks Time Series Library for Time Series Forecasting
As you can see, the library offers a comprehensive set of tools specifically designed for time-series analysis, including:
Data ingestion and transformation: Handles various data formats, including CSV, JSON, and parquet. Can clean and pre-process data for analysis.
Feature engineering: Creates new features from existing data, such as lags, rolling averages, and seasonality indicators.
Missing value handling: Provides multiple imputation techniques like linear interpolation, forward filling, and statistical methods like ARIMA or ETS.
Modeling and forecasting: Supports various time-series forecasting models, including ARIMA, Prophet, and LSTMs.
Evaluation and visualization: Offers metrics and visualizations to assess model performance and analyze results.
Databricks Time Series Library seamlessly integrates with Spark DataFrames, allowing you to leverage the power of distributed processing for large datasets. This makes it a powerful and efficient solution for handling complex time-series problems like yours.
ドメイン
Spark ML Algorithms

問題2-6
未回答
How can you install the databricks-feature-engineering client in Databricks Runtime?
pip install databricks-feature-engineering
正解
%pip install databricks-feature-engineering
conda install databricks-feature-engineering
spark install databricks-feature-engineering
全体的な説明
Correct Answer:
%pip install databricks-feature-engineering
Explanation:
To install the databricks-feature-engineering client in Databricks Runtime, use the %pip magic command in a notebook cell. This ensures the package is installed in the cluster’s Python environment and is available for all users attached to the cluster.
Steps:
Open a Databricks notebook.
Run:
%pip install databricks-feature-engineering
Restart the Python kernel if prompted (or run dbutils.library.restartPython()).
Why This Works:
%pip: The recommended way to install Python packages in Databricks notebooks.
Cluster-wide availability: Installs the package for the current cluster session.
Why Other Options Are Incorrect:
pip install databricks-feature-engineering:
Works in local Python environments but not in Databricks notebooks (use %pip instead).
conda install databricks-feature-engineering:
Conda is not the default package manager in Databricks Runtime.
spark install databricks-feature-engineering:
Invalid command (Spark does not handle package installations).
Key Takeaway:
For installing Python packages in Databricks Runtime, always use:
✅ %pip install <package> in a notebook cell.
Note: For production, consider installing via:
Cluster init scripts (for persistent installations).
Libraries UI (under Cluster settings).
ドメイン
Feature Store

問題2-7
未回答
Which option in Pandas API on Spark controls the visual limit on top-n-based plots, such as plot.bar and plot.pie?
正解
plotting.max_rows
compute.default_index_type
compute.shortcut_limit
compute.ops_on_diff_frames
全体的な説明
The answer is A. plotting.max_rows.
Explanation:
Purpose: The plotting.max_rows option controls the maximum number of rows that will be visually displayed in top-n-based plots generated by Pandas API on Spark, such as:
plot.bar()
plot.pie()
Other plots that inherently select a subset of rows for visualization
Effect: It helps manage the visual complexity of plots, especially when dealing with large datasets, by limiting the number of bars, slices, or other visual elements.
Incorrect Options:
B. compute.default_index_type: This option controls the default index type for new DataFrames, not plotting behavior.
C. compute.shortcut_limit: This option controls the maximum number of rows that can be collected to the driver for certain operations, but it doesn't directly affect plotting.
D. compute.ops_on_diff_frames: This option determines how operations on DataFrames with different indexes are handled, but it's not related to plotting.
Example:
import pandas as pd
# Set the maximum visual limit for top-n plots
pd.set_option("plotting.max_rows", 20)  # Show a maximum of 20 rows in plots
# Create a DataFrame
df = pd.DataFrame({"A": [1, 2, 3, 4, 5], "B": [5, 4, 3, 2, 1]})
# Plot a bar chart (will show bars for top 20 rows)
df.plot.bar()
Key Points:
Adjusting plotting.max_rows can be helpful for:
Making plots less cluttered and easier to interpret.
Improving performance for large datasets by limiting data processed for plotting.
You can check the current value using pd.get_option("plotting.max_rows").
ドメイン
Pandas API on Spark

問題2-8
未回答
Which in-memory columnar data format is used by Pandas API on Spark to efficiently transfer data between JVM and Python processes?
Parquet
ORC
Avro
正解
Apache Arrow
全体的な説明
Correct Answer:
Apache Arrow
Explanation:
The Pandas API on Spark (Koalas) uses Apache Arrow as its in-memory columnar data format to efficiently transfer data between JVM (Spark) and Python (pandas) processes. Here’s why:
Zero-Copy Data Sharing:
Arrow eliminates serialization/deserialization overhead by allowing direct memory access between JVM and Python.
Columnar Format:
Optimized for analytical workloads (like Spark) with vectorized operations.
Seamless Integration:
Pandas DataFrames ↔ Spark DataFrames conversions are accelerated via Arrow.
Example Workflow:
import databricks.koalas as ks
# Spark DataFrame → Koalas (uses Arrow under the hood)
kdf = spark_df.to_koalas()
# Koalas → Pandas (Arrow-backed)
pdf = kdf.to_pandas()
Why Other Options Are Incorrect:
Parquet/ORC:
Disk-based columnar formats (used for storage, not in-memory transfer).
Avro:
Row-based serialization format, not optimized for JVM-Python interoperability.
Key Takeaway:
Apache Arrow is the backbone for:
✅ High-speed data transfer between Spark and pandas.
✅ Zero-copy operations in Pandas API on Spark.
ドメイン
Spark ML

問題2-9
未回答
A data scientist employs MLflow for tracking their machine learning experiment. As part of each MLflow run, they conduct hyperparameter tuning. The scientist wishes to organize one parent run for the tuning procedure and have a child run for each unique combination of hyperparameter values. They manually initiate all parent and child runs using 'mlflow.start_run()'.
Which methodology should the data scientist adopt to achieve this MLflow run organization?
Choose only ONE best answer.
They could initiate each child run with the identical experiment ID as the parent run.
正解
They could specify 'nested=True' when initiating the child run for each unique combination of hyperparameter values.
They could begin each child run inside the indented code block of the parent run using 'mlflow.start_run()'.
They could enable Databricks Autologging.
They could specify 'nested=True' when initiating the parent run for the tuning process.
全体的な説明
Correct Answer:
They could specify nested=True when initiating the child run for each unique combination of hyperparameter values.
Explanation:
To organize MLflow runs into a parent-child hierarchy (e.g., one parent run for hyperparameter tuning and child runs for each hyperparameter combination), use the nested=True flag when starting child runs. This ensures child runs are nested under the parent run, maintaining a clear structure in the MLflow UI.
Steps to Implement:
Start the parent run:
with mlflow.start_run() as parent_run:
    # Parent run logic (e.g., tuning setup)
Start child runs inside the parent context:
for params in hyperparameter_combinations:
    with mlflow.start_run(nested=True) as child_run:  # Critical: nested=True
        # Log hyperparameters and metrics for each child run
        mlflow.log_params(params)
Why This Works:
nested=True: Explicitly declares the child run as nested under the active parent run.
UI Organization: Child runs appear grouped under the parent in the MLflow Experiments page.
Why Other Options Are Incorrect:
Same experiment ID for all runs:
Does not create a hierarchy; runs are flat siblings.
Indented code block without nested=True:
Runs are siblings unless nested=True is specified.
Databricks Autologging:
Automates logging but does not control run hierarchy.
nested=True on the parent run:
nested=True is only valid for child runs (parent runs cannot be nested).
Key Takeaway:
For parent-child run organization:
✅ Use with mlflow.start_run(nested=True) for child runs.
✅ Parent runs must be active when child runs are created.
Example MLflow UI View:
Parent Run (Tuning)
├── Child Run (Params 1)
├── Child Run (Params 2)
└── Child Run (Params 3)
ドメイン
Databricks ML

問題2-10
未回答
How does SparkTrials distribute tuning tasks in Hyperopt, and what is the role of the driver node and worker nodes?
SparkTrials generates trials on worker nodes and evaluates them on the driver node
正解
Each trial is generated with a Spark job on the driver node and evaluated on worker nodes
SparkTrials uses a centralized approach, where all trials are evaluated on the driver node
Trials are generated and evaluated on worker nodes independently
全体的な説明
The answer is b) Each trial is generated with a Spark job on the driver node and evaluated on worker nodes.
Here's a breakdown of how SparkTrials distributes tuning tasks in Hyperopt:
1. Driver Node:
Trial Generation:
The driver node, acting as the central coordinator, iteratively generates new hyperparameter configurations (trials) using Hyperopt's search algorithms.
For each trial, it creates a Spark job with a single task.
Job Distribution:
The driver node distributes these Spark jobs to available worker nodes for execution.
2. Worker Nodes:
Trial Evaluation:
Each worker node receives a Spark job corresponding to a trial.
It executes the task within the job, which involves:
Loading necessary data and model code.
Fitting the model using the specified hyperparameter configuration.
Evaluating the model's performance on a validation set.
Result Return:
The worker node sends the evaluation results (loss value, hyperparameters, etc.) back to the driver node.
Key Points:
Centralized Trial Generation: The driver node is responsible for generating new trials, ensuring coordination and consistency in the search process.
Distributed Evaluation: The computationally intensive task of evaluating trials is parallelized across worker nodes, leveraging the cluster's computational resources.
Asynchronous Execution: Trials are evaluated asynchronously, allowing multiple trials to run simultaneously on different worker nodes, accelerating the tuning process.
Communication and Feedback: The driver node collects and aggregates results from worker nodes, utilizing this information to guide the generation of subsequent trials and steer the search towards promising hyperparameter regions.
Incorrect Options:
Option a) is incorrect because trial generation happens on the driver node, not worker nodes.
Option c) is incorrect because evaluation is distributed to worker nodes, not centralized on the driver node.
Option d) is incorrect because trial generation is centralized on the driver node, not independent on worker nodes.
ドメイン
Hyperopt & Sparktail

問題2-11
未回答
A health organization is developing a classification model to determine whether or not a patient currently has a specific type of infection. The organization's leaders want to maximize the number of cases identified by the model.
Which of the following classification metrics should be used to evaluate themodel?
Choose only ONE best answer.
Accuracy
Precision
正解
Recall
RMSE
Area under the ROC curve.
全体的な説明
Correct Answer:
Recall
Explanation:
For the health organization's goal of maximizing the number of cases identified (i.e., capturing as many true infections as possible), Recall (also called Sensitivity or True Positive Rate) is the most appropriate metric.
Why Recall?
Recall measures the proportion of actual positives (infected patients) correctly identified by the model:
High Recall = Few false negatives (missed infections).
Critical for healthcare: Missing an infection (false negative) is far worse than a false alarm (false positive).
Example:
If 100 patients are infected and the model identifies:
Recall = 90% → Correctly flags 90 infected patients (misses 10).
Recall = 50% → Misses 50 infected patients (unacceptable for health risks).
Why Other Options Are Incorrect:
Accuracy:
Misleading for imbalanced datasets (e.g., if 95% of patients are healthy, a "always healthy" model achieves 95% accuracy but catches 0 infections).
Precision:
Measures how many flagged cases are correct (focuses on reducing false positives).
Not ideal here because the priority is catching all infections, even if it means some false alarms.
RMSE:
Used for regression, not classification.
Area under the ROC curve (AUC-ROC):
Evaluates overall model performance across thresholds but doesn’t directly optimize for catching all positives.
Key Takeaway:
For health-critical classification (e.g., infections, cancer detection):
✅ Maximize Recall to minimize missed cases.
⚠️ Trade-off: Lower precision (more false alarms) may be acceptable.
Practical Tip:
Use precision-recall curves (not ROC) for imbalanced datasets.
Set a low decision threshold to boost recall (e.g., classify as "infected" even with 30% probability).
ドメイン
ML Workflows

問題2-12
未回答
A data scientist is working on a collaborative project in Databricks where multiple notebooks are being used. They want to ensure that changes made by one team member do not overwrite the changes made by another team member in a shared notebook.
What Databricks feature or strategy should they use?
Enable "Auto-Save" in the notebook settings.
正解
Implement version control using Git within the Databricks workspace.
Share notebook access using a common username and password.
Use the "Lock" feature to prevent simultaneous editing by multiple users.
全体的な説明
Correct Answer:
Implement version control using Git within the Databricks workspace.
Explanation:
To prevent conflicts and ensure collaborative work without overwriting changes, the team should integrate Git version control with their Databricks notebooks. This allows:
Branching: Each team member works on their own branch.
Merge Requests: Changes are reviewed before merging into the main branch.
History Tracking: Roll back to previous versions if needed.
Steps to Set Up Git in Databricks:
Connect to Git:
Go to User Settings > Git Integration and link your GitHub/GitLab/Bitbucket account.
Clone the Repository:
Use the Databricks UI to clone the repo into your workspace.
Commit and Push Changes:
Regularly commit changes with descriptive messages and push to the remote repo.
Why This Works:
No Overwrites: Changes are isolated in branches until merged.
Audit Trail: Full history of who changed what and when.
Why Other Options Are Incorrect:
"Auto-Save":
Automatically saves edits but does not prevent conflicts if multiple users edit the same notebook.
Shared Username/Password:
Highly insecure and offers no conflict resolution.
"Lock" Feature:
Databricks does not have a built-in lock feature for notebooks.
Key Takeaway:
For collaborative notebook editing:
✅ Use Git for version control (branching, merging, history).
✅ Avoid direct editing of shared notebooks without review.
Example Workflow:
User A creates a feature branch → edits notebook → commits/pushes.
User B reviews changes → approves merge request.

問題2-13
未回答
What should be considered when using SparkTrials on GPU-enabled clusters, and how should parallelism be configured?
GPU clusters use maximum parallelism, so no specific configuration is needed
GPU clusters use multiple executor threads per node for optimal parallelism
正解
Configure parallelism based on the number of GPU-enabled instances, avoiding conflicts among Spark tasks
GPU clusters automatically adjust parallelism based on SparkTrials configuration
全体的な説明
The answer is c) Configure parallelism based on the number of GPU-enabled instances, avoiding conflicts among Spark tasks.
Key considerations for using SparkTrials on GPU-enabled clusters:
Executor Thread Configuration:
CPU vs. GPU:
CPU clusters typically use multiple executor threads per worker node to maximize CPU utilization.
GPU clusters, however, often use only one executor thread per node to prevent conflicts among Spark tasks trying to access the same GPU. This is generally optimal for libraries written for GPUs.
Parallelism Adjustment:
Reduced Maximum Parallelism: The single-executor-thread-per-node configuration on GPU clusters means the maximum achievable parallelism is lower compared to CPU clusters.
Manual Configuration: You need to manually configure parallelism to match the number of GPU-enabled instances in your cluster to effectively utilize resources and avoid conflicts.
How to configure parallelism:
Consider the Number of GPUs: Determine the total number of GPU-enabled instances in your cluster.
Adjust Spark Configuration: Set the following Spark configuration options to match the number of GPUs:
spark.executor.instances: Number of executors (ideally, one per GPU-enabled node).
spark.executor.cores: Number of cores per executor (usually 1 on GPU clusters).
spark.default.parallelism: Number of parallel tasks (set according to the number of GPUs).
Example:
# Assuming a cluster with 4 GPU-enabled nodes
spark = SparkSession.builder.appName("my_app").config(
    spark.executor.instances=4,
    spark.executor.cores=1,
    spark.default.parallelism=4
).getOrCreate()
Additional Considerations:
Instance Types: Select GPU instance types that align with your workload's requirements (e.g., memory, compute power).
GPU Compatibility: Ensure your libraries and frameworks are compatible with GPU acceleration.
Resource Monitoring: Monitor GPU utilization to identify potential bottlenecks and adjust configurations as needed.
ドメイン
Hyperopt & Sparktail

問題2-14
未回答
A team of data scientists is working on a collaborative project, and they want to track and compare the performance of different machine learning experiments.
Which MLflow component can assist in achieving this?
正解
MLflow Tracking Server.
MLlib CrossValidator.
MLflow REST API.
Databricks Jobs.
全体的な説明
Correct Answer:
MLflow Tracking Server
Explanation:
The MLflow Tracking Server is the core component designed to track, log, and compare machine learning experiments. It allows teams to:
Log Parameters, Metrics, and Artifacts:
Record hyperparameters, evaluation metrics (e.g., accuracy, RMSE), and output files (e.g., models, plots).
Compare Experiments:
View results across runs in a unified UI, including parallel coordinates plots and metric trends.
Collaborate:
Shared access to experiment results for all team members.
Example Workflow:
import mlflow
# Start an experiment
mlflow.set_experiment("fraud_detection")
# Log parameters and metrics
with mlflow.start_run():
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_metric("accuracy", 0.95)
    # Log the model
    mlflow.sklearn.log_model(model, "model")
Key Features:
✅ Centralized UI: Compare runs visually.
✅ Reproducibility: Capture all dependencies and code versions.
Why Other Options Are Incorrect:
MLlib CrossValidator:
Used for hyperparameter tuning in Spark ML, not experiment tracking.
MLflow REST API:
Allows programmatic access to the Tracking Server but is not the primary tool for comparison.
Databricks Jobs:
Schedules and runs workflows but does not track or compare experiments.
Key Takeaway:
For collaborative ML experiment tracking, use the MLflow Tracking Server to:
Log and visualize runs.
Compare models holistically.
Share results across the team.
ドメイン
Spark ML Basics

問題2-15
未回答
In Databricks, which of the following components is used to transform a column of scalar values into a column of vector type, as required by an estimator's .fit() method?
Choose only ONE best answer.
VectorScaler
VectorConverter
正解
VectorAssembler
VectorTransformer
全体的な説明
Correct Answer:
VectorAssembler
Explanation:
In Databricks (and Apache Spark MLlib), the VectorAssembler is the component used to transform scalar columns (e.g., numerical features) into a single vector column, which is required by most ML estimators (e.g., LinearRegression, RandomForestClassifier) for the .fit() method.
How It Works:
Input: Multiple scalar columns (e.g., age, income, score).
Output: A single vector column (e.g., features), where each vector combines the scalar values for a row.
Example Code:
from pyspark.ml.feature import VectorAssembler
# Sample DataFrame with scalar columns
df = spark.createDataFrame([
    (25, 50000, 3.5),
    (30, 80000, 4.2)
], ["age", "income", "score"])
# Combine columns into a vector
assembler = VectorAssembler(
    inputCols=["age", "income", "score"],
    outputCol="features"
)
# Transform the DataFrame
df_vector = assembler.transform(df)
df_vector.show()
Output:
+---+------+-----+------------------+
|age|income|score|          features|
+---+------+-----+------------------+
| 25| 50000|  3.5|[25.0,50000.0,3.5]|
| 30| 80000|  4.2|[30.0,80000.0,4.2]|
+---+------+-----+------------------+
Why Other Options Are Incorrect:
VectorScaler:
Scales vector columns (e.g., normalization) but does not create vectors from scalars.
VectorConverter:
Does not exist in Spark MLlib.
VectorTransformer:
A generic term, not a specific Spark class.
Key Takeaway:
Use VectorAssembler to:
✅ Convert scalar columns into a vector format for ML models.
✅ Prepare data for estimators like .fit().
ドメイン
Spark ML

問題2-16
未回答
What is the potential downside of using Pandas API on Spark instead of PySpark?
Limited support for distributed computing
Inefficient data structure
正解
Increased computation time due to internal frame conversion
Limited functionality compared to PySpark
全体的な説明
Correct Answer:
Increased computation time due to internal frame conversion
Explanation:
The primary downside of using Pandas API on Spark (Koalas) instead of PySpark is the overhead of internal frame conversion between Spark and pandas formats. Here’s why:
Conversion Overhead:
Pandas API on Spark operates by translating pandas-like syntax to Spark operations under the hood.
Each operation may require converting between Spark DataFrames (distributed) and pandas DataFrames (single-node), adding latency.
Performance Impact:
For small-to-medium datasets, the convenience of pandas syntax outweighs the cost.
For large datasets, frequent conversions can slow down computations compared to native PySpark.
Example:
import databricks.koalas as ks
# Pandas API on Spark (hidden conversions)
kdf = ks.DataFrame(...)
result = kdf.groupby("col1").mean()  # Converts to Spark, then back to pandas-like format
Why Other Options Are Less Relevant:
Limited support for distributed computing:
Incorrect. Pandas API on Spark fully leverages Spark’s distributed computing.
Inefficient data structure:
Partially true, but the main issue is conversion, not the structure itself.
Limited functionality compared to PySpark:
While some PySpark features may be missing, the API covers most common use cases.
Key Takeaway:
Use Pandas API on Spark for:
✅ Pandas-like syntax on big data.
⚠️ Trade-off: Accept conversion overhead for usability.
For maximal performance, use native PySpark when:
Working with very large datasets.
Needing low-level control over Spark optimizations.
ドメイン
Spark ML

問題2-17
未回答
A machine learning engineer is working to upgrade a machine learning project in a way that enables automatic model refresh every time the project runs. The project is linked to an existing model referred to as model_name in the MLflow Model Registry.
The following block of code is part of their approach:
mlflow.sklearn.log_model
(
sk_model=model,
artifact_path="model",
registered_model_name=model_name
)
Given that model_name already exists in the MLflow Model Registry, what does the parameter and argument registered_model_name=model_name denote?
It eliminates the requirement of specifying the model name in the subsequent
obligatory call to mlflow.register_model.
It records a new model titled model_name in the MLflow Model Registry.
It represents the name of the logged model in the MLflow Experiment.
正解
It registers a new version of the model_name model in the MLflow Model Registry.
It denotes the name of the Run in the MLflow Experiment
全体的な説明
Correct Answer:
It registers a new version of the model_name model in the MLflow Model Registry.
Explanation:
When registered_model_name=model_name is specified in mlflow.sklearn.log_model() and the model (model_name) already exists in the MLflow Model Registry:
A new version of the model is automatically created under the existing model_name.
The model is logged as an artifact in the current MLflow run.
No additional call to mlflow.register_model() is needed.
Key Behavior:
✅ Versioning: Each run creates a new version (e.g., Version 2, Version 3).
✅ No Duplicates: Avoids creating duplicate model entries; uses the existing model_name.
Example Workflow:
import mlflow
# Log and register a new version under "model_name"
mlflow.sklearn.log_model(
    sk_model=model,
    artifact_path="model",
    registered_model_name="model_name"  # Links to existing model
)
Result in MLflow UI:
model_name → Version 2 (if Version 1 already exists).
Why Other Options Are Incorrect:
"Eliminates the requirement of calling mlflow.register_model":
True, but this is a side effect, not the primary purpose of registered_model_name.
"Records a new model titled model_name":
Only true if model_name doesn’t exist. Here, it adds a version to the existing model.
"Name of the logged model in the Experiment":
The experiment tracks runs, but registered_model_name ties to the Model Registry.
"Name of the Run":
The run name is set separately (e.g., mlflow.start_run(run_name="...")).
Key Takeaway:
Use registered_model_name to:
✅ Auto-register models under existing names in the Model Registry.
✅ Enable versioning without manual steps.
Note: Omit registered_model_name if you only want to log the model (not register it).
ドメイン
ML Workflows

問題2-18
未回答
What is the purpose of Pandas API on Spark?
To replace PySpark in data analysis tasks
To provide scalable data structures for Python
正解
To extend the functionality of pandas to big data
To introduce a new Python package for Apache Spark
全体的な説明
Correct Answer:
To extend the functionality of pandas to big data
Explanation:
The Pandas API on Spark (also known as Koalas) is designed to bridge the gap between pandas and PySpark, allowing users to:
Use pandas-like syntax on large datasets that require distributed computing with Spark.
Scale pandas workflows to big data without rewriting code.
Key Features:
✅ Familiar Interface:
Methods like df.groupby(), df.pivot(), and df.plot() work identically to pandas.
✅ Distributed Execution:
Under the hood, operations are parallelized across a Spark cluster.
✅ Seamless Integration:
Converts between pandas and Spark DataFrames (e.g., to_pandas(), to_spark()).
Example:
import databricks.koalas as ks
# Create a Koalas DataFrame (scales to big data)
kdf = ks.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
# Use pandas-like operations
result = kdf.groupby("A").sum()  # Executed on Spark
Why Other Options Are Incorrect:
"Replace PySpark":
Koalas complements PySpark but doesn’t replace it. PySpark is still needed for low-level control.
"Scalable data structures for Python":
Partially true, but the primary goal is pandas compatibility, not introducing new structures.
"New Python package for Spark":
Koalas builds on Spark but focuses on pandas users, not replacing Spark’s native API.
Key Takeaway:
Use Pandas API on Spark to:
✅ Leverage pandas syntax on big data.
✅ Avoid rewriting code when scaling from single-node to distributed.
Note: For advanced Spark optimizations, switch to native PySpark.
ドメイン
Pandas API on Spark

問題2-19
未回答
A data scientist is carrying out hyperparameter optimization using an iterative optimization algorithm. Each assessment of unique hyperparameter values is being trained on a distinct compute node. They are conducting eight evaluations in total on eight compute nodes. Although the accuracy of the model varies across the eight evaluations, they observe that there's no consistent pattern of enhancement in the accuracy.What modifications could the data scientist make to enhance their model's accuracy throughout the tuning process?
Choose only ONE best answer.
Adjust the count of compute nodes to be half or fewer than half of the number of
evaluations.
正解
Switch the iterative optimization algorithm used to aid the tuning process.
Adjust the count of compute nodes to be double or more than double the number of
evaluations.
Alter both the number of compute nodes and evaluations to be considerably smaller.
Adjust both the number of compute nodes and evaluations to be substantially larger.
全体的な説明
Correct Answer:
Switch the iterative optimization algorithm used to aid the tuning process.
Explanation:
The core issue here is that the iterative optimization algorithm (e.g., random search, grid search) is not effectively guiding the hyperparameter search toward better accuracy. The lack of consistent improvement suggests the current algorithm is not learning from past evaluations. Here’s how switching algorithms helps:
Why Switch Algorithms?
Current Problem:
The algorithm evaluates hyperparameters independently (no shared knowledge between nodes).
Example: Random search may miss optimal regions.
Better Alternatives:
Bayesian Optimization (e.g., Hyperopt):
Uses past evaluations to focus on promising hyperparameters.
Balances exploration (trying new combinations) and exploitation (refining good ones).
TPE (Tree-structured Parzen Estimator):
Models the probability of good hyperparameters based on prior results.
Example with Hyperopt:
from hyperopt import fmin, tpe, hp, SparkTrials
# Define search space and objective
space = {
    'learning_rate': hp.loguniform('lr', -5, 0),
    'max_depth': hp.choice('depth', range(1, 10))
}
def objective(params):
    # Train/evaluate model
    return -accuracy  # Hyperopt minimizes, so negate accuracy
# Use TPE for smarter search
best = fmin(objective, space, algo=tpe.suggest, max_evals=8, trials=SparkTrials())
Why Other Options Are Incorrect:
Adjusting compute nodes (fewer/more):
Doesn’t address the algorithm’s inability to learn from evaluations.
More nodes just parallelize inefficient searches.
Smaller/larger node/evaluation counts:
May affect speed but won’t improve search efficiency.
Key Takeaway:
To enhance accuracy during hyperparameter tuning:
✅ Switch to a smarter algorithm (e.g., Bayesian Optimization, TPE).
✅ Keep compute nodes as-is (8 nodes for 8 evaluations is fine).
Note: For large-scale tuning, combine better algorithms with SparkTrials for parallelism.
ドメイン
ML Workflows

問題2-20
未回答
A machine learning engineer is optimizing a Spark job in Databricks that involves a large dataset with many features. They want to ensure efficient handling of data and minimize memory usage during transformations.
What advanced optimization technique should they consider?
Use the cache method to persist intermediate DataFrames in memory.
正解
Utilize the broadcast hint for smaller DataFrames in join operations.
Increase the size of the Spark executor memory.
Enable automatic schema inference in the Spark cluster settings.
全体的な説明
Correct Answer:
Utilize the broadcast hint for smaller DataFrames in join operations.
Explanation:
To optimize a Spark job with large datasets and minimize memory usage, the broadcast hint is a powerful technique for efficient joins when one DataFrame is small enough to fit in memory. Here’s why:
How It Works:
broadcast forces Spark to send the smaller DataFrame to all worker nodes, avoiding costly shuffles of the larger DataFrame.
Example:
from pyspark.sql.functions import broadcast
# Efficient join with broadcast
large_df.join(broadcast(small_df), "join_key")
Benefits:
Reduces Shuffle Overhead: Eliminates network traffic for the smaller DataFrame.
Saves Memory: Workers load the small DataFrame once (per node) instead of redistributing large data.
When to Use:
The smaller DataFrame should fit entirely in executor memory (typically <100MB, but depends on cluster config).
Why Other Options Are Less Optimal:
cache():
Persisting intermediate DataFrames helps but doesn’t address join inefficiencies.
Risk of OOM if too much data is cached.
Increase Executor Memory:
Band-Aid solution; doesn’t fix inefficient operations like unoptimized joins.
Automatic Schema Inference:
Reduces manual schema definition but doesn’t impact memory/performance directly.
Key Takeaway:
For memory-efficient Spark jobs:
✅ Broadcast small DataFrames in joins.
✅ Combine with:
Partitioning large DataFrames.
Using cache() selectively for reused DataFrames.
Example Workflow:
# Optimal join with broadcast
result = large_df.join(broadcast(small_lookup_df), "id")
# Avoid (causes shuffle)
result = large_df.join(small_lookup_df, "id")  # No broadcast hint
ドメイン
Databricks ML

問題2-21
未回答
A machine learning engineer is aiming to execute batch model prediction. The engineer intends to leverage a decision tree model stored at the path model_uri to generate predictions for the DataFrame batch_df, which has the schema:
 order_id STRING
 To perform prediction on batch_df using the decision tree model at model_uri, the
 following code block is executed:
 predictions = fs.score_batch(
 model_uri,
 batch_df
 )
Under what circumstances will the engineer's code block successfully perform the
desired prediction?
Choose only ONE best answer.
This code block will not achieve the desired prediction in any situation.
When the model at model_uri uses only order_id as a feature.
When the Feature Store feature set was registered along with the model at model_uri.
When all of the features utilized by the model at model_uri are housed within one Feature Store table.
正解
When all of the features utilized by the model at model_uri are present in a Spark DataFrame in the PySpark session
全体的な説明
Answer: E
The fs.score_batch() method provided by Databricks is utilized for scoring a
DataFrame in batch mode with a designated model. This method assumes that the
DataFrame provided to it carries all the necessary features required for the model
to generate predictions. Here, the DataFrame is batch_df, containing just one
feature, order_id. Consequently, the code block will carry out the desired
prediction only when all the features utilized by the model at model_uri are
available in a Spark DataFrame within the PySpark session.
Option A is incorrect as this option suggests that the code will never work, but it's
possible for the code to work under certain conditions, so it's not the best choice.
Option B is incorrect as this option assumes that the model only uses order_id as a
feature, but it might need additional features for accurate predictions. Therefore,
it's too restrictive.
Option C is incorrect as using batch score implies the model is generated using the
feature set available in feature store which is automatically registered.
Option D is incorrect as This option might work if all the necessary features are in
one Feature Store table, but it's not necessary. Features can be sourced from
different tables or even external sources
ドメイン
Feature Store

問題2-22
未回答
Your team is working on a machine learning project that involves processing geospatial data in a distributed computing environment. What technique allows efficient indexing and retrieval of geospatial data for analysis?
Spatial Partitioning
Geospatial Clustering
正解
Spatial Indexing
Geospatial Replication
全体的な説明
Correct Answer:
Spatial Indexing
Explanation:
Spatial indexing is the technique that enables efficient indexing and retrieval of geospatial data in distributed computing environments (e.g., Apache Spark). It organizes spatial data (points, polygons, etc.) into structured indexes, allowing rapid querying and analysis.
Key Features of Spatial Indexing:
Optimized Queries:
Accelerates operations like range searches ("find all points within 10km") or nearest-neighbor lookups.
Distributed-Friendly:
Indexes (e.g., R-trees, QuadTrees, GeoHash) can be partitioned across a cluster.
Supported in Spark:
Libraries like Sedona (formerly GeoSpark) implement spatial indexing for PySpark.
Example (Using Sedona in Databricks):
from sedona.core.SpatialRDD import PointRDD
from sedona.core.enums import IndexType
# Create a spatial RDD from points
spatial_rdd = PointRDD(sc, "path/to/points.csv")
# Build an R-tree index
spatial_rdd.buildIndex(IndexType.RTREE, True)  # True = store index in memory
# Efficient spatial query
result = spatial_rdd.rangeQuery(query_polygon)
Why Other Options Are Incorrect:
Spatial Partitioning:
Divides data geographically but doesn’t enable fast lookup (indexing does).
Geospatial Clustering:
Groups similar data points (e.g., DBSCAN) but doesn’t optimize retrieval.
Geospatial Replication:
Copies data for redundancy, unrelated to indexing/querying.
Key Takeaway:
For efficient geospatial analysis in distributed systems:
✅ Use spatial indexing (R-trees, QuadTrees, GeoHash).
✅ Leverage libraries like Sedona or GeoPandas on Spark.
ドメイン
Scaling ML Models

問題2-23
未回答
Do you need to manage runs explicitly in the objective function when logging from workers with SparkTrials?
Yes, it is necessary to manage runs explicitly
正解
No, SparkTrials handles run management automatically
Only when using MLlib or Horovod in the objective function
It depends on the complexity of the objective function
全体的な説明
Correct Answer:
No, SparkTrials handles run management automatically
Explanation:
When using SparkTrials in Hyperopt for distributed hyperparameter tuning, run management is fully automated. Here’s why:
Automatic Logging:
SparkTrials automatically creates MLflow runs for each trial (hyperparameter evaluation) on worker nodes.
No need to manually call mlflow.start_run() in the objective function.
Worker-Level Tracking:
Each trial’s metrics, parameters, and artifacts are logged under a nested run linked to the parent Hyperopt run.
Example:
from hyperopt import fmin, tpe, SparkTrials
def objective(params):
    # No manual MLflow run needed!
    accuracy = train_and_evaluate(params)
    return {"loss": -accuracy, "status": "ok"}  # Auto-logged by SparkTrials
spark_trials = SparkTrials()
best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=spark_trials)
Centralized UI:
All nested runs appear under the parent run in the MLflow UI for easy comparison.
Why Other Options Are Incorrect:
"Necessary to manage runs explicitly":
Overcomplicates the process; SparkTrials handles it.
"Only for MLlib/Horovod":
Irrelevant. SparkTrials’ automation works for any objective function.
"Depends on complexity":
False. Automation is consistent regardless of complexity.
Key Takeaway:
With SparkTrials:
✅ No manual run management in the objective function.
✅ All metrics/artifacts are auto-logged.
Note: For custom logging (e.g., extra artifacts), use mlflow.log_* inside the objective function without start_run().
ドメイン
Hyperopt & Sparktail

問題2-24
未回答
What is the main difference between DataFrame.transform() and DataFrame.apply() in pandas-on-Spark?
正解
transform requires the function to return the same length as the input, while apply allows an arbitrary length.
apply requires the function to return the same length as the input, while transform allows an arbitrary length.
Both transform and apply require the function to return the same length as the input.
Both transform and apply allow an arbitrary length for the output.
全体的な説明
The answer is A. transform requires the function to return the same length as the input, while apply allows an arbitrary length.
Here's a more detailed explanation:
Key Differences:
transform():
Applies a function element-wise to a pandas-on-Spark DataFrame.
Requires the function to return a Series or DataFrame with the same length as the input.
Preserves the index and column labels of the original DataFrame.
apply():
Applies a function along a specified axis (rows or columns) of a pandas-on-Spark DataFrame.
Allows the function to return a value of arbitrary length, including a scalar, Series, or DataFrame.
Might not preserve the index or column labels, depending on the function's output.
When to Use Each:
transform(): Use when you want to apply a function that directly transforms each element or row into a corresponding output of the same length, maintaining the DataFrame's structure.
apply(): Use when you need more flexibility in the output length or want to perform operations that might involve aggregations, reductions, or manipulations that change the DataFrame's shape or structure.
Example:
import pandas as pd
# Sample DataFrame
df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
# transform() example (same length output)
def double_values(x):
    return x * 2
df_transformed = df.transform(double_values)  # Output: DataFrame with columns A and B doubled
# apply() example (arbitrary length output)
def sum_row(row):
    return row.sum()
df_applied = df.apply(sum_row, axis=1)  # Output: Series with the sum of each row
Key Points:
Choose transform() for element-wise operations that maintain DataFrame structure.
Choose apply() for more flexible operations that might change DataFrame shape.
Be mindful of the expected output length when selecting the appropriate method.
ドメイン
Pandas API on Spark

問題2-25
未回答
A data engineering team is tasked with optimizing the performance of Spark jobs in a Databricks environment. They want to reduce data shuffling during transformations.
What should the team consider to achieve this optimization?
Increase the number of partitions in the DataFrame.
正解
Use the repartition method to control the number of partitions.
Decrease the number of worker nodes in the Spark cluster.
Enable automatic optimization in the Databricks cluster settings.
全体的な説明
Correct Answer:
Use the repartition method to control the number of partitions.
Explanation:
To reduce data shuffling (a costly operation in Spark), the team should strategically repartition DataFrames based on:
Partition Size:
Aim for ~100-200MB per partition (avoid too many small or few large partitions).
Use df.repartition(n) to evenly distribute data.
Partition Keys:
Repartition by a join/group-by key to minimize shuffles:
df.repartition("customer_id")  # Minimizes shuffles for joins on `customer_id`
Why This Works:
Co-locates related data on the same workers, reducing cross-node transfers.
Balances parallelism (avoid skewed partitions).
Why Other Options Are Incorrect:
Increase partitions blindly:
May cause more small tasks, increasing overhead.
Decrease worker nodes:
Reduces parallelism but doesn’t address shuffle inefficiency.
Automatic optimization:
Helpful (e.g., Delta Lake auto-compaction) but doesn’t replace manual partitioning.
Key Takeaway:
To minimize shuffling:
✅ Repartition by join/group-by keys.
✅ Monitor partition sizes (df.rdd.getNumPartitions() + df.rdd.glom().map(len).collect()).
Example:
# Optimize before join to avoid shuffle
df1 = df1.repartition("join_key")
df2 = df2.repartition("join_key")
result = df1.join(df2, "join_key")  # No shuffle!

問題2-26
未回答
A data scientist is trying to use Spark ML to fill in missing values in their PySpark DataFrame 'features_df'. They want to replace the missing values in all numeric columns in 'features_df' with the median value of each corresponding numeric column.
However, the code they have written does not perform the task correctly. Can you identify the reason why the code is not performing the imputation task as intended?
my_imputer = imputer
(strategy = "median",
inputCols = input_columns,
outputCols = output_columns
)
imputed_df = my_imputer.transform(features_df)
Imputing using a median value is not possible.
It does not simultaneously impute both the training and test datasets.
The 'inputCols' and 'outputCols' need to match exactly.
正解
The code fails to fit the imputer to the data to create an 'ImputerModel'.
The 'fit' method needs to be invoked instead of 'transform'.
全体的な説明
Correct Answer:
The code fails to fit the imputer to the data to create an 'ImputerModel'.
Explanation:
The code is incorrect because it skips the fit() step, which is required to:
Calculate the median for each specified column.
Create an ImputerModel that stores these medians for transformation.
Corrected Code:
from pyspark.ml.feature import Imputer
# Define input/output columns (assuming they're pre-defined)
input_columns = ["col1", "col2"]  # Numeric columns with missing values
output_columns = ["col1_imputed", "col2_imputed"]  # Output column names
# Initialize Imputer
my_imputer = Imputer(
    strategy="median",
    inputCols=input_columns,
    outputCols=output_columns
)
# Fit to compute medians and create ImputerModel
imputer_model = my_imputer.fit(features_df)  # Critical step!
# Transform to impute missing values
imputed_df = imputer_model.transform(features_df)
Why Other Options Are Incorrect:
"Imputing using a median value is not possible":
Incorrect. Imputer supports "median", "mean", and "mode".
"Does not impute training/test sets simultaneously":
Irrelevant. The issue is the missing fit() step, not dataset splitting.
"inputCols/outputCols need to match exactly":
They don’t need to match; outputCols are the names for imputed columns.
"fit() needs to be invoked instead of transform":
Misleading. Both are needed (fit() first, then transform()).
Key Takeaway:
For Spark ML imputation:
✅ fit() computes statistics (e.g., median) and creates a model.
✅ transform() applies the imputation using the fitted model.
Note: Always check inputCols are numeric (e.g., DoubleType, IntegerType).
ドメイン
ML Workflows

問題2-27
未回答
A data scientist is transitioning their pandas DataFrame code to make use of the
pandas API on Spark. They're working with the following incomplete code:
________BLANK_________
df = ps.read_parquet(path)
df["category"].value_counts()
Which line of code should they use
to complete the refactoring successfully with the
pandas API on Spark?
import pandas as ps
import databricks.pandas as ps
正解
import pyspark.pandas as ps
import pandas.spark as ps
import databricks.pyspark as ps
全体的な説明
Correct Answer:
import pyspark.pandas as ps
Explanation:
To use the pandas API on Spark (formerly Koalas), the correct import statement is:
import pyspark.pandas as ps  # Official package name
This alias (ps) allows you to use pandas-like syntax while leveraging Spark’s distributed computing.
Why This Works:
ps.read_parquet():
Reads data into a pandas-on-Spark DataFrame (not a pandas or PySpark DataFrame).
df["category"].value_counts():
Executes a distributed count operation under the hood.
Example:
import pyspark.pandas as ps
# Read data (distributed)
df = ps.read_parquet("/path/to/data.parquet")
# Pandas-like operation (runs on Spark)
counts = df["category"].value_counts()
Why Other Options Are Incorrect:
import pandas as ps:
Uses vanilla pandas (single-node), not Spark.
import databricks.pandas as ps:
Incorrect package name (no such module).
import pandas.spark as ps:
Doesn’t exist; pandas has no native Spark integration.
import databricks.pyspark as ps:
Incorrect; pyspark is the native Spark API, not pandas-on-Spark.
Key Takeaway:
For pandas API on Spark:
✅ Use import pyspark.pandas as ps.
✅ Operations mimic pandas but scale to big data.
Note: Ensure the package is installed (%pip install pyspark-pandas).
ドメイン
Pandas API on Spark

問題2-28
未回答
Scenario: While configuring an AutoML run for classification, you want to control the duration of the run. Which parameter should you use?
max_trials
正解
timeout_minutes
exclude_cols
pos_label
全体的な説明
Correct Answer:
timeout_minutes
Explanation:
To control the duration of an AutoML run in Databricks, use the timeout_minutes parameter. This sets a time limit (in minutes) for the entire AutoML experiment, after which the run stops even if not all trials are completed.
Example:
from databricks import automl
# Set a 60-minute timeout for the AutoML run
summary = automl.classify(
    dataset=df,
    target_col="label",
    timeout_minutes=60,  # Stops after 1 hour
    exclude_cols=["id"]  # Optional: columns to ignore
)
Why Other Options Are Incorrect:
max_trials:
Limits the number of trials (hyperparameter combinations), not the runtime.
exclude_cols:
Specifies columns to exclude from training, unrelated to time limits.
pos_label:
Defines the positive class label for metrics (e.g., precision/recall), not runtime.
Key Takeaway:
For time-bound AutoML runs:
✅ Use timeout_minutes to cap duration.
✅ Combine with max_trials to balance exploration vs. time.
Note: Default timeout is 120 minutes (2 hours) in Databricks AutoML.
ドメイン
AutoML

問題2-29
未回答
A data scientist aims to one-hot encode the categorical attributes in theirPySpark DataFrame, named 'features_df', by leveraging Spark ML. The list of string column names has been assigned to the 'input_columns' variable.
They have prepared a block of code for this operation, but it's returning an error. What modification does the data scientist need to make in their code to achieve their goal?
 oneHotEnc = OneHotEncoder(
 InputCols = input_columns,
 outputCols = output_columns
 )
 oneHotEnc_model = oneHotEnc.fit(features_df)
 oneHotEnc_features_df = oneHotEnc_model.transform (features_df)
Choose only ONE best answer.
The columns need to be returned with the same name(s) as those in the 'input_columns'.
The 'method' parameter needs to be specified in the OneHotEncoder.
正解
StringIndexer needs to be utilized before executing the one-hot encoding of the features.
The line containing the 'fit' operation needs to be removed
全体的な説明
Correct Answer:
StringIndexer needs to be utilized before executing the one-hot encoding of the features.
Explanation:
In Spark ML, categorical string columns must first be converted to numerical indices using StringIndexer before applying OneHotEncoder. Here’s why:
OneHotEncoder Requirement:
Spark’s OneHotEncoder only works on numeric columns (typically produced by StringIndexer).
It cannot directly encode raw string values (e.g., "red", "blue").
Correct Workflow:
from pyspark.ml.feature import StringIndexer, OneHotEncoder
# Step 1: Convert strings to numerical indices
indexers = [
    StringIndexer(inputCol=col, outputCol=f"{col}_index")
    for col in input_columns
]
# Step 2: One-hot encode the indices
encoder = OneHotEncoder(
    inputCols=[f"{col}_index" for col in input_columns],
    outputCols=output_columns
)
# Pipeline to chain steps
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=indexers + [encoder])
encoded_df = pipeline.fit(features_df).transform(features_df)
Why Other Options Are Incorrect:
"Columns need same names as input_columns":
Incorrect. outputCols can differ (e.g., "color_index" → "color_encoded").
"Specify 'method' parameter":
OneHotEncoder in Spark ML has no method parameter (unlike scikit-learn).
"Remove the 'fit' operation":
fit() is required to learn categories and create the encoder model.
Key Takeaway:
For one-hot encoding in Spark ML:
✅ First use StringIndexer to convert strings to indices.
✅ Then apply OneHotEncoder to the indexed columns.
Note: For large cardinality, use OneHotEncoder(dropLast=True) to avoid dimensionality explosion.
ドメイン
ML Workflows

問題2-30
未回答
A data analyst has constructed an ML pipeline utilizing a fixed input dataset with Spark ML. However, the processing time of the pipeline is excessive. To improve efficiency, the analyst expanded the number of workers in the cluster. Interestingly, they observed a discrepancy in the row count of the training set post-cluster reconfiguration compared to its count prior to the adjustment.
Which strategy ensures a consistent training and test set for each model iteration?
Implement manual partitioning of the input dataset
正解
Persistently store the split datasets
Adjust the cluster configuration manually
Prescribe a rate in the data splitting process
There exists no strategy to assure consistent training and test set
全体的な説明
Correct Answer:
Persistently store the split datasets
Explanation:
To ensure consistent training and test sets across pipeline runs (regardless of cluster changes), the analyst should:
Split the dataset once (e.g., using randomSplit()).
Persist the splits (e.g., save to Delta Lake or cache in memory/storage).
Reuse the same splits for all subsequent runs.
Why This Works:
Avoids randomness: Prevents Spark from recalculating splits due to repartitioning or cluster resizing.
Reproducibility: Guarantees identical training/test data across executions.
Example:
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
# Step 1: Split and persist
train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)
train_df.write.mode("overwrite").parquet("/path/to/train")
test_df.write.mode("overwrite").parquet("/path/to/test")
# Step 2: Reload in future runs
train_df = spark.read.parquet("/path/to/train")
test_df = spark.read.parquet("/path/to/test")
Why Other Options Are Incorrect:
Manual partitioning:
Does not address randomness in randomSplit().
Adjust cluster config:
Irrelevant; splits depend on data partitioning, not cluster size.
Prescribe a rate in splitting:
Not a valid Spark ML concept.
"No strategy exists":
Incorrect. Persistence ensures consistency.
Key Takeaway:
For deterministic splits in Spark ML:
✅ Persist splits (Delta/Parquet/cache).
✅ Use a fixed seed in randomSplit().
Note: Caching (train_df.cache()) works but is volatile across sessions. Disk storage (Delta/Parquet) is more reliable.
ドメイン
Spark ML

問題2-31
未回答
At which version of Apache Spark did Pandas API on Spark become available?
Apache Spark 2.4
Apache Spark 3.0
正解
Apache Spark 3.2
Apache Spark 4.0
全体的な説明
Correct Answer:
Apache Spark 3.2
Explanation:
The Pandas API on Spark (formerly known as Koalas) was officially integrated into Apache Spark 3.2 (released in October 2021). This integration allows users to write pandas-like code that automatically scales to distributed Spark clusters.
Key Milestones:
Spark 3.2: Pandas API on Spark became part of the official Spark distribution under the pyspark.pandas module.
Prior to Spark 3.2: Users had to install the separate koalas package.
Example Usage (Spark 3.2+):
import pyspark.pandas as ps  # Requires Spark 3.2+
# Read data with pandas-like syntax (runs on Spark)
df = ps.read_csv("data.csv")
df.groupby("category").sum()
Why Other Options Are Incorrect:
Spark 2.4:
Predates the Pandas API on Spark. Users relied on third-party tools like koalas.
Spark 3.0:
Introduced many improvements but not the Pandas API.
Spark 4.0:
Does not exist yet (as of 2024).
Key Takeaway:
For pandas-like syntax on Spark:
✅ Use pyspark.pandas in Spark 3.2 or later.
✅ No need for separate koalas installations.
ドメイン
Pandas API on Spark

問題2-32
未回答
A data scientist wants to use a GPU-accelerated version of a machine learning library, gpu_ml, on a Databricks GPU-enabled cluster.
What is the recommended approach to install this library?
Edit the cluster to use the Databricks Runtime for GPU.
Set the PYTHON_GPU_LIB variable in the cluster configuration to include gpu_ml.
Run %pip install gpu_ml once on any notebook attached to the cluster.
正解
Add gpu_ml to the cluster’s GPU library dependencies.
There is no way to install GPU-accelerated libraries on a Databricks cluster.
全体的な説明
Correct Answer:
Add gpu_ml to the cluster’s GPU library dependencies.
Explanation
When you need to use a Python library (including GPU-accelerated ones) across all notebooks on a Databricks GPU-enabled cluster, the recommended and most seamless approach is to install it as a cluster library. This way:
All notebooks attached to that cluster automatically have access to the library.
Restarting the cluster does not require reinstalling the library each time.
You ensure the library is installed in the same environment that the GPU runtime is using.
Steps to do this in Databricks usually include:
Navigate to your cluster in the Databricks UI.
Choose Libraries.
Click Install New → select PyPI → enter the library name (gpu_ml) → click Install.
Why the Other Options Are Incorrect
Edit the cluster to use the Databricks Runtime for GPU
While you do need a GPU runtime to leverage GPU acceleration, just switching to a GPU runtime doesn’t automatically install gpu_ml. You still need to install the library itself.
Set the PYTHON_GPU_LIB variable in the cluster configuration
Databricks does not install Python packages based on an environment variable. You either install via cluster libraries or %pip install.
Run %pip install gpu_ml once on any notebook
You can certainly do this, and it will install the library into the environment for that notebook session. However, it is not recommended if you want the package to persist on the cluster for all notebooks (especially after restarts). You would have to re-run %pip install after the cluster restarts.
There is no way to install GPU-accelerated libraries on a Databricks cluster
This is clearly incorrect. Databricks absolutely supports GPU-accelerated libraries and workloads if you choose a GPU-enabled cluster and install the libraries accordingly.
Therefore, the best practice is to add gpu_ml to the cluster’s GPU library dependencies so that it is available cluster-wide and persists across sessions.

問題2-33
未回答
What does AutoML do if columns have custom imputation methods specified?
It ignores the columns during training.
It performs semantic type detection for those columns.
It performs feature engineering for those columns.
正解
It does not perform semantic type detection for those columns.
全体的な説明
Correct Answer:
It does not perform semantic type detection for those columns.
Explanation:
In Databricks AutoML, if a column has a custom imputation method specified, AutoML skips semantic type detection for that column. This means:
AutoML does not infer the column type automatically
Normally, AutoML detects semantic types (e.g., categorical, numerical, datetime, text) to apply preprocessing steps like encoding or feature engineering.
If a custom imputation strategy is defined, AutoML assumes the user has manually handled missing values and does not override the imputation method.
The column remains unchanged beyond the specified imputation
AutoML respects user-defined imputation methods and does not modify the column further.
It still uses the column for model training if the imputed values are valid.
Why Other Options Are Incorrect?
"It ignores the columns during training."
Incorrect because AutoML does not ignore the columns if imputation is applied; it still includes them in training.
"It performs semantic type detection for those columns."
Incorrect because custom imputation disables AutoML's automatic type detection.
The type is assumed to be handled correctly by the user.
"It performs feature engineering for those columns."
Incorrect because feature engineering (like encoding or transformation) typically depends on semantic type detection, which AutoML skips for custom-imputed columns.
Conclusion:
When custom imputation is specified, AutoML does not perform semantic type detection for those columns, assuming that the user has already handled the preprocessing correctly.
ドメイン
AutoML

問題2-34
未回答
You are working on a machine learning project and need to create a cluster for exploratory data analysis and model prototyping. Which cluster type is the most suitable choice for this scenario?
Multi-node Cluster
正解
Single-node Cluster
Task-specific Cluster
Standard Cluster
全体的な説明
Answer:
A Single-node Cluster is typically the most suitable choice for exploratory data analysis (EDA) and model prototyping.
Why a Single-Node Cluster?
Lower Complexity:
Single-node clusters run the driver and executor on the same node. This simplifies resource allocation and environment setup during early experimentation phases.
Cost-Effectiveness:
If your dataset isn’t massive, a single-node cluster can be more cost-effective than spinning up multiple nodes.
Faster Spin-up and Iteration:
Single-node clusters generally start quickly, allowing you to iterate faster as you explore data and prototype models.
Why Not the Other Options?
Multi-node Cluster:
Typically used for large-scale data processing or training workloads that require parallelism and distributed computing. If your EDA or model prototyping doesn’t involve huge datasets, this might be overkill.
Task-specific Cluster:
Often refers to job clusters created specifically to run a job task. They spin up for the job and shut down afterwards. This isn’t as flexible for interactive exploration and prototyping where you need a persistent session.
Standard Cluster:
In many Databricks contexts, “standard cluster” can be a multi-node cluster. Unless you specifically need distributed computing for huge data, single-node is more straightforward for quick EDA and prototyping.
Hence, for exploratory and initial model experiments, a Single-node Cluster is usually the best choice.
ドメイン
Cluster Creation and Management

問題2-35
未回答
What does the 'batch' postfix signify in functions like DataFrame.pandas_on_spark.transform_batch() in pandas-on-Spark?
It indicates a specific row of the DataFrame.
正解
It specifies a chunk of pandas-on-Spark DataFrame or Series.
It refers to the entire DataFrame.
It denotes a single operation on a column.
全体的な説明
The answer is B. It specifies a chunk of pandas-on-Spark DataFrame or Series.
Explanation:
Distributed Processing: Pandas API on Spark leverages Spark's distributed processing capabilities to handle large datasets efficiently.
Chunk-Based Operations: To achieve this, it often divides DataFrames or Series into smaller, manageable chunks for processing.
'batch' Postfix: The batch postfix in functions like transform_batch() signifies that they operate on these chunks of data, rather than the entire DataFrame or Series at once.
Key Points:
Optimized Performance: Chunk-based operations often improve performance and memory efficiency for large datasets.
Function-Specific Behavior: The exact behavior of functions with the batch postfix varies depending on the function's purpose:
Some apply the function to each chunk independently, returning a combined result.
Others might aggregate or combine results across chunks in specific ways.
Example:
import pandas as pd
# Sample pandas-on-Spark DataFrame
df = pd.DataFrame({"A": [1, 2, 3, 4, 5], "B": [6, 7, 8, 9, 10]})
# Define a function to apply to chunks
def square_values(df):
    return df * df
# Apply the function to chunks using transform_batch
df_squared = df.pandas_on_spark.transform_batch(square_values)
# Output: DataFrame with columns A and B squared, processed in chunks
Benefits of Chunk-Based Operations:
Scalability: Enables efficient handling of large datasets that might not fit into a single machine's memory.
Parallelism: Allows for parallel processing of chunks across multiple nodes in a Spark cluster, potentially accelerating computations.
Resource Management: Optimizes memory usage by processing data in smaller portions.
ドメイン
Pandas API on Spark

問題2-36
未回答
In a distributed computing system, what is the purpose of data locality?
Minimizing Data Replication
正解
Reducing Data Transmission Overhead
Ensuring Data Consistency
Enhancing Data Compression
全体的な説明
Correct Answer:
Reducing Data Transmission Overhead
Explanation:
Data locality in distributed computing (e.g., Apache Spark) refers to the principle of minimizing data movement across the network by ensuring computations are performed where the data resides. This is critical for:
Performance Optimization:
Avoids transferring large datasets between nodes, reducing latency and network congestion.
Efficiency:
Tasks are scheduled on nodes that already have the required data partitions.
How Spark Achieves Data Locality:
RDD/DataFrame Partitioning: Data is split into partitions distributed across workers.
Task Scheduling: Spark prioritizes scheduling tasks on nodes that hold the relevant data ("preferred locations").
Example:
# Read data (partitions distributed across workers)
df = spark.read.parquet("data.parquet")
# Filter operation runs on nodes where data resides (no shuffling if partitioned correctly)
filtered_df = df.filter(df["value"] > 100)
Why Other Options Are Incorrect:
Minimizing Data Replication:
Data locality doesn’t reduce replication (replication is for fault tolerance).
Ensuring Data Consistency:
Consistency is managed by storage systems (e.g., Delta Lake), not locality.
Enhancing Data Compression:
Compression is independent of locality.
Key Takeaway:
Data locality reduces network overhead by:
✅ Co-locating computation with data.
✅ Leveraging partitioning (e.g., by key in joins).
Note: Use repartition() or partitionBy() to optimize locality for specific workflows.
ドメイン
Scaling ML Models

問題2-37
未回答
What is the potential reason for the reduced performance speed when using the pandas API compared to native Spark DataFrames, especially for large datasets?
Choose only ONE best answer.
正解
The employment of an internalFrame to maintain metadata
The requirement for an increased amount of code
The dependence on CSV files
The immediate evaluation of all processing operations
The absence of data distribution
全体的な説明
Correct Answer:
The employment of an internalFrame to maintain metadata
Explanation:
The pandas API on Spark (Koalas) introduces an internalFrame layer to bridge pandas-like operations with Spark's distributed execution. While this enables familiar syntax, it adds overhead due to:
Metadata Management:
The internalFrame tracks pandas-like indices, column names, and data types, requiring extra bookkeeping.
Conversion Costs:
Pandas operations are translated to Spark plans via this layer, which can slow down execution compared to native Spark DataFrames.
Example Impact:
import pyspark.pandas as ps
# pandas API on Spark (uses internalFrame)
kdf = ps.DataFrame(...)
result = kdf.groupby("col1").sum()  # Slower due to metadata handling
# Native Spark (direct execution)
sdf = spark.createDataFrame(...)
result = sdf.groupBy("col1").sum()  # Faster
Why Other Options Are Incorrect:
"Increased code amount":
Irrelevant; performance is about execution, not code volume.
"Dependence on CSV files":
Unrelated; data source format doesn’t affect API performance.
"Immediate evaluation":
Both APIs use lazy evaluation.
"Absence of data distribution":
False; pandas API on Spark does distribute data (unlike vanilla pandas).
Key Takeaway:
For large datasets, prefer native Spark DataFrames when:
✅ Performance is critical (avoid internalFrame overhead).
✅ Advanced Spark optimizations (e.g., predicate pushdown) are needed.
Use pandas API on Spark for:
✅ Pandas familiarity on small-to-medium distributed data.
ドメイン
Spark ML

問題2-38
未回答
How does AutoML handle forecasting problems when there are multiple values for a timestamp in a time series?
It takes the maximum value.
It takes the minimum value.
正解
It averages the values.
It discards the additional values.
全体的な説明
Correct Answer:
It averages the values.
Explanation:
When AutoML encounters multiple values for the same timestamp in a time series forecasting problem, it aggregates the values by averaging them by default. This ensures:
Consistency: Smooths out noise or duplicates in the data.
Stability: Prevents bias toward extreme values (e.g., max/min).
Example Workflow in AutoML:
from databricks import automl
# AutoML automatically handles duplicate timestamps
summary = automl.forecast(
    dataset=df,
    time_col="timestamp",
    target_col="value",
    horizon=7  # Forecast 7 periods ahead
)
Input Data:
| timestamp           | value |
|---------------------|-------|
| 2023-01-01 00:00:00 | 10    |
| 2023-01-01 00:00:00 | 20    | → AutoML averages to **15**
Why Other Options Are Incorrect:
Max/Min Values:
Introduces bias (overestimates/underestimates trends).
Discarding Values:
Loses information and reduces dataset quality.
Key Takeaway:
For forecasting with duplicate timestamps:
✅ AutoML averages values by default.
✅ Manually preprocess data if custom aggregation (e.g., sum, median) is needed.
ドメイン
AutoML

問題2-39
未回答
A data scientist is experimenting with different hyperparameters for a machine learning model using MLflow. They want to compare the performance of different runs and select the best hyperparameters.
What MLflow function should they use for hyperparameter tuning?
正解
mlflow.search_runs
mlflow.log_param
mlflow.start_run
mlflow.search_hyperparams
全体的な説明
Correct Answer:
mlflow.search_runs
Explanation:
When performing hyperparameter tuning in MLflow, a data scientist needs to compare different runs and find the best-performing hyperparameter values. The mlflow.search_runs() function allows users to query and filter past runs based on metrics, parameters, and tags, making it the ideal choice.
How mlflow.search_runs() Helps in Hyperparameter Tuning:
Retrieves and compares all experiment runs
Lists all recorded runs, including hyperparameters (log_param) and evaluation metrics (log_metric).
Filters runs based on performance
Example: Find the run with the highest accuracy or lowest loss.
Sorts runs to identify the best model
Sort by a specific metric to select the best hyperparameter combination.
Example Usage in MLflow:
import mlflow
import pandas as pd
# Search and retrieve all runs from an experiment
experiment_id = mlflow.get_experiment_by_name("my_experiment").experiment_id
runs_df = mlflow.search_runs(experiment_ids=[experiment_id])
# Find the best run based on accuracy
best_run = runs_df.loc[runs_df["metrics.accuracy"].idxmax()]
print(best_run[["run_id", "params.learning_rate", "metrics.accuracy"]])
This retrieves all runs, sorts them by accuracy, and selects the best hyperparameters.
Why Other Options Are Incorrect?
mlflow.log_param
❌ Incorrect because log_param is used to log a single hyperparameter value within a run but does not compare multiple runs.
mlflow.start_run
❌ Incorrect because start_run initiates an MLflow run but does not retrieve past runs for hyperparameter tuning.
mlflow.search_hyperparams
❌ Incorrect because this function does not exist in MLflow.
Conclusion:
To retrieve, compare, and filter past runs for hyperparameter tuning, mlflow.search_runs() is the correct function to use.
ドメイン
Databricks ML

問題2-40
未回答
In a machine learning project, your team is dealing with highly imbalanced classes. Which Spark ML algorithm is suitable for addressing class imbalance and improving the performance of the model in such scenarios?
Decision Trees
Random Forest
Support Vector Machines
正解
Synthetic Minority Over-sampling Technique (SMOTE)
全体的な説明
Correct Answer:
Synthetic Minority Over-sampling Technique (SMOTE)
Explanation:
When dealing with highly imbalanced classes in a machine learning project, the best approach is to balance the dataset before training to prevent the model from being biased toward the majority class. In Spark ML, SMOTE (Synthetic Minority Over-sampling Technique) is a suitable technique for oversampling the minority class by generating synthetic samples.
Why SMOTE?
Balances Class Distribution
SMOTE creates synthetic samples of the minority class, making the dataset more balanced without simply duplicating existing instances.
Improves Model Generalization
Helps models learn better decision boundaries instead of overfitting to the dominant class.
Available in Spark ML
Can be applied using libraries like pyspark.ml combined with Imbalanced-learn (imbalanced-learn.org), or custom implementations in Spark.
Example Usage of SMOTE in PySpark
from imblearn.over_sampling import SMOTE
from pyspark.sql import SparkSession
import pandas as pd
# Initialize Spark session
spark = SparkSession.builder.appName("SMOTE Example").getOrCreate()
# Sample imbalanced dataset (convert to Pandas for SMOTE)
data = pd.DataFrame({
    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'feature2': [5, 4, 3, 6, 7, 2, 8, 9, 1, 0],
    'label': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # Imbalanced classes
})
# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(data[['feature1', 'feature2']], data['label'])
# Convert back to Spark DataFrame
balanced_df = spark.createDataFrame(pd.DataFrame(X_resampled, columns=['feature1', 'feature2']).assign(label=y_resampled))
balanced_df.show()
Why Other Options Are Incorrect?
Decision Trees
❌ Incorrect because Decision Trees do not inherently handle class imbalance well.
If trained on imbalanced data, they tend to favor the majority class.
Random Forest
❌ Incorrect because Random Forest does not solve class imbalance directly.
Although ensemble methods can help, Random Forest still needs resampling techniques like SMOTE for better performance.
Support Vector Machines (SVMs)
❌ Incorrect because SVMs do not inherently handle imbalanced data well unless used with weighted classes or resampling.
Conclusion:
When dealing with highly imbalanced classes, SMOTE (Synthetic Minority Over-sampling Technique) is the best approach in Spark ML because it balances the dataset by generating synthetic samples, improving model performance.
ドメイン
Spark ML Algorithms

問題2-41
未回答
How can you get and set the value of a single option in Pandas API on Spark?
正解
Using get_option() and set_option() functions
Directly modifying the configuration file
Accessing the options as attributes of the top-level options attribute
Using config.py file in the pandas_on_spark namespace
全体的な説明
Correct Answer:
Using get_option() and set_option() functions
Explanation:
In Pandas API on Spark (formerly Koalas), you can get and set configuration options using the same familiar functions as pandas:
get_option(option_name): Retrieves the current value of an option.
set_option(option_name, value): Updates the option’s value.
Example:
import pyspark.pandas as ps
# Get the current maximum display rows
current_rows = ps.get_option("display.max_rows")
# Set the maximum display rows to 100
ps.set_option("display.max_rows", 100)
Common Options:
display.max_rows: Controls how many rows to show when printing a DataFrame.
compute.max_rows: Limits the number of rows for operations (e.g., head()).
compute.ops_on_diff_frames: Enables/disables operations between different DataFrames.
Why Other Options Are Incorrect:
Modifying configuration file:
Pandas API on Spark doesn’t use external config files for runtime options.
Top-level options attribute:
Pandas uses this, but Pandas API on Spark uses functions (get_option/set_option).
config.py file:
No such file exists in the pandas_on_spark namespace.
Key Takeaway:
For configuration in Pandas API on Spark:
✅ Use get_option() and set_option() (like pandas).
✅ Options control display, computation, and behavior.
Note: These settings are session-specific (not cluster-wide).
ドメイン
Pandas API on Spark

問題2-42
未回答
Your team is designing a machine learning model that requires processing large-scale graph data. Which Spark component is specifically designed for graph processing tasks in distributed environments?
Spark SQL
Spark MLlib
正解
Spark GraphX
Spark Streaming
全体的な説明
Correct Answer:
Spark GraphX
Explanation:
Spark GraphX is Apache Spark's dedicated library for distributed graph processing. It enables efficient computation and analysis of large-scale graph data by:
Graph-Specific Optimizations:
Provides vertex-centric APIs (e.g., Pregel abstraction) for iterative graph algorithms.
Optimizes for graph partitioning and in-memory computation.
Integration with Spark Ecosystem:
Graphs are represented as property graphs (vertices + edges with attributes).
Seamlessly combines with DataFrames and RDDs for hybrid workflows.
Use Cases:
Social network analysis (e.g., PageRank, community detection).
Fraud detection (e.g., identifying connected entities).
Recommendation systems (e.g., collaborative filtering).
Example:
from pyspark.sql import SparkSession
from graphframes import GraphFrame  # Uses GraphX under the hood
spark = SparkSession.builder.appName("GraphX").getOrCreate()
# Create vertices and edges DataFrames
vertices = spark.createDataFrame([("A", "Alice"), ("B", "Bob")], ["id", "name"])
edges = spark.createDataFrame([("A", "B", "friend")], ["src", "dst", "relationship"])
# Build a graph
graph = GraphFrame(vertices, edges)
# Run PageRank
results = graph.pageRank(resetProbability=0.15, maxIter=10)
results.vertices.show()
Why Other Options Are Incorrect:
Spark SQL:
For structured data queries, not graph algorithms.
Spark MLlib:
Focuses on traditional ML (e.g., classification, regression), not graph analytics.
Spark Streaming:
Processes real-time data streams, not graph structures.
Key Takeaway:
For graph processing in Spark:
✅ Use GraphX (or higher-level libraries like GraphFrames).
✅ Avoid reinventing graph operations with vanilla Spark.
Note: For Python, GraphFrames (built on GraphX) offers a more user-friendly API.
ドメイン
Scaling ML Models

問題2-43
未回答
Scenario:
You are concerned about the performance of operations involving the default index in Pandas API on Spark. Which option allows you to configure the default index type to improve performance in large datasets?
compute.default_index_cache
正解
compute.default_index_type
compute.ops_on_diff_frames
compute.shortcut_limit
全体的な説明
Correct Answer:
compute.default_index_type
Explanation:
In Pandas API on Spark, the compute.default_index_type option controls the type of index generated for operations that require a new index (e.g., reset_index(), concatenation). Configuring this can significantly improve performance for large datasets by choosing an index type optimized for your workload.
Key Index Types:
sequence (default):
Simple sequential index (best for small-to-medium data).
distributed-sequence:
Generates unique, distributed indices (ideal for large datasets).
Avoids bottlenecks by parallelizing index creation.
distributed:
Uses hashing for distributed indices (faster but may have duplicates).
How to Set:
import pyspark.pandas as ps
# Optimize for large datasets
ps.set_option("compute.default_index_type", "distributed-sequence")
Impact on Performance:
distributed-sequence:
Eliminates single-node bottlenecks during index generation.
Adds slight overhead for uniqueness guarantees.
distributed:
Fastest but risks duplicate indices (use only if uniqueness isn’t critical).
Why Other Options Are Incorrect:
compute.default_index_cache:
Doesn’t exist; caching is handled separately.
compute.ops_on_diff_frames:
Enables operations between DataFrames but doesn’t affect index performance.
compute.shortcut_limit:
Controls when to use fast-path optimizations, unrelated to index types.
Key Takeaway:
For large datasets in Pandas API on Spark:
✅ Set compute.default_index_type="distributed-sequence" for scalable, unique indices.
✅ Use "distributed" for speed if duplicates are acceptable.
Example Workflow:
# Configure for large data
ps.set_option("compute.default_index_type", "distributed-sequence")
# Operations now use optimized indices
df = ps.DataFrame(...)
df.reset_index()  # Uses distributed-sequence index
ドメイン
Pandas API on Spark

問題2-44
未回答
A data engineering team is responsible for maintaining data quality in Databricks Delta tables. They want to ensure that only valid data is written to the tables, and any data that doesn't meet the validation criteria is rejected.
What Databricks feature or option can help achieve this?
Use the UPDATE statement to modify invalid data.
Implement a Databricks Job to perform data validation before writing to Delta tables.
正解
Utilize Delta table constraints to enforce data quality rules.
Export the invalid data to a separate file for manual review.
全体的な説明
Correct Answer:
Utilize Delta table constraints to enforce data quality rules.
Explanation:
In Databricks Delta Lake, ensuring data quality and preventing invalid records from being written to tables can be achieved using Delta table constraints, specifically CHECK constraints and NOT NULL constraints.
Why Use Delta Table Constraints?
Prevents Invalid Data from Being Written
Enforces business rules at the storage level, ensuring that only valid data is persisted.
Invalid rows cause transactions to fail, preventing bad data from entering the system.
Improves Data Integrity
CHECK constraints validate data values (e.g., price > 0).
NOT NULL constraints prevent missing values in required fields.
Enforces Schema Compliance at Write Time
Unlike post-processing validation jobs, constraints prevent bad data from ever being written.
Example: Implementing Delta Table Constraints
-- Create a Delta table with constraints
CREATE TABLE sales_data (
    sale_id STRING NOT NULL,
    amount DOUBLE CHECK (amount > 0),
    category STRING
) USING DELTA;
The NOT NULL constraint ensures that sale_id is always populated.
The CHECK constraint enforces that amount must always be greater than 0.
Why Other Options Are Incorrect?
"Use the UPDATE statement to modify invalid data."
❌ Incorrect because this only fixes data after it is already written.
The goal is to prevent bad data from entering in the first place.
"Implement a Databricks Job to perform data validation before writing to Delta tables."
❌ Partially correct, but it requires additional logic and manual intervention.
Delta constraints enforce rules automatically at the table level, making them more robust.
"Export the invalid data to a separate file for manual review."
❌ Incorrect because this is reactive rather than preventative.
Constraints stop invalid data at write time, preventing the need for later corrections.
Conclusion:
To enforce data quality in Databricks Delta tables, Delta table constraints are the best approach because they automatically validate data at the write level, preventing invalid records from being stored.

問題2-45
未回答
A data scientist is utilizing Spark SQL to import data into a machine learning pipeline. Once the data is imported, the scientist gathers all their data into a pandas DataFrame and executes machine learning tasks using scikit-learn.
Which of the following Databricks cluster modes is the most appropriate for this specific use case?
Choose only ONE best answer.
SQL Endpoint
Standard
High Concurrency
Pooled
正解
Single Node
全体的な説明
Correct Answer:
Single Node
Explanation:
Why Single Node?
The scenario describes a workflow where data is collected into a pandas DataFrame and processed using scikit-learn, which are single-node, non-distributed libraries.
Since Spark SQL is only used for data import (not distributed ML training), a Single Node cluster is sufficient and cost-effective.
Single Node clusters avoid unnecessary overhead from distributed computing when the workload doesn’t require it.
Example Workflow:
# Spark SQL to import data (runs on the driver)
spark_df = spark.sql("SELECT * FROM table")
# Convert to pandas (collects data to the driver)
pandas_df = spark_df.toPandas()
# Train scikit-learn model (single-node)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier().fit(pandas_df[features], pandas_df[label])
Why Other Options Are Incorrect:
Standard Mode:
Designed for distributed workloads (e.g., Spark MLlib). Overkill for single-node scikit-learn.
High Concurrency:
Optimized for multiple users running SQL queries, not single-user ML workloads.
SQL Endpoint:
A serverless SQL service, not meant for running Python ML code.
Pooled:
Used for job clusters with instance pooling, not relevant here.
Key Takeaway:
Use Single Node when the ML pipeline relies on single-node libraries (pandas/scikit-learn) after data import. Distributed clusters waste resources in this case.
ドメイン
Databricks ML

問題2-46
未回答
Why is it potentially expensive to rely on schema inference when using certain pandas API on Spark functions?
Schema inference is always cheap.
It leads to better performance.
正解
It may result in executing the Spark job twice.
Schema inference is not supported in pandas API on Spark.
全体的な説明
Correct Answer:
It may result in executing the Spark job twice.
Explanation:
In pandas API on Spark (pyspark.pandas), relying on schema inference can be potentially expensive because it may trigger the execution of the Spark job twice. This happens because Spark needs to scan the dataset to determine the schema before applying transformations.
Why Schema Inference Can Be Expensive?
Requires an Initial Scan of the Data
When schema inference is used, Spark must read a portion of the data to deduce column types.
If the dataset is large, this adds extra computational overhead.
Can Cause Double Execution
Certain operations in pandas-on-Spark trigger a Spark job once for schema inference and again for actual computation.
This can double the processing time and resource consumption.
Example of Expensive Schema Inference
import pyspark.pandas as ps
# Reading a CSV file without specifying schema
df = ps.read_csv("large_dataset.csv")
# Performing a transformation
df["new_col"] = df["existing_col"] * 2
Since schema is not explicitly provided, Spark scans the data to infer column types.
If transformations require another scan, Spark re-executes the job, leading to performance degradation.
Best Practice: Avoiding Expensive Schema Inference
✔ Explicitly define schema when reading data using read_csv() or read_parquet().
df = ps.read_csv("large_dataset.csv", dtype={"existing_col": "int"})
✔ Use cached DataFrames to avoid redundant scans.
Why Other Options Are Incorrect?
"Schema inference is always cheap."
❌ Incorrect because schema inference requires an extra scan, which increases computational cost for large datasets.
"It leads to better performance."
❌ Incorrect because schema inference can slow down performance due to extra Spark job executions.
Explicitly defining schema is faster and more efficient.
"Schema inference is not supported in pandas API on Spark."
❌ Incorrect because pandas-on-Spark does support schema inference, but it is not always optimal for performance.
Conclusion:
Relying on schema inference in pandas API on Spark can be expensive because it may trigger a Spark job twice, leading to performance degradation. The best practice is to explicitly define the schema to avoid unnecessary computations.
ドメイン
Pandas API on Spark

問題2-47
未回答
Your team is implementing a machine learning model that requires synchronization of distributed nodes to ensure accurate results. What is the process of synchronizing distributed nodes in a parallel processing environment called?
Parallelization
正解
Synchronization
Coordination
Aggregation
全体的な説明
Correct Answer:
Synchronization
Explanation:
Why Synchronization?
In distributed ML (e.g., Spark MLlib, Horovod, or TensorFlow distributed training), synchronization ensures that worker nodes align their computations (e.g., gradient updates in deep learning) to maintain model consistency.
Examples:
AllReduce (in frameworks like Horovod) synchronizes gradients across GPUs/CPUs.
Barrier execution in Spark forces nodes to wait for others before proceeding.
Code Example (Horovod for TensorFlow):
import horovod.tensorflow as hvd
hvd.init()  # Initialize synchronization context
optimizer = hvd.DistributedOptimizer(optimizer)  # Sync gradients
Why Other Options Are Incorrect:
Parallelization:
Refers to splitting work across nodes (e.g., data parallelism), but doesn’t imply coordination.
Coordination:
Broader than synchronization (e.g., task scheduling), not specific to aligning computations.
Aggregation:
Combines results (e.g., averaging gradients) but doesn’t ensure node alignment.
Key Takeaway:
Synchronization is the precise term for aligning distributed nodes in ML training, critical for avoiding race conditions and ensuring correctness.
ドメイン
Scaling ML Models

問題2-48
未回答
Which algorithms are used by Databricks AutoML for classification models?
Decision trees and random forests.
Decision trees, random forests, and Auto-ARIMA.
正解
Decision trees, random forests, logistic regression, and XGBoost.
Decision trees, random forests, logistic regression, XGBoost, and Prophet.
全体的な説明
Correct Answer:
Decision trees, random forests, logistic regression, and XGBoost.
Explanation:
Why This Is Correct?
Databricks AutoML for classification tasks automatically trains and compares multiple algorithms, including:
Decision Trees (simple, interpretable models)
Random Forests (ensemble of decision trees for better accuracy)
Logistic Regression (linear model for probabilistic classification)
XGBoost (gradient-boosted trees for high performance)
These algorithms are chosen because they cover a wide range of use cases, from interpretability to high accuracy, and work well in distributed environments.
Example (Databricks AutoML Workflow):
from databricks import automl
summary = automl.classify(
    dataset=df,
    target_col="label",
    timeout_minutes=30
)
# Output includes trials for all 4 algorithm types
Why Other Options Are Incorrect?
"Decision trees and random forests":
Too limited—misses logistic regression (baseline) and XGBoost (high-performance option).
"Decision trees, random forests, and Auto-ARIMA":
Auto-ARIMA is for time-series forecasting, not classification. Irrelevant here.
"Decision trees, random forests, logistic regression, XGBoost, and Prophet":
Prophet is another time-series tool, not used in classification tasks.
Key Takeaway:
Databricks AutoML for classification uses decision trees, random forests, logistic regression, and XGBoost—a balanced mix of simple and advanced models. Time-series algorithms (ARIMA, Prophet) are never included in classification workflows.
ドメイン
AutoML

問題2-49
未回答
How do you apply a grouped map Pandas UDF to a PySpark DataFrame?
Choose only ONE best answer.
By using the apply method on a DataFrame column
By using the applyInPandas method on a DataFrame
正解
By using the groupBy method followed by the applyInPandas method on a DataFrame.
By using the groupBy method followed by the agg method on a DataFrame
全体的な説明
Correct Answer:
By using the groupBy method followed by the applyInPandas method on a DataFrame.
Explanation:
A grouped map Pandas UDF (User-Defined Function) allows applying custom Pandas functions to each group of a grouped PySpark DataFrame. The correct way to apply a grouped map Pandas UDF is to use:
grouped_df = df.groupBy("column_name").applyInPandas(pandas_udf_function, schema)
Why Use applyInPandas()?
Allows Processing Each Group Independently
Each group in the PySpark DataFrame is passed as a pandas DataFrame, and the function is applied to each group separately.
Returns a Structured PySpark DataFrame
The function must return a pandas DataFrame, which is then converted back to a PySpark DataFrame based on the specified schema.
Example of a Grouped Map Pandas UDF:
from pyspark.sql import SparkSession
import pandas as pd
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType
import pyspark.sql.functions as F
# Initialize Spark session
spark = SparkSession.builder.appName("GroupedMapPandasUDF").getOrCreate()
# Create a sample PySpark DataFrame
data = [(1, "A", 10), (2, "A", 20), (3, "B", 30), (4, "B", 40)]
df = spark.createDataFrame(data, ["id", "category", "value"])
# Define the schema for the output DataFrame
schema = StructType([
    StructField("category", col("category").dataType),
    StructField("mean_value", DoubleType())
])
# Define the grouped map Pandas UDF function
def compute_mean(pdf):
    return pd.DataFrame({"category": pdf["category"].iloc[0], "mean_value": pdf["value"].mean()}, index=[0])
# Apply the grouped map Pandas UDF
result_df = df.groupBy("category").applyInPandas(compute_mean, schema)
# Show results
result_df.show()
Output:
+--------+-----------+
|category|mean_value|
+--------+-----------+
|      A | 15.0     |
|      B | 35.0     |
+--------+-----------+
Why Other Options Are Incorrect?
"By using the apply method on a DataFrame column"
❌ Incorrect because apply() is not a valid method for PySpark DataFrames.
PySpark does not support pandas-like apply() on DataFrames.
"By using the groupBy method followed by the apply method on a DataFrame"
❌ Incorrect because PySpark does not have a groupBy().apply() method.
applyInPandas() is the correct method for grouped map UDFs.
"By using the groupBy method followed by the agg method on a DataFrame"
❌ Incorrect because agg() is used for aggregations like sum, count, avg, but does not allow custom Pandas UDFs to be applied.
agg() is used as follows:
df.groupBy("category").agg(F.avg("value"))
This approach only works with built-in Spark functions, not Pandas UDFs.
Conclusion:
To apply a grouped map Pandas UDF in PySpark, use:
✔ groupBy().applyInPandas(), which executes a Pandas function on each group and returns a structured PySpark DataFrame.
ドメイン
Spark ML

問題2-50
未回答
Your team is optimizing Spark performance for a machine learning project. What technique involves combining multiple small tasks into larger tasks to reduce scheduling overhead and improve processing efficiency?
正解
Task Aggregation
Task Decomposition
Task Parallelism
Task Fusion
全体的な説明
Answer:
Task Aggregation is the technique that involves combining multiple small tasks into larger tasks to reduce scheduling overhead and improve processing efficiency.
Explanation
In a distributed environment like Spark, each partition of data is generally processed as a task. If there are many very small partitions, the overhead of scheduling and coordinating a large number of tasks can become significant. To mitigate this, Spark (especially with features like Adaptive Query Execution) can coalesce or aggregate partitions at runtime—combining small tasks into fewer, larger tasks.
This consolidation means:
Less scheduling overhead overall, since there are fewer tasks to manage.
Potentially improved parallelism once tasks reach a more optimal size (i.e., not too large to cause stragglers, but not so small that overhead dominates).
Among the provided options:
Task Decomposition is the opposite: breaking large tasks into smaller ones.
Task Parallelism refers to running tasks in parallel, rather than merging them into fewer tasks.
Task Fusion is not a standard Spark term for merging tasks; it might refer to operator fusion (e.g., code generation optimizations) but does not specifically address combining tasks to reduce scheduling overhead.
Hence, Task Aggregation is the correct concept.
ドメイン
Scaling ML Models

問題2-51
未回答
A machine learning engineer is working to upgrade a machine learning project in a  way that enables automatic model refresh every time the project runs. The project is linked to an existing model referred to as model_name in the MLflow Model Registry.
The following block of code is part of their approach:
mlflow.sklearn.log_model
(
sk_model=model,
artifact_path="model",
registered_model_name=model_name
)
Given that model_name already exists in the MLflow Model Registry, what does the parameter and argument registered_model_name=model_name denote?
It eliminates the requirement of specifying the model name in the subsequent
obligatory call to mlflow.register_model.
It records a new model titled model_name in the MLflow Model Registry.
It represents the name of the logged model in the MLflow Experiment.
正解
It registers a new version of the model_name model in the MLflow Model Registry.
It denotes the name of the Run in the MLflow Experiment.
全体的な説明
Correct Answer:
It registers a new version of the model_name model in the MLflow Model Registry.
Explanation:
Why This Is Correct?
When registered_model_name=model_name is used in mlflow.sklearn.log_model(), and the model already exists in the MLflow Model Registry:
It creates a new version of the existing model (e.g., increments from v1 to v2).
It does not create a duplicate model or overwrite the existing one.
This is the standard way to version models in MLflow for continuous updates (e.g., automatic retraining).
Example Workflow:
import mlflow
# Log and register a new version of an existing model
mlflow.sklearn.log_model(
    sk_model=retrained_model,
    artifact_path="model",
    registered_model_name="churn_prediction"  # Existing model in Registry
)
If churn_prediction already has versions v1, v2, this code adds v3.
Why Other Options Are Incorrect?
"Eliminates the requirement of calling mlflow.register_model":
Partially true (the log_model call does both logging and registering), but the key point is versioning, not just avoiding a second call.
"Records a new model titled model_name":
Incorrect. If model_name exists, it versions the model rather than creating a new one.
"Name of the logged model in the MLflow Experiment":
The experiment tracks runs, but registered_model_name refers to the Registry, not the experiment.
"Name of the Run in the MLflow Experiment":
The run name is set separately (e.g., mlflow.start_run(run_name="...")).
Key Takeaway:
Use registered_model_name=model_name to auto-version an existing model in the MLflow Registry. Each run adds a new version (v1, v2, etc.), enabling traceability and rollbacks.

問題2-52
未回答
In the context of distributed computing, what does scaling machine learning models refer to?
Increasing the size of individual algorithms
Expanding the dataset size
Adjusting hardware resources
正解
Handling machine learning tasks at scale
全体的な説明
Correct Answer:
Handling machine learning tasks at scale
Explanation:
Why This Is Correct?
In distributed computing, scaling ML models refers to the ability to efficiently process large datasets and complex computations across multiple nodes or clusters.
It involves:
Parallelizing training (e.g., using Spark MLlib, Horovod, or TensorFlow distributed strategies).
Distributing inference (e.g., serving models with Ray or Kubernetes).
Managing resources (e.g., auto-scaling clusters in Databricks).
The focus is on system-level scalability, not just algorithm or data size.
Example (Distributed Training in Spark MLlib):
from pyspark.ml.classification import LogisticRegression
# Spark automatically distributes the model training across workers
lr = LogisticRegression(maxIter=10)
model = lr.fit(large_spark_df)  # Handles data at scale
Why Other Options Are Incorrect?
"Increasing the size of individual algorithms":
This describes model complexity (e.g., deeper neural networks), not distributed scaling.
"Expanding the dataset size":
While scaling often involves larger data, the term refers to how the system handles it (not the data itself).
"Adjusting hardware resources":
A subset of scaling (e.g., adding GPUs), but the broader concept includes software/algorithmic optimizations.
Key Takeaway:
Scaling ML models in distributed systems means efficiently managing large-scale tasks (training/inference) across clusters, leveraging parallelism and resource optimization.
Bonus: Tools like Databricks ML Runtime and Ray abstract this complexity, enabling seamless scaling.
ドメイン
Distributed Computing Concepts

問題2-53
未回答
Your team is dealing with a large-scale machine learning project that requires distributed computing power. Which cluster type in Databricks is designed for such tasks?
Standard Cluster
正解
Multi-node Cluster
Single-node Cluster
Task-specific Cluster
全体的な説明
Correct Answer:
Multi-node Cluster
Explanation:
Why Multi-node Cluster?
Multi-node clusters in Databricks are specifically designed for distributed computing, where workloads (like large-scale ML training) are parallelized across multiple worker nodes.
They leverage Apache Spark under the hood to split data and computations, enabling horizontal scaling for big data and ML tasks.
Example use cases:
Training a deep learning model on terabytes of data.
Running distributed hyperparameter tuning with HyperOpt.
Key Features:
Worker Nodes: Execute tasks in parallel (Spark executors).
Driver Node: Coordinates the cluster (Spark driver).
Auto-scaling: Dynamically adjusts nodes based on workload.
# Example: Distributed ML training in Databricks
from pyspark.ml.classification import RandomForestClassifier
model = RandomForestClassifier().fit(train_df)  # Runs across multiple nodes
Why Other Options Are Incorrect?
Standard Cluster:
A general-purpose cluster (can be multi-node), but the term doesn’t explicitly emphasize distributed computing like "Multi-node" does.
Single-node Cluster:
Has only one node (no parallelism)—useful for small tasks or testing, but not for large-scale ML.
Task-specific Cluster:
Not a standard Databricks term. Task-optimized clusters (e.g., GPU clusters) are a subset of multi-node clusters.
Key Takeaway:
For distributed ML workloads, always choose a Multi-node cluster in Databricks. Configure worker/driver nodes based on your data size and compute requirements.
Pro Tip: Use Databricks Runtime for ML (includes pre-installed libraries like TensorFlow/PyTorch) for optimized performance.
ドメイン
Cluster Creation and Management

問題2-54
未回答
What is the purpose of the early_stop_fn argument in the fmin() function, and how is it used?
It defines the maximum number of seconds an fmin() call can take
It specifies the adaptivity level of Hyperopt
正解
It serves as an optional early stopping function to stop before max_evals is reached
It controls the maximum number of trials to evaluate concurrently
全体的な説明
The answer is c) It serves as an optional early stopping function to stop fmin() before max_evals is reached.
Here's a more detailed explanation:
Purpose:
Early stopping: The early_stop_fn argument allows you to define a custom function that determines when to halt the hyperparameter optimization process prematurely, even if the maximum number of evaluations (max_evals) hasn't been reached. This can save time and resources by avoiding unnecessary evaluations if progress has plateaued or the desired performance has been achieved.
How it works:
Function definition: You create a callable function that takes the current Trials object as input and returns a boolean value (True to stop, False to continue).
Integration with fmin(): You pass this function as the early_stop_fn argument when calling fmin().
Evaluation after each trial: After each trial (i.e., evaluation of a hyperparameter configuration), Hyperopt invokes your early stopping function.
Decision to stop: If your function returns True, Hyperopt terminates the optimization process.
Common use cases:
No improvement in loss: Stopping when the loss hasn't improved for a certain number of trials.
Reaching a target performance: Stopping when a desired metric (e.g., accuracy) has been attained.
Time constraints: Stopping if a maximum time limit has been exceeded.
Example:
from hyperopt import Trials, fmin, tpe
from hyperopt.early_stop import no_progress_loss
# Define an early stopping function to halt if the loss hasn't improved in 20 trials
early_stop_fn = no_progress_loss(20)
# Use early_stop_fn in fmin()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=500, trials=Trials(), early_stop_fn=early_stop_fn)
To clarify:
Option a) is incorrect because the timeout argument controls the maximum time limit.
Option b) is incorrect because adaptivity is determined by the search algorithm itself, not the early_stop_fn argument.
Option d) is incorrect because the max_queue_len argument controls concurrent trials.
ドメイン
Hyperopt & Sparktail

問題2-55
未回答
Why might the loss not decrease monotonically with each run when using stochastic search algorithms like those in Hyperopt?
Stochastic search algorithms always decrease the loss monotonically
Loss does not decrease in Hyperopt due to a bug
正解
Hyperopt's search algorithms aim for faster convergence, allowing occasional increases in loss
Monotonic loss decrease is a requirement for Hyperopt
全体的な説明
Correct Answer:
Hyperopt's search algorithms aim for faster convergence, allowing occasional increases in loss.
Explanation:
Why This Is Correct?
Stochastic search algorithms (e.g., TPE in Hyperopt) explore the hyperparameter space probabilistically, not greedily.
They temporarily accept worse configurations to:
Escape local minima.
Explore promising regions that may yield better long-term results.
This non-monotonic behavior is by design and often leads to better global optima.
Example in Hyperopt:
from hyperopt import fmin, tpe, Trials
def objective(params):
    # Simulated loss with randomness (e.g., validation accuracy)
    loss = (params["x"] - 2) ** 2 + np.random.normal(0, 0.1)
    return loss
trials = Trials()
best = fmin(
    fn=objective,
    space={"x": hp.uniform("x", -10, 10)},
    algo=tpe.suggest,
    max_evals=50,
    trials=trials,
)
# Loss may fluctuate due to exploration vs. exploitation trade-off
Why Other Options Are Incorrect?
"Stochastic search always decreases loss monotonically":
False. Only deterministic algorithms (e.g., gradient descent) guarantee monotonicity.
"Loss does not decrease due to a bug":
Non-monotonicity is expected behavior, not a bug.
"Monotonic loss is a requirement":
Hyperopt explicitly does not enforce this—it prioritizes exploration over strict improvement.
Key Takeaway:
Hyperopt’s stochastic nature means loss may fluctuate, but this helps avoid suboptimal solutions. Use Trials.trials to track progress and adjust max_evals/early_stop_fn as needed.
Pro Tip: Visualize loss over trials to distinguish exploration noise from true convergence issues.
ドメイン
Hyperopt & Sparktail

問題2-56
未回答
How can you execute code with specific option values in Pandas API on Spark?
Using the config.py file
Modifying the Spark configuration directly
Using the set_option() function
正解
Using the option_context context manager
全体的な説明
Correct Answer:
Using the option_context context manager
Explanation:
Why This Is Correct?
The option_context context manager in Pandas API on Spark (formerly Koalas) allows temporary setting of options within a specific code block, ensuring changes don’t persist globally.
This is ideal for testing or adjusting behavior (e.g., display formatting, computation mode) without affecting other parts of your code.
Example Usage:
import pyspark.pandas as ps
# Temporarily set max displayed rows to 10
with ps.option_context("display.max_rows", 10):
    print(ps.DataFrame(range(100)))  # Only shows 10 rows
Common Options:
compute.max_rows: Limits rows computed for lazy evaluation.
display.max_columns: Controls column display.
Why Other Options Are Incorrect?
config.py file:
Pandas API on Spark doesn’t use a standalone config file—it relies on Spark sessions or runtime options.
Modifying Spark configuration:
Changes via spark.conf (e.g., spark.sql.shuffle.partitions) affect Spark SQL, not Pandas API on Spark options.
set_option() function:
While Pandas has pd.set_option(), Pandas API on Spark uses ps.option_context for scoped changes.
Key Takeaway:
For temporary, localized option changes in Pandas API on Spark, always use:
with ps.option_context("option_name", value):
    # Your code here
This avoids side effects and aligns with Spark’s lazy evaluation model.
Bonus: For global settings, use ps.set_option("compute.default_index_type", "distributed") (but prefer option_context for safety).
ドメイン
Pandas API on Spark

問題2-57
未回答
What is the key difference between pandas-on-Spark DataFrame and pandas DataFrame?
正解
The former is distributed, and the latter is in a single machine.
The former is in a single machine, and the latter is distributed.
They are identical in terms of distribution.
The former has more advanced functionalities than the latter.
全体的な説明
Correct Answer:
The former is distributed, and the latter is in a single machine.
Explanation:
Why This Is Correct?
Pandas-on-Spark DataFrame (pyspark.pandas):
Runs on Apache Spark, distributing data and computations across a cluster.
Scales to large datasets (TB+) by parallelizing operations (e.g., groupby, apply).
Example:
import pyspark.pandas as ps
df = ps.read_csv("s3://large_dataset.csv")  # Distributed read
Pandas DataFrame:
Runs on a single machine, limited by RAM/CPU.
Best for small to medium data (GBs).
Example:
import pandas as pd
df = pd.read_csv("local_data.csv")  # Single-node read
Key Differences:
FeaturePandas-on-Spark (pyspark.pandas)PandasExecutionDistributed (Spark)Single-nodeScalabilityHandles TB+ dataLimited by RAMAPI CompatibilityMirrors Pandas APINative Pandas
Why Other Options Are Incorrect?
"Pandas-on-Spark is single-machine, Pandas is distributed":
Backwards—Pandas-on-Spark leverages Spark’s distributed engine.
"Identical in distribution":
Pandas lacks native distributed capabilities.
"Pandas-on-Spark has more functionalities":
Both share similar APIs, but distribution is the core difference.
Key Takeaway:
Use pandas-on-Spark for big data (Spark clusters) and Pandas for small data (single-machine workflows).
Pro Tip: Convert between them with:
pandas_df = ps_df.to_pandas()  # Distributed → Single-node
ps_df = ps.from_pandas(pandas_df)  # Single-node → Distributed
ドメイン
Pandas API on Spark

問題2-58
未回答
How can you control access to a Unity Catalog feature table?
By using the fe.control_access function.
正解
By using the Permissions button on the Catalog Explorer table details page.
By using the fe.manage_permissions function.
By using the fe.update_access function.
全体的な説明
Correct Answer:
By using the Permissions button on the Catalog Explorer table details page.
Explanation:
Why This Is Correct?
Unity Catalog provides a centralized governance interface for managing access to tables, including feature tables.
The Permissions button in the Catalog Explorer UI (Databricks workspace) allows you to:
Grant/revoke read/write access to users/groups.
Set row/column-level security (fine-grained permissions).
Manage ownership of the feature table.
Steps to Configure Access:
Navigate to Catalog Explorer → Select your feature table → Click Permissions.
Add users/groups and assign roles (SELECT, MODIFY, OWN).
Why Other Options Are Incorrect?
fe.control_access / fe.manage_permissions / fe.update_access:
These do not exist in Unity Catalog or Feature Engineering (FE) APIs.
Permissions are managed via UI/REST API/SDK (e.g., databricks-permissions CLI), not fictional functions.
Key Takeaway:
For Unity Catalog feature tables, always use:
UI: Catalog Explorer → Permissions.
API/SDK: Databricks Terraform provider or REST API (/api/2.1/unity-catalog/permissions).
Pro Tip: Combine with Delta Sharing for secure cross-organization access.
ドメイン
Feature Store

問題2-59
未回答
How would you characterize boosting for machine learning models?
正解
Boosting is the ensemble process of training machine learning models sequentially with each model learning from the errors of the preceding models.
Boosting is the ensemble process of training a machine learning model for each sample in a set of bootstrapped samples of the training data and combining the predictions of each model to get a final estimate.
Boosting is the ensemble process of training machine learning models sequentially with each model being trained on a distinct subset of the data.
Boosting is the ensemble process of training machine learning models sequentially with each model being trained on a progressively larger sample of the training data.
Boosting is the ensemble process of training a machine learning model for each sample in a set of bootstrapped samples of the training data, and then appending the model estimates as a feature variable on the training set which is used to train another model
全体的な説明
Correct Answer:
Boosting is the ensemble process of training machine learning models sequentially with each model learning from the errors of the preceding models.
Explanation:
Why This Is Correct?
Boosting is a sequential ensemble method where:
Each new model (weak learner) focuses on correcting the errors of the previous models.
Examples: AdaBoost (reweights misclassified samples), Gradient Boosting (fits residuals), XGBoost/LightGBM (optimized gradient boosting).
Key properties:
Adaptive learning: Models adjust based on prior mistakes.
Weighted voting: Final prediction combines models with weights (e.g., more accurate models have higher influence).
Example (AdaBoost):
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
# Each tree corrects errors of the previous one
model = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner (stump)
    n_estimators=50,
    learning_rate=1.0
)
model.fit(X_train, y_train)
Why Other Options Are Incorrect?
"Training on bootstrapped samples":
Describes bagging (e.g., Random Forest), not boosting.
"Training on distinct subsets":
Implies parallel training (like bagging), but boosting is sequential.
"Progressively larger samples":
Boosting uses the full dataset but reweights samples (no progressive resizing).
"Appending estimates as features":
Resembles stacking, not boosting.
Key Takeaway:
Boosting’s core idea is sequential error correction. It’s powerful for high bias (underfitting) problems but requires careful tuning to avoid overfitting.
Bonus: Use early_stopping_rounds in XGBoost/LightGBM to halt training if validation performance plateaus.
ドメイン
Scaling ML Models

問題2-60
未回答
In which scenario should you use StringIndexer?
正解
When you want the machine learning algorithm to identify a column as a categorical variable
When you want to differentiate between categorical and non-categorical data without knowing the data types
When you want to convert the final output column back to its textual representation
When you want to perform dimensionality reduction on the input data
全体的な説明
Correct Answer:
When you want the machine learning algorithm to identify a column as a categorical variable.
Explanation:
Why This Is Correct?
StringIndexer in PySpark ML is used to convert string/categorical columns into numerical indices, which is required for most ML algorithms (e.g., decision trees, logistic regression).
It assigns a unique integer to each category, enabling the algorithm to treat the column as a categorical feature rather than a string.
Example:
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="category", outputCol="category_index")
indexed_df = indexer.fit(df).transform(df)
Converts:
+--------+--------------+
|category|category_index|
+--------+--------------+
|   "dog"|           0.0|
|   "cat"|           1.0|
+--------+--------------+
Key Use Case:
Preparing categorical text data (e.g., "red", "blue") for ML pipelines.
Why Other Options Are Incorrect?
"Differentiate categorical vs. non-categorical":
StringIndexer doesn’t detect data types—it requires you to specify the categorical column.
"Convert output back to text":
This is done by IndexToString (the inverse of StringIndexer), not StringIndexer itself.
"Dimensionality reduction":
StringIndexer doesn’t reduce dimensions; it encodes labels. Use PCA or FeatureHasher for reduction.
Key Takeaway:
Use StringIndexer before training to encode categorical strings as numerical indices. Always pair it with OneHotEncoder if the algorithm expects one-hot vectors (e.g., linear models).
Pro Tip: For ordinal categories (e.g., "low", "medium", "high"), use OrdinalEncoder instead.
ドメイン
ML Workflows

問題2-61
未回答
In the context of distributed decision trees, what is the primary advantage of using an ensemble method like random forests over a single decision tree?
Choose only ONE best answer.
Ensemble methods are more interpretable than single decision trees
正解
Ensemble methods are less prone to overfitting than single decision trees
Ensemble methods can be trained faster than single decision trees
Ensemble methods require less memory than single decision trees
None of the above
全体的な説明
Correct Answer:
Ensemble methods are less prone to overfitting than single decision trees.
Explanation:
Why This Is Correct?
Random Forests (an ensemble method) combine multiple decision trees through bagging (bootstrap aggregating) and feature randomness, which:
Reduces variance by averaging predictions from diverse trees.
Mitigates overfitting (a common issue with single trees that memorize noise in training data).
Example: A single tree may overfit to outliers, but a forest’s majority vote is more robust.
Key Advantage in Distributed Settings:
In Spark MLlib, distributed training of random forests parallelizes tree construction across workers, maintaining the ensemble’s scalability and generalization benefits.
Why Other Options Are Incorrect?
"More interpretable":
Ensembles are less interpretable (harder to trace decisions than a single tree).
"Trained faster":
Ensembles are slower (train multiple trees) but more accurate.
"Require less memory":
Ensembles use more memory (store multiple trees).
Key Takeaway:
For distributed ML, random forests outperform single trees by balancing bias and variance, especially in noisy or high-dimensional data. Use RandomForestClassifier in PySpark MLlib for scalable training.
Bonus: Tune numTrees and maxDepth to optimize the bias-variance trade-off.
ドメイン
Scaling ML Models

問題2-62
未回答
A Data scientist has created two regression models. The first model uses price as a label variable and the second model uses log(price) as a label variable.
When evaluating the RMSE of each model by comparing the label prediction to the actual price values, the data scientist notices that the RMSE for the second model is much longer than the RMSE of the first model.
Which of the following explanations for these differences is valid?
Choose only ONE best answer.
The second model is much more accurate than the first model.
The data scientist failed to take the log of the predictions in the first model prior to computing the RMSE.
正解
The data scientist failed to exponentiate the predictions in the second model prior to computing the RMSE.
The RMSEis an invalid evaluation metric for regression problems.
The first model is much more accurate than the second model.
全体的な説明
Correct Answer:
The data scientist failed to exponentiate the predictions in the second model prior to computing the RMSE.
Explanation:
When training a model on log-transformed labels (e.g., log(price)), the model's predictions will also be in log space. To compare these predictions to the original scale (e.g., actual price), you must reverse the transformation (exponentiate the predictions) before calculating RMSE.
Why This Matters:
Model 1: Predicts price directly → RMSE is calculated on the same scale.
Model 2: Predicts log(price) → Predictions must be exponentiated (exp(pred)) to match the price scale.
If not exponentiated: RMSE compares log(pred) to price, which is invalid (apples-to-oranges).
Example:
import numpy as np
from sklearn.metrics import mean_squared_error
# Actual prices
actual_prices = np.array([100, 200, 300])
# Model 1: Predicts price directly
pred_prices = np.array([110, 190, 310])
rmse1 = np.sqrt(mean_squared_error(actual_prices, pred_prices))  # Valid
# Model 2: Predicts log(price)
pred_log_prices = np.array([4.7, 5.2, 5.7])
# Incorrect RMSE (log(pred) vs. actual price)
rmse_incorrect = np.sqrt(mean_squared_error(actual_prices, pred_log_prices))  # Wrong!
# Correct RMSE (exp(pred) vs. actual price)
rmse_correct = np.sqrt(mean_squared_error(actual_prices, np.exp(pred_log_prices)))  # Valid
Why Other Options Are Incorrect:
"The second model is much more accurate":
Invalid because the RMSE comparison is flawed (log vs. linear scale).
"Failed to take the log of predictions in the first model":
The first model predicts price directly; no log transformation is needed.
"RMSE is invalid for regression":
RMSE is a standard metric for regression, but scale consistency is critical.
"The first model is more accurate":
The comparison is invalid unless predictions are on the same scale.
Key Takeaway:
For models trained on log-transformed labels:
Exponentiate predictions (exp(pred)) before calculating RMSE.
Compare on the original scale (e.g., price).
Impact of Skipping Exponentiation:
RMSE will appear artificially high because log(pred) values are much smaller than price.

問題2-63
未回答
You are using Databricks AutoML to train a classification model on a dataset with 1M records and 50 features. You want to ensure the best model performance while minimizing manual effort. Which configuration should you apply?
Run AutoML with timeout_minutes=10 and let it select the best algorithm.
Preprocess the data manually, then run AutoML with exclude_frameworks=["XGBoost"] to speed up training.
正解
Use AutoML with primary_metric="f1" and timeout_minutes=60 to allow thorough exploration.
Disable feature engineering in AutoML to avoid unexpected transformations.
全体的な説明
Correct Answer:
"Use AutoML with primary_metric="f1" and timeout_minutes=60 to allow thorough exploration."
✅ Explanation:
When using Databricks AutoML for classification, the goal is to find the best-performing model while minimizing manual effort. Given that the dataset has 1 million records and 50 features, we need sufficient exploration time to allow AutoML to train, evaluate, and optimize multiple models effectively.
Why primary_metric="f1"?
F1-score is ideal for imbalanced datasets as it balances precision and recall.
It ensures that the model does not favor the majority class while sacrificing recall.
If the dataset has class imbalance, f1 is a better metric than accuracy.
Why timeout_minutes=60?
10 minutes is too short for thorough model exploration on a large dataset.
60 minutes provides enough time for AutoML to evaluate different algorithms, perform hyperparameter tuning, and optimize feature selection.
How to Configure AutoML in Databricks:
from databricks.automl import classification
classification.classify(
    input_df=df,
    target_col="label",
    primary_metric="f1",  # Optimize for balanced performance
    timeout_minutes=60  # Allow enough time for thorough model selection
)
❌ Why Other Options Are Incorrect:
"Run AutoML with timeout_minutes=10 and let it select the best algorithm."
❌ Incorrect:
10 minutes is not enough time for AutoML to properly evaluate different models, especially with 1M records and 50 features.
Risk: It may stop early and fail to find the best model.
"Preprocess the data manually, then run AutoML with exclude_frameworks=["XGBoost"] to speed up training."
❌ Incorrect:
XGBoost is one of the best-performing models for structured data.
Excluding XGBoost may eliminate a high-quality model, reducing accuracy.
AutoML already performs automatic feature engineering, reducing the need for manual preprocessing.
"Disable feature engineering in AutoML to avoid unexpected transformations."
❌ Incorrect:
Feature engineering is a crucial step in improving model performance.
AutoML uses techniques like scaling, encoding, and missing value imputation, which help rather than hurt performance.
Manually disabling feature engineering might lead to suboptimal models.
✅ Final Takeaway:
For large datasets, configure Databricks AutoML with:
primary_metric="f1", timeout_minutes=60
This ensures enough time for model exploration and hyperparameter tuning, leading to better performance.

問題2-64
未回答
After training a black-box GBM model, you must explain predictions to stakeholders. You generate SHAP values but find the computation too slow for 1M rows.
How do you optimize this?
Use Kernel SHAP instead of Tree SHAP for faster approximations.
正解
Downsample to 1K rows and compute SHAP only for this subset.
Enable GPU acceleration for SHAP via nvidia-smi.
Cache the model in memory before SHAP computation.
全体的な説明
Correct Answer:
"Downsample to 1K rows and compute SHAP only for this subset."
✅ Explanation:
SHAP (SHapley Additive exPlanations) values are a powerful technique for interpreting black-box models like Gradient Boosting Machines (GBMs). However, SHAP computation can be extremely slow, especially for large datasets like 1M rows, due to the combinatorial nature of Shapley values.
Why Downsampling to 1K Rows is the Best Approach?
SHAP values are highly correlated across similar instances
Computing SHAP on a representative subset (e.g., 1K rows) gives insights without the full computational cost.
Reduces computation time significantly
SHAP scales exponentially with data size.
Computing SHAP for 1M rows may take hours, while 1K rows takes just a few minutes.
Ensures interpretability without performance bottlenecks
Stakeholders don’t need SHAP for every row, just general insights.
A well-selected sample (e.g., stratified or random) preserves interpretability.
Optimized SHAP Computation Using Downsampling
import shap
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
# Train a sample GBM model
X, y = np.random.rand(1000000, 20), np.random.randint(2, size=1000000)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LGBMClassifier()
model.fit(X_train, y_train)
# Downsample to 1K rows
X_sample = X_test[:1000]
# Use TreeExplainer (optimized for GBMs)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_sample)
# Visualize SHAP summary
shap.summary_plot(shap_values, X_sample)
TreeExplainer is efficient but still slow on large datasets.
Using only 1K rows makes SHAP feasible while maintaining interpretability.
❌ Why Other Options Are Incorrect?
"Use Kernel SHAP instead of Tree SHAP for faster approximations."
❌ Incorrect
Kernel SHAP is even slower because it relies on a Monte Carlo approximation.
Tree SHAP is optimized for GBMs and should be preferred.
"Enable GPU acceleration for SHAP via nvidia-smi."
❌ Incorrect
nvidia-smi only monitors GPU usage; it does not accelerate SHAP computations.
SHAP has limited GPU support, and GPU acceleration mainly benefits deep learning models, not GBMs.
"Cache the model in memory before SHAP computation."
❌ Incorrect
The SHAP computation depends on feature interactions, not caching.
Caching won’t significantly reduce SHAP computation time.
✅ Final Takeaway:
For large datasets (1M rows), downsample to 1K rows and compute SHAP only for this subset to maintain interpretability while optimizing performance.

問題2-65
未回答
Scenario:
You are building a real-time recommendation system. Features like user_engagement_30m are computed from a streaming Delta table. You need to ensure these features are continuously updated in the Feature Store for low-latency model inference.
What is the most efficient approach?
Use a scheduled job to recompute the feature batch every 30 minutes and overwrite the Feature Store table.
正解
Set up a Structured Streaming job with writeStream to incrementally update the Feature Store.
Manually trigger a notebook to recompute features using fs.write_table() whenever new data arrives.
Store features in a Redis cache instead of Feature Store for faster updates.
全体的な説明
Correct Answer:
"Set up a Structured Streaming job with writeStream to incrementally update the Feature Store."
✅ Explanation:
For a real-time recommendation system, features like user_engagement_30m need to be continuously updated for low-latency inference. The most efficient approach is to use Structured Streaming with writeStream to incrementally update the Feature Store as new data arrives in the streaming Delta table.
Why Use Structured Streaming with writeStream?
Real-time feature updates
Ensures that the latest feature values are always available in the Feature Store.
Low-latency updates
writeStream ingests new data incrementally, reducing overhead compared to batch recomputation.
Optimized for ML inference
Feature Store supports incremental feature updates, making it ideal for real-time ML applications.
Example: Streaming Feature Updates to Databricks Feature Store
from databricks.feature_store import feature_table
from pyspark.sql.functions import col
from pyspark.sql.streaming import DataStreamWriter
# Read streaming features from Delta table
streaming_df = spark.readStream.format("delta").table("user_engagement")
# Process & prepare features for Feature Store
feature_df = streaming_df.select(
    col("user_id"),
    col("engagement_score"),
    col("timestamp")
)
# Define Feature Store table name
feature_table_name = "user_features"
# Write features to Feature Store using Structured Streaming
feature_df.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/mnt/checkpoints/user_features") \
    .table(feature_table_name)
Reads from a streaming Delta table (user_engagement).
Writes processed features incrementally to the Feature Store table using Structured Streaming.
Maintains checkpointing for fault tolerance.
❌ Why Other Options Are Incorrect?
"Use a scheduled job to recompute the feature batch every 30 minutes and overwrite the Feature Store table."
❌ Incorrect:
Batch processing introduces delay (30-minute lag).
Overwriting the table is inefficient for real-time ML use cases.
Structured Streaming is a better approach for real-time updates.
"Manually trigger a notebook to recompute features using fs.write_table() whenever new data arrives."
❌ Incorrect:
Manual execution is not scalable and not real-time.
Automated streaming updates are required for real-time recommendations.
"Store features in a Redis cache instead of Feature Store for faster updates."
❌ Incorrect:
While Redis is fast for retrieval, it does not support feature versioning, lineage, or ML integration like Databricks Feature Store.
Feature Store is optimized for ML workflows and integrates with model serving.
✅ Final Takeaway:
For real-time feature updates in a recommendation system, use Structured Streaming (writeStream) to incrementally update the Databricks Feature Store, ensuring low-latency and up-to-date features for ML inference.
