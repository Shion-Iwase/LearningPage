問題11
未回答
Which of the following is an example of a distributed machine learning framework?
TensorFlow
正解
Apache Spark MLlib
Scikit-learn
XGBoost
All of the above
全体的な説明
Correct Answer:
Apache Spark MLlib
Explanation:
Apache Spark MLlib is a distributed machine learning framework designed to handle large-scale data processing and machine learning tasks across a cluster of computers. It is built on top of Apache Spark, which provides distributed computing capabilities, making MLlib suitable for big data applications.
Key features of Spark MLlib:
Distributed data processing and machine learning.
Scalability to handle large datasets.
Integration with Spark's ecosystem (e.g., Spark SQL, Spark Streaming).
Example of using Spark MLlib:
from pyspark.ml.classification import LogisticRegression
from pyspark.sql import SparkSession
# Initialize Spark session
spark = SparkSession.builder.appName("MLlibExample").getOrCreate()
# Load dataset
data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")
# Train a logistic regression model
lr = LogisticRegression(maxIter=10, regParam=0.01)
model = lr.fit(data)
Why Other Options Are Incorrect:
TensorFlow:
TensorFlow is a deep learning framework that can be distributed but is primarily designed for deep learning tasks, not general-purpose distributed machine learning like Spark MLlib.
Scikit-learn:
Scikit-learn is a popular machine learning library for Python, but it is not distributed. It is designed for single-node execution and cannot handle large-scale data processing across a cluster.
XGBoost:
XGBoost is a scalable and efficient implementation of gradient boosting, but it is not inherently distributed. While it can handle large datasets efficiently, it does not provide the same distributed computing capabilities as Spark MLlib.
All of the above:
This is incorrect because only Apache Spark MLlib is a distributed machine learning framework. The others are either not distributed or not designed for general-purpose distributed machine learning.
Key Takeaway:
Apache Spark MLlib is the only framework listed that is specifically designed for distributed machine learning, making it the best choice for large-scale data processing and machine learning tasks.
ドメイン
Spark ML

問題12
未回答
How would you obtain summary statistics of spark dataframe for comprehensive data analysis?
spark_dataframe.describe()
spark_dataframe.summary()
dbutils.data.summarize(spark_dataframe)
正解
All of the above
全体的な説明
Correct Answer: All of the above
Explanation:
In Apache Spark, there are multiple ways to obtain summary statistics of a Spark DataFrame for comprehensive data analysis. Each method has its own use case:
1. spark_dataframe.describe()
Provides basic statistical summary (count, mean, stddev, min, max) only for numerical columns.
Does not include advanced percentiles (e.g., median, quartiles).
Example:
df.describe().show()
Output:
summarycolumn1column2count10001000mean45.320.5stddev5.63.2min105max10050
2. spark_dataframe.summary()
Provides a more detailed summary than .describe(), including percentiles (25%, 50%, 75%).
Works on both numerical and categorical columns.
Example:
df.summary().show()
Output:
summarycolumn1column2count10001000mean45.320.5stddev5.63.2min10525%301550%452075%6025max10050
3. dbutils.data.summarize(spark_dataframe) (Databricks-specific)
Databricks utility (dbutils) for exploratory data analysis (EDA).
Provides interactive summary statistics in Databricks notebooks.
Supports visualizations, distributions, and data insights.
Example:
dbutils.data.summarize(df)
Generates an interactive widget in Databricks, where you can explore:
Column distribution
Missing values
Statistical metrics
Why Other Options Are Incorrect?
✅ All options are correct, as they provide different levels of summary statistics.
Thus, the best answer is: "All of the above".
ドメイン
ML workflow

問題13
未回答
A machine learning engineer attempts to scale an ML pipeline by distributing its single-node model tuning procedure. After broadcasting the entire training data onto each core, each core in the cluster is capable of training one model at once. As the tuning process is still sluggish, the engineer plans to enhance the parallelism from 4 to 8 cores to expedite the process. Unfortunately, the total memory in the cluster can't be increased. Under which conditions would elevating the parallelism from 4 to 8 cores accelerate the tuning process?
Choose only ONE best answer.
When the data has a lengthy shape
When the data has a broad shape
When the model can't be parallelized
When the tuning process is randomized
正解
When the entire data can fit on each core
全体的な説明
Correct Answer: When the entire data can fit on each core
Explanation:
In distributed ML training, parallelism works efficiently only if each core has enough memory to handle its assigned workload. Since the total cluster memory cannot be increased, adding more cores (from 4 to 8) will only improve performance if each core has enough memory to store and process the full dataset.
If the dataset fits on each core, then each core can train a separate model independently.
If the dataset exceeds available memory per core, then swapping, disk I/O, or memory overflow will slow down training.
Example in Spark MLlib (Parallel Model Training)
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.classification import LogisticRegression
# Initialize Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ParallelTuning").getOrCreate()
# Load data
df = spark.read.csv("data.csv", header=True, inferSchema=True)
# Define model
lr = LogisticRegression(featuresCol="features", labelCol="label")
# Hyperparameter grid
paramGrid = ParamGridBuilder()\
    .addGrid(lr.regParam, [0.01, 0.1, 1.0])\
    .build()
# Cross-validation with parallelism
crossval = CrossValidator(
    estimator=lr,
    estimatorParamMaps=paramGrid,
    evaluator=None,
    numFolds=3,
    parallelism=8  # Only beneficial if each core can fit the full dataset
)
model = crossval.fit(df)
If each core can handle the dataset, increasing parallelism from 4 to 8 will reduce overall tuning time.
Why Other Options Are Incorrect:
"When the data has a lengthy shape"
Incorrect because long/narrow datasets (many rows, few columns) are not necessarily the bottleneck.
The real issue is whether each core has enough memory to process the dataset.
"When the data has a broad shape"
Incorrect because wide datasets (many columns, few rows) require more memory per core.
If memory is limited, increasing parallelism may worsen performance due to memory overhead.
"When the model can't be parallelized"
Incorrect because if the model cannot be parallelized, then adding more cores will not improve performance.
The question assumes each core trains one model independently, which is a parallelizable task.
"When the tuning process is randomized"
Incorrect because randomized tuning (e.g., Random Search) does not guarantee better performance by adding more cores.
The primary limitation in the question is memory, not randomness.
Thus, the correct answer is: "When the entire data can fit on each core".
ドメイン
ML workflows

問題14
未回答
A data scientist is working with a Spark DataFrame, named 'spark_df'. They intend to generate a new Spark DataFrame that retains only the rows from 'spark_df' where the value in the 'discount' column is less than 0.
Which of the following code segments would accomplish this objective?
spark_df.find(spark_df("discount") < 0)
spark_df.loc[spark_df("discount") < 0]
spark_df.loc[spark_df("discount") < 0,:]
SELECT * FROM spark_df WHERE discount < 0
正解
spark_df.filter(col("discount) < 0)
全体的な説明
Correct Answer:
spark_df.filter(col("discount") < 0)

Explanation:
The correct way to filter rows in a Spark DataFrame based on a condition is to use the filter() method (or its alias where()). The filter() method takes a column expression as input, and col("discount") < 0 is the correct way to specify the condition.
Example:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
# Initialize Spark session
spark = SparkSession.builder.appName("FilterExample").getOrCreate()
# Sample data
data = [(1, 0.1), (2, -0.5), (3, 0.0), (4, -0.2)]
columns = ["id", "discount"]
spark_df = spark.createDataFrame(data, columns)
# Filter rows where discount < 0
filtered_df = spark_df.filter(col("discount") < 0)
filtered_df.show()

Output:
+---+--------+
| id|discount|
+---+--------+
|  2|    -0.5|
|  4|    -0.2|
+---+--------+
Why Other Options Are Incorrect:
spark_df.find(spark_df("discount") < 0):
The find() method does not exist in Spark DataFrame API. This code will result in an error.
spark_df.loc[spark_df("discount") < 0]:
The loc accessor is used in pandas, not in Spark DataFrames. This code will result in an error.
spark_df.loc[spark_df("discount") < 0,:]:
Similar to the above, loc is not applicable to Spark DataFrames. This code will result in an error.
SELECT * FROM spark_df WHERE discount < 0:
This is SQL syntax and cannot be directly used on a Spark DataFrame unless you register the DataFrame as a temporary view and use Spark SQL. For example:

spark_df.createOrReplaceTempView("spark_df")
filtered_df = spark.sql("SELECT * FROM spark_df WHERE discount < 0")
However, this is not the most straightforward way to filter rows in a Spark DataFrame.
Key Takeaway:
Use filter() or where() with a column expression (e.g., col("discount") < 0) to filter rows in a Spark DataFrame based on a condition.
ドメイン
Scaling ML Models

問題15
未回答
A data scientist is looking to efficiently fine-tune the hyperparameters of a scikit-learn model concurrently. They decide to leverage the Hyperopt library to assist with this process. Which tool within the Hyperopt library offers the ability to optimize hyperparameters in parallel?
Choose only ONE best answer.
fmin
Search Space
hp.quniform
正解
SparkTrials
Trials
全体的な説明
Correct Answer:
SparkTrials
Explanation:
The SparkTrials class in the Hyperopt library is specifically designed to enable parallel hyperparameter optimization in a distributed computing environment using Apache Spark. It allows you to distribute the evaluation of hyperparameter combinations across a cluster, significantly speeding up the tuning process.
Here’s how it works:
Define the search space and objective function.
Use SparkTrials to parallelize the evaluation of hyperparameter combinations.
Pass SparkTrials to the fmin function to perform distributed optimization.
Example:
from hyperopt import fmin, tpe, hp, SparkTrials
# Define the objective function
def objective(params):
    # Model training and evaluation logic
    return loss
# Define the search space
space = {
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'max_depth': hp.choice('max_depth', range(1, 10))
}
# Use SparkTrials for parallel optimization
spark_trials = SparkTrials()
# Run hyperparameter optimization
best_hyperparams = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,
    trials=spark_trials
)
In this example, SparkTrials enables the evaluation of hyperparameter combinations to be distributed across a Spark cluster, making the process more efficient.
Why Other Options Are Incorrect:
fmin:
The fmin function is used to perform hyperparameter optimization, but it does not inherently enable parallel execution. It relies on the trials argument (e.g., SparkTrials) to enable parallelism.
Search Space:
The search space defines the range of hyperparameters to explore but does not control parallel execution. It is specified using hp functions like hp.uniform or hp.choice.
hp.quniform:
This is a function used to define a quantized uniform distribution in the search space. It does not enable parallel execution.
Key Takeaway:
To enable parallel hyperparameter optimization in Hyperopt, use the SparkTrials class. It integrates with Apache Spark to distribute the evaluation of hyperparameter combinations across a cluster, making the tuning process faster and more efficient.
ドメイン
Spark ML

問題16
未回答
A data scientist has developed a Python function titled 'generate_features'. This function produces a Spark DataFrame with two columns: 'Customer INT' and 'Region STRING'.
The output DataFrame is stored in a variable called 'feature_set'. The next objective for the scientist is to form a Feature Store table utilizing 'feature_set'.
What is the correct snippet of code that the data scientist can use to initiate and populate the Feature Store table using the Feature Store Client 'feature_client'?

feature_client.create_table
(
function='generate_features',
schema=feature_set.schema,
description='Client features'
)

 feature_client.create_table
(
name='new_feature_table',
primary_keys='customer_id',
schema=feature_set.schema,
description='Client features'
)
正解
feature_client.create_table
(
name='new_feature_table',
primary_keys='customer_id',
df=feature_set,
schema=feature_set.schema,
description='Client features'
)
feature_set.write.mode('feature').saveAsTable('new_feature_table')
feature_set.write('fs').saveAs('new_feature_table')
全体的な説明
Correct Answer:
feature_client.create_table
(
name='new_feature_table',
primary_keys='customer_id',
df=feature_set,
schema=feature_set.schema,
description='Client features'
)
Explanation:
To create and populate a Feature Store table using the Feature Store Client (feature_client), you need to specify:
The name of the table (name='new_feature_table').
The primary key(s) (primary_keys='customer_id').
The Spark DataFrame containing the data (df=feature_set).
The schema of the DataFrame (schema=feature_set.schema).
A description of the table (description='Client features').
The correct code snippet is:

feature_client.create_table(
    name='new_feature_table',
    primary_keys='customer_id',
    df=feature_set,
    schema=feature_set.schema,
    description='Client features'
)
This code creates a new Feature Store table named new_feature_table with customer_id as the primary key, populates it with the data from feature_set, and provides a description.
Why Other Options Are Incorrect:
feature_client.create_table(function='generate_features', schema=feature_set.schema, description='Client features'):
The function argument is not valid for creating a Feature Store table.
The primary_keys and df arguments are missing, which are required to define the table and populate it with data.

feature_client.create_table(name='new_feature_table', primary_keys='customer_id', schema=feature_set.schema, description='Client features'):
The df argument is missing, so the table will not be populated with data from feature_set.

feature_set.write.mode('feature').saveAsTable('new_feature_table'):
This is not a valid way to create a Feature Store table. The write method does not support the mode('feature') argument, and saveAsTable is used for saving to a Spark SQL table, not a Feature Store table.

feature_set.write('fs').save:
This is not a valid syntax for saving a DataFrame to a Feature Store table. The correct approach is to use the Feature Store Client's create_table method.
Key Takeaway:
To create and populate a Feature Store table, use the create_table method of the Feature Store Client with the required arguments: name, primary_keys, df, schema, and description.
ドメイン
Databricks ML

問題17
未回答
A data scientist has crafted a feature engineering notebook that leverages the pandas library. As the volume of data processed by the notebook grows, the runtime significantly escalates and the processing speed decreases proportionally with the size of the included data. What tool can the data scientist adopt to minimize the time spent refactoring their notebook to scale with big data?
Feature Store
PySpark DataFrame API
Spark SQL
Scala Dataset API
正解
pandas API on Spark
全体的な説明
Correct Answer: pandas API on Spark
Explanation:
The pandas API on Spark (pyspark.pandas) allows data scientists to scale up Pandas code to handle large datasets without major code refactoring.
Problem:
Pandas works well for small to medium-sized datasets but becomes slow and memory-intensive as data grows.
Since Pandas operates on a single node, performance deteriorates when handling big data.
Solution:
pandas API on Spark (pyspark.pandas) enables Pandas-like operations on distributed Spark clusters.
It minimizes the effort required to refactor Pandas code, making it easy to transition from Pandas to Spark without rewriting the entire logic.
Example: Converting a Pandas Notebook to Scale with Big Data
import pyspark.pandas as ps  # Import pandas API on Spark
# Load data using pandas API on Spark
df = ps.read_csv("large_dataset.csv")
# Perform operations using familiar pandas syntax
df["new_col"] = df["existing_col"] * 2
df_filtered = df[df["value"] > 100]
# Convert to Spark DataFrame if needed for further processing
spark_df = df.to_spark()
spark_df.show()
The same Pandas operations now run on a distributed Spark cluster.
No need to rewrite code using PySpark DataFrame API manually.
Why Other Options Are Incorrect?
Feature Store
Incorrect because Feature Store is for feature management and model versioning, not for optimizing Pandas workflows.
It does not directly improve processing speed for large datasets.
PySpark DataFrame API
Partially correct but not the best answer.
While PySpark DataFrames provide efficient distributed processing, converting a Pandas notebook to PySpark requires significant refactoring.
The pandas API on Spark minimizes this effort while still leveraging Spark.
Spark SQL
Incorrect because Spark SQL is used for querying structured data using SQL syntax.
While useful for data processing, it does not help directly in converting Pandas-based notebooks to Spark.
Scala Dataset API
Incorrect because the Scala Dataset API is for Spark applications written in Scala, not Python.
This would require a complete language change, which is unnecessary when using pandas API on Spark.
Final Answer:
✅ pandas API on Spark (pyspark.pandas) is the best tool for scaling Pandas workflows with minimal refactoring.
ドメイン
Spark ML

問題18
未回答
When converting a pandas-on-Spark DataFrame to a PySpark DataFrame, how can type casting be performed for specific columns using the astype method?
Type casting is not supported in pandas-on-Spark.
Use the astype method on the entire DataFrame.
Type casting is automatic; no additional steps are needed.
正解
Apply astype to specific columns using psdf['column_name'].astype('desired_type').
全体的な説明
Correct Answer:

Apply astype to specific columns using psdf['column_name'].astype('desired_type').
Explanation:
In pandas-on-Spark (Koalas), you can perform type casting for specific columns using the astype method. This method allows you to explicitly convert the data type of a column to a desired type. For example, if you want to convert a column to an integer type, you can use psdf['column_name'].astype('int').
Example:

import databricks.koalas as ks
import pandas as pd
# Create a pandas DataFrame
pandas_df = pd.DataFrame({
    'A': [1.1, 2.2, 3.3],
    'B': [4.4, 5.5, 6.6]
})
# Convert to pandas-on-Spark DataFrame
psdf = ks.from_pandas(pandas_df)
# Perform type casting on specific columns
psdf['A'] = psdf['A'].astype('int')  # Convert column 'A' to integer type
psdf['B'] = psdf['B'].astype('str')  # Convert column 'B' to string type
print(psdf)
Output:

   A    B
0  1  4.4
1  2  5.5
2  3  6.6
In this example, column A is cast to an integer type, and column B is cast to a string type.
Why Other Options Are Incorrect:
Type casting is not supported in pandas-on-Spark:
This is incorrect. Type casting is supported in pandas-on-Spark using the astype method.
Use the astype method on the entire DataFrame:
While you can use astype on the entire DataFrame, it is not necessary and may not be efficient if you only need to cast specific columns.
Type casting is automatic; no additional steps are needed:
Type casting is not automatic. You need to explicitly use the astype method to convert column types.
Key Takeaway:
To perform type casting for specific columns in a pandas-on-Spark DataFrame, use the astype method on the desired columns. This allows you to explicitly control the data types of individual columns.
ドメイン
Pandas API on Spark

問題19
未回答
What is the primary use case for mapInPandas() in Databricks?
Choose only ONE best answer.
Executing multiple models in parallel
正解
Applying a function to each partition of a DataFrame
Applying a function to grouped data within a DataFrame
Applying a function to co-grouped data from two DataFrames
全体的な説明
Correct Answer:
Applying a function to each partition of a DataFrame

Explanation:
The mapInPandas() function in Databricks is primarily used to apply a Python function to each partition of a Spark DataFrame. It allows you to leverage pandas operations within a distributed Spark environment by processing each partition as a pandas DataFrame. This is particularly useful when you need to perform complex transformations or computations that are easier to express using pandas.

Example:
from pyspark.sql import SparkSession
import pandas as pd
# Initialize Spark session
spark = SparkSession.builder.appName("mapInPandasExample").getOrCreate()
# Create a Spark DataFrame
data = [(1, 10), (2, 20), (3, 30), (4, 40)]
columns = ["id", "value"]
spark_df = spark.createDataFrame(data, columns)
# Define a function to apply to each partition
def transform_pandas(df: pd.DataFrame) -> pd.DataFrame:
    df['value'] = df['value'] * 2  # Double the 'value' column
    return df
# Apply the function to each partition using mapInPandas
result_df = spark_df.mapInPandas(transform_pandas, schema=spark_df.schema)
result_df.show()
Output:
+---+-----+
| id|value|
+---+-----+
|  1|   20|
|  2|   40|
|  3|   60|
|  4|   80|
+---+-----+
In this example, the transform_pandas function is applied to each partition of the Spark DataFrame, doubling the values in the value column.
Why Other Options Are Incorrect:
Executing multiple models in parallel:
While mapInPandas() can be used to parallelize computations, its primary purpose is not to execute multiple models in parallel. It is designed for applying functions to partitions of a DataFrame.
Applying a function to grouped data within a DataFrame:
For grouped data, you would typically use groupBy() followed by applyInPandas() or similar methods, not mapInPandas().
Applying a function to co-grouped data from two DataFrames:
Co-grouping and applying functions to data from two DataFrames is typically handled using cogroup() or join() operations, not mapInPandas().
Key Takeaway:
The primary use case for mapInPandas() is to apply a function to each partition of a Spark DataFrame, enabling pandas-like operations within a distributed Spark environment.
ドメイン
Spark ML

問題20
未回答
How can you install the databricks-feature-store client in a local Python environment?
正解
pip install databricks-feature-store
%pip install databricks-feature-store
Conda install databricks-feature-store
spark install databricks-feature-store
全体的な説明
Correct Answer:
pip install databricks-feature-store
Explanation:
To install the databricks-feature-store client in a local Python environment, you can use the pip package manager. The correct command is:

pip install databricks-feature-store
This command downloads and installs the databricks-feature-store package from the Python Package Index (PyPI) and makes it available for use in your local Python environment.
Why Other Options Are Incorrect:
%pip install databricks-feature-store:
The %pip magic command is used in Jupyter notebooks or Databricks notebooks to install packages. It is not used in a local Python environment outside of these contexts.
Conda install databricks-feature-store:
While conda is a valid package manager for Python, the correct syntax for installing a package with conda is conda install <package_name>. However, the databricks-feature-store package is not typically available via conda, so pip is the preferred method.
spark install databricks-feature-store:
There is no spark install command. Spark is a distributed computing framework and does not handle Python package installations.
Key Takeaway:
To install the databricks-feature-store client in a local Python environment, use the pip install databricks-feature-store command. This ensures the package is installed and ready for use in your local development setup.
ドメイン
Feature Store

問題21
未回答
A data scientist has developed three new models for a singular machine learning problem, replacing a solution that previously used a single model. All four models have roughly identical prediction latency. However, a machine learning engineer suggests that the new solution will be less time efficient during inference.
Under what circumstances would the engineer's observation be correct?
Choose only ONE best answer.
When the average size of the new solution's models exceeds the size of the original model
正解
When the new solution necessitates each model to compute a prediction forevery record
When the new solution's models have an average latency that is larger than the latency of the original model
When the new solution involves if-else logic determining which model to employ for each prediction
When the new solution requires fewer feature variables than the original model
全体的な説明
Correct Answer:
✅ When the new solution necessitates each model to compute a prediction for every record
Explanation:
If the new solution requires all three models to compute a prediction for every record, then:
Inference time increases because all models need to run for each input.
The system must aggregate or ensemble predictions, adding additional computation.
Even though individual model latency is unchanged, the overall inference time triples (or more), making it less efficient.
Example: Multi-Model Prediction Inefficiency
# Running three models for every input increases inference time
def ensemble_prediction(input_data):
    pred1 = model1.predict(input_data)  # Model 1 inference
    pred2 = model2.predict(input_data)  # Model 2 inference
    pred3 = model3.predict(input_data)  # Model 3 inference
    return (pred1 + pred2 + pred3) / 3  # Average prediction
Here, each record must pass through three models, slowing down inference.
Why Other Options Are Incorrect?
"When the average size of the new solution's models exceeds the size of the original model"
Incorrect, because model size affects storage and memory usage, not inference speed.
Larger models can slow down inference only if they exceed available RAM or VRAM.
"When the new solution's models have an average latency that is larger than the latency of the original model"
Incorrect, because the question states that all models have similar latencies.
If latency per model is the same, then adding more models per inference is what makes it inefficient.
"When the new solution involves if-else logic determining which model to employ for each prediction"
Incorrect, because if-else logic is selective, meaning only one model runs per input.
This is more efficient than running all models for every record.
"When the new solution requires fewer feature variables than the original model"
Incorrect, because reducing the number of features typically reduces computation time, improving efficiency.
Final Answer:
✅ The engineer is correct when the new solution necessitates each model to compute a prediction for every record, making inference less efficient.
ドメイン
Pandas API on Spark

問題22
未回答
Which of the following is a primary benefit of having Apache Arrow insidePandas API on Spark?
Choose only ONE best answer.
正解
Arrow allows for efficient data transfer between JVM and Python processes.
Arrow automatically optimizes Spark SQL queries.
Arrow enables the use of non-columnar data formats.
Arrow performs faster joins between DataFrames.
全体的な説明
Correct Answer:
✅ Arrow allows for efficient data transfer between JVM and Python processes.
Explanation:
Apache Arrow is a key component of the pandas API on Spark (pyspark.pandas) because it enables zero-copy, high-performance data exchange between Python (PySpark) and the JVM (Spark engine).
In traditional PySpark, data transfer between Spark (JVM-based) and Python (PyData ecosystem) involves serialization and deserialization (pickling/unpickling), which slows down operations.
Apache Arrow eliminates this overhead by using a shared memory format, allowing fast, zero-copy communication between JVM and Python.
This significantly improves performance when handling large DataFrames in pandas API on Spark.
Example: Enabling Apache Arrow in PySpark
from pyspark.sql import SparkSession
# Enable Apache Arrow for Pandas UDFs
spark = SparkSession.builder.config("spark.sql.execution.arrow.pyspark.enabled", "true").getOrCreate()
By enabling Arrow, operations like .toPandas() and mapInPandas() run significantly faster.
Why Other Options Are Incorrect?
"Arrow automatically optimizes Spark SQL queries."
Incorrect because Apache Arrow does not optimize Spark SQL queries.
Arrow optimizes data transfer but does not improve query execution plans (this is done by Catalyst Optimizer in Spark).
"Arrow enables the use of non-columnar data formats."
Incorrect because Apache Arrow is a columnar in-memory format, designed for fast vectorized operations.
It does not support row-based data formats.
"Arrow performs faster joins between DataFrames."
Incorrect because Arrow does not speed up Spark joins directly.
Joins are optimized by Spark's Catalyst Optimizer and Tungsten Engine, not Apache Arrow.
Final Answer:
✅ Apache Arrow enables efficient data transfer between JVM and Python processes, significantly improving pandas API on Spark performance.
ドメイン
Spark ML

問題23
未回答
When using hp.choice() in Hyperopt, what does Hyperopt return, and how can you retrieve the actual parameter values?
Hyperopt returns the actual parameter values
正解
Hyperopt returns the index of the choice list, and hyperopt.space_eval() is used to retrieve the parameter values
Hyperopt returns a dictionary of parameter values
Hyperopt returns the index of the choice list, and the parameter values cannot be retrieved
全体的な説明
Correct Answer:
Hyperopt returns the index of the choice list, and hyperopt.space_eval() is used to retrieve the parameter values
Explanation:
When using hp.choice() in Hyperopt:
Hyperopt returns the index of the selected value in the choice list (e.g., 0 for the first item, 1 for the second, etc.).
To retrieve the actual parameter value corresponding to the index, use hyperopt.space_eval(). This function maps the index back to the original value in the search space.
Example:
from hyperopt import hp, fmin, tpe, Trials, space_eval
# Define search space with hp.choice
space = {
    'model': hp.choice('model', ['svm', 'rf', 'xgboost']),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.1)
}
# Optimization
trials = Trials()
best = fmin(fn=lambda params: 0.5,  # Dummy objective function
            space=space,
            algo=tpe.suggest,
            max_evals=1,
            trials=trials)
print("Best index:", best)  # Output: {'model': 1, 'learning_rate': 0.05}
# Retrieve actual parameter values
actual_params = space_eval(space, best)
print("Actual params:", actual_params)  # Output: {'model': 'rf', 'learning_rate': 0.05}
Why Other Options Are Incorrect:
"Hyperopt returns the actual parameter values":
Incorrect. hp.choice() returns indices, not the actual values.
"Hyperopt returns a dictionary of parameter values":
While Hyperopt returns a dictionary, the values for hp.choice() entries are indices, not the actual values.
"Hyperopt returns the index of the choice list, and the parameter values cannot be retrieved":
Incorrect. space_eval() exists specifically to convert indices to actual values.
Key Takeaway:
Use hyperopt.space_eval() to map indices returned by hp.choice() to their actual values in the search space.
ドメイン
Hyperopt

問題24
未回答
A data science team is working on a collaborative project in Databricks, and they need to version control their notebooks and track changes.
What is the recommended approach for implementing version control in Databricks notebooks?
Utilize Databricks Jobs to create snapshots of notebooks at regular intervals.
Export and commit notebooks to a version control system (e.g., Git) outside of Databricks.
Use Databricks MLflow Tracking to log notebook versions automatically.
正解
Enable the built-in version control feature in Databricks notebooks.
全体的な説明
Correct Answer:
✅ Enable the built-in version control feature in Databricks notebooks.
Explanation:
Databricks provides a built-in version control system that allows users to track changes, revert to previous versions, and collaborate efficiently within the notebook environment.
This feature includes:
Automatic versioning (Databricks saves previous notebook versions).
Revert functionality (restore older versions if needed).
Integration with Git (GitHub, GitLab, Azure DevOps).
How to Enable Version Control in Databricks Notebooks:
Open a Databricks notebook.
Click on the "Revision History" button (top-right corner).
View and restore previous versions as needed.
For Git integration, navigate to:
Repos → Add a remote Git repository (GitHub, GitLab, Bitbucket, or Azure DevOps).
Sync changes directly with the Git repository.
Example: Using Databricks Repos (Git Integration)
# Clone a Git repository into Databricks
databricks repos create --url https://github.com/user/project --provider gitHub --path /Repos/project
Why Other Options Are Incorrect?
"Utilize Databricks Jobs to create snapshots of notebooks at regular intervals."
Incorrect because Databricks Jobs are meant for task scheduling, not version control.
Jobs do not track changes over time or allow rollbacks.
"Export and commit notebooks to a version control system (e.g., Git) outside of Databricks."
Partially correct, but not the best approach.
Manually exporting notebooks for Git versioning is inefficient when Databricks has built-in Git integration via Databricks Repos.
"Use Databricks MLflow Tracking to log notebook versions automatically."
Incorrect because MLflow Tracking is for model versioning and experiment tracking, not notebook version control.
MLflow logs experiment metadata, but it does not track notebook edits.
Final Answer:
✅ Enable the built-in version control feature in Databricks notebooks to efficiently track changes and integrate with Git repositories.

問題25
未回答
In which of the following scenarios should you put the CrossValidator inside the Pipeline?
Choose only ONE best answer.
When there are estimators or transformers in the pipeline
正解
When there is a risk of data leakage from earlier steps in the pipeline
When you want to refit in the pipeline
When you want to train models in parallel
全体的な説明
Correct Answer:
When there is a risk of data leakage from earlier steps in the pipeline
Explanation:
Placing the CrossValidator inside the Pipeline is essential when there is a risk of data leakage from earlier steps in the pipeline. Data leakage occurs when information from the validation or test set inadvertently influences the training process, leading to overly optimistic performance estimates. By including the CrossValidator in the pipeline, you ensure that all preprocessing steps (e.g., feature scaling, imputation) are applied correctly during cross-validation, preventing leakage.
For example:
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.feature import StandardScaler
# Define preprocessing steps
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
# Define model
lr = LogisticRegression(featuresCol="scaled_features")
# Create pipeline
pipeline = Pipeline(stages=[scaler, lr])
# Define parameter grid
param_grid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \
    .build()
# Define CrossValidator
cross_validator = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=param_grid,
    evaluator=BinaryClassificationEvaluator(),
    numFolds=5
)
# Fit the CrossValidator
cv_model = cross_validator.fit(train_data)
In this example, the CrossValidator is inside the pipeline, ensuring that the StandardScaler is applied correctly during cross-validation to prevent data leakage.
Why Other Options Are Incorrect:
When there are estimators or transformers in the pipeline:
While estimators and transformers are part of the pipeline, this is not the primary reason for placing the CrossValidator inside the pipeline. The key reason is to prevent data leakage.
When you want to refit in the pipeline:
Refitting is a general step in model training and does not specifically require the CrossValidator to be inside the pipeline.
When you want to train models in parallel:
Parallel training is handled by Spark's distributed computing capabilities and does not depend on the placement of the CrossValidator in the pipeline.
Key Takeaway:
Place the CrossValidator inside the Pipeline to prevent data leakage from earlier steps, ensuring that preprocessing is applied correctly during cross-validation.
ドメイン
ML workflows

問題26
未回答
In Databricks MLflow, you have retrieved the most recent run from an experiment using the
MLflow client. runs = client.search_runs(experiment_id,
order_by=[""attributes.start_time desc""], max_results=1)),
How can you access the metrics of this best run?

Choose only ONE best answer.
正解
metrics = runs[0].data.metrics
metrics = runs[0].get_metrics()
metrics = runs[0].fetch_metrics()
metrics = runs[0].metrics.data
全体的な説明
Correct Answer:
metrics = runs[0].data.metrics
Explanation:
In Databricks MLflow, after retrieving the most recent run using the search_runs method, you can access the metrics of the run using the data.metrics attribute of the run object. The data attribute contains all the logged data (metrics, parameters, and tags), and metrics is a dictionary of the logged metrics.
Example:

from mlflow.tracking import MlflowClient
# Initialize MLflow client
client = MlflowClient()
# Retrieve the most recent run
runs = client.search_runs(experiment_id, order_by=["attributes.start_time desc"], max_results=1)
# Access metrics of the most recent run
metrics = runs[0].data.metrics
print(metrics)
In this example, runs[0].data.metrics returns a dictionary of the metrics logged during the run.
Why Other Options Are Incorrect:
metrics = runs[0].get_metrics():
There is no get_metrics() method in the MLflow run object. The correct way to access metrics is through data.metrics.
metrics = runs[0].fetch_metrics():
There is no fetch_metrics() method in the MLflow run object. The correct way to access metrics is through data.metrics.
metrics = runs[0].metrics.data:
The metrics attribute does not have a data sub-attribute. The correct way to access metrics is through data.metrics.
Key Takeaway:
To access the metrics of a run in MLflow, use the data.metrics attribute of the run object. This provides a dictionary of the logged metrics.
ドメイン
Databricks ML

問題27
未回答
A data scientist is working on a collaborative project in Databricks, and multiple team members are contributing to a shared notebook. One team member accidentally deleted a critical cell in the notebook. How can the team recover the deleted cell?
正解
Use the notebook's version history to revert to a previous version.
Request the team member to re-run the deleted cell.
Export the notebook, find the deleted code, and re-import it.
Use the Databricks Trash feature to recover the deleted cell.
全体的な説明
Correct Answer:
Use the notebook's version history to revert to a previous version.
Explanation:
Databricks provides a version history feature for notebooks, which allows users to track changes and revert to a previous version if needed. If a critical cell is accidentally deleted, the team can:
Open the notebook in Databricks.
Click on the "Revision History" button (clock icon) in the notebook toolbar.
Browse through the saved versions of the notebook.
Select a previous version where the deleted cell was present and restore it.
This feature ensures that changes to notebooks are tracked, and accidental deletions or modifications can be easily undone.
Why Other Options Are Incorrect:
Request the team member to re-run the deleted cell:
If the cell was deleted, re-running it is not possible unless the code is recreated manually. This is not an efficient or reliable solution.
Export the notebook, find the deleted code, and re-import it:
Exporting and re-importing the notebook is unnecessary when Databricks provides a built-in version history feature for recovering deleted content.
Use the Databricks Trash feature to recover the deleted cell:
The Databricks Trash feature is used to recover deleted notebooks or files, not individual cells within a notebook. The version history is the correct tool for recovering deleted cells.
Key Takeaway:
To recover a deleted cell in a Databricks notebook, use the version history feature to revert to a previous version of the notebook where the cell was present.
ドメイン
Databricks ML

問題28
未回答
Which feature of Databricks Jobs allows the orchestration of tasks based on external events or triggers?
正解
Event-driven scheduling
Time-based scheduling
Dependency-based scheduling
Task-based scheduling
全体的な説明
Correct Answer:
Event-driven scheduling
Explanation:
Event-driven scheduling in Databricks Jobs allows tasks to be orchestrated based on external events or triggers. This feature enables workflows to be initiated automatically in response to specific events, such as the arrival of new data in a cloud storage bucket (e.g., AWS S3, Azure Blob Storage) or the completion of another job. Event-driven scheduling is particularly useful for building real-time or near-real-time data pipelines.
For example:
A Databricks Job can be triggered when a new file is uploaded to an S3 bucket.
A job can be initiated when a specific message is published to a message queue (e.g., Kafka, Azure Event Hubs).
This feature integrates with cloud-native event systems (e.g., AWS EventBridge, Azure Event Grid) to enable seamless automation.
Why Other Options Are Incorrect:
Time-based scheduling:
Time-based scheduling allows jobs to be run at specific times or intervals (e.g., daily, hourly). It does not respond to external events.
Dependency-based scheduling:
Dependency-based scheduling ensures tasks are executed in a specific order based on dependencies between them. It does not involve external triggers.
Task-based scheduling:
Task-based scheduling refers to the execution of tasks within a job but does not involve external event triggers.
Key Takeaway:
Event-driven scheduling in Databricks Jobs enables the orchestration of tasks based on external events or triggers, making it ideal for real-time or event-based workflows.
ドメイン
Databricks ML

問題29
未回答
Which among the following tools can be utilized to enable a Bayesian hyperparameter tuning procedure for distributed Spark ML machine learning models?
Choose only ONE best answer.
正解
Hyperopt
Autoscaling clusters
Feature Store
MLflow Experiment Tracking
AutoML
全体的な説明
Correct Answer:
Hyperopt
Explanation:
Hyperopt is a Python library designed for distributed hyperparameter optimization, including Bayesian optimization. It supports parallel tuning of machine learning models and integrates well with Apache Spark through the SparkTrials class, which enables distributed hyperparameter tuning across a Spark cluster.
Key features of Hyperopt:
Bayesian optimization: Uses the Tree-structured Parzen Estimator (TPE) algorithm to intelligently search the hyperparameter space.
Distributed tuning: Leverages SparkTrials to parallelize hyperparameter evaluations across a cluster.
Scalability: Efficiently handles large-scale hyperparameter tuning tasks.
Example of using Hyperopt with SparkTrials:
from hyperopt import fmin, tpe, hp, SparkTrials
# Define the objective function
def objective(params):
    # Model training and evaluation logic
    return loss
# Define the search space
space = {
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'max_depth': hp.choice('max_depth', range(1, 10))
}
# Use SparkTrials for distributed tuning
spark_trials = SparkTrials()
# Run hyperparameter optimization
best_hyperparams = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,
    trials=spark_trials
)
In this example, Hyperopt with SparkTrials enables distributed Bayesian hyperparameter tuning for Spark ML models.
Why Other Options Are Incorrect:
Autoscaling clusters:
Autoscaling clusters optimize resource allocation but do not directly support hyperparameter tuning.
Feature Store:
The Feature Store is used for managing and sharing features across machine learning workflows but does not provide hyperparameter tuning capabilities.
MLflow Experiment Tracking:
MLflow tracks experiments, parameters, and metrics but does not perform hyperparameter optimization.
AutoML:
AutoML automates the end-to-end machine learning process but is not specifically designed for distributed Bayesian hyperparameter tuning like Hyperopt.
Key Takeaway:
Hyperopt is the tool of choice for enabling Bayesian hyperparameter tuning for distributed Spark ML models, especially when combined with SparkTrials for parallel execution.
ドメイン
Hyperopt

問題30
未回答
What role does the SparkTrials class play in Hyperopt, and when should it be used?
It defines the hyperparameter space for distributed models
正解
It accelerates single-machine tuning by distributing trials to Spark workers
It logs tuning results to MLflow
It executes distributed ML algorithms such as MLlib or Horovod
全体的な説明
Correct Answer:
It accelerates single-machine tuning by distributing trials to Spark workers
Explanation:
The SparkTrials class in Hyperopt is designed to accelerate hyperparameter tuning by distributing the evaluation of trials across a Spark cluster. Instead of running trials sequentially on a single machine, SparkTrials parallelizes the process, allowing multiple trials to be executed simultaneously on Spark workers. This significantly speeds up the tuning process for large-scale machine learning models.
Key features of SparkTrials:
Distributed execution: Evaluates multiple hyperparameter combinations in parallel across a Spark cluster.
Scalability: Handles large-scale tuning tasks efficiently by leveraging Spark's distributed computing capabilities.
Integration with Hyperopt: Works seamlessly with Hyperopt's fmin function to optimize hyperparameters.
Example:
from hyperopt import fmin, tpe, hp, SparkTrials
# Define the objective function
def objective(params):
    # Model training and evaluation logic
    return loss
# Define the search space
space = {
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'max_depth': hp.choice('max_depth', range(1, 10))
}
# Use SparkTrials for distributed tuning
spark_trials = SparkTrials()
# Run hyperparameter optimization
best_hyperparams = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,
    trials=spark_trials
)
In this example, SparkTrials distributes the evaluation of hyperparameter combinations across the Spark cluster, accelerating the tuning process.
Why Other Options Are Incorrect:
It defines the hyperparameter space for distributed models:
The hyperparameter space is defined using Hyperopt's hp functions (e.g., hp.choice, hp.uniform), not SparkTrials.
It logs tuning results to MLflow:
While MLflow can be used to track tuning results, SparkTrials itself does not handle logging. MLflow integration is separate.
It executes distributed ML algorithms such as MLlib or Horovod:
SparkTrials is specific to Hyperopt and does not execute distributed ML algorithms like MLlib or Horovod. It focuses on distributing hyperparameter tuning trials.
Key Takeaway:
SparkTrials accelerates hyperparameter tuning by distributing trials across a Spark cluster, making it ideal for large-scale optimization tasks.
ドメイン
Hyperopt

問題31
未回答
Your team is implementing a distributed machine learning model that requires iterative processing. What Spark feature allows caching of intermediate data in memory for faster iterative computations?
Spark SQL
Spark MLlib
Spark GraphX
正解
Spark RDD Persistence
全体的な説明
Correct Answer:
Spark RDD Persistence
Explanation:
Spark RDD Persistence (caching) is the feature that allows intermediate data to be stored in memory (or on disk) for faster access during iterative computations. By caching RDDs (Resilient Distributed Datasets), you avoid recomputing the same data across multiple iterations, which significantly improves performance for iterative algorithms like those used in machine learning.
Key points about RDD Persistence:
Caching levels: You can specify the storage level (e.g., MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY) based on your memory and performance requirements.
Iterative algorithms: Caching is particularly useful for iterative algorithms (e.g., gradient descent, k-means clustering) where the same data is reused across iterations.
Manual control: You explicitly call .persist() or .cache() on an RDD to enable persistence.
Example:
from pyspark import SparkContext
from pyspark.storagelevel import StorageLevel
# Initialize SparkContext
sc = SparkContext("local", "RDD Persistence Example")
# Create an RDD
data = sc.parallelize(range(1000000))
# Cache the RDD in memory
data.persist(StorageLevel.MEMORY_ONLY)
# Perform iterative computations
for i in range(10):
    result = data.map(lambda x: x * i).reduce(lambda a, b: a + b)
    print(f"Iteration {i}: {result}")
# Unpersist the RDD when done
data.unpersist()
In this example, the RDD is cached in memory to avoid recomputation during iterative processing.
Why Other Options Are Incorrect:
Spark SQL:
Spark SQL is used for querying structured data using SQL syntax. It does not directly provide caching for iterative computations.
Spark MLlib:
Spark MLlib is a machine learning library that leverages RDD persistence internally but does not directly provide the caching feature.
Spark GraphX:
Spark GraphX is a library for graph processing. While it may use RDD persistence internally, it is not the primary feature for caching intermediate data in iterative computations.
Key Takeaway:
Spark RDD Persistence is the feature that enables caching of intermediate data in memory (or on disk) for faster iterative computations, making it essential for distributed machine learning models.
ドメイン
Scaling ML Models

問題32
未回答
A machine learning team wants to use the Python library newpackage on all of their projects. They share a cluster for all of their projects.
Which approach makes the Python library newpackage available to all notebooks run on a cluster?
Edit the cluster to use the Databricks Runtime for Machine Learning
Set the runtime-version variable in their Spark session to "ml"
Running %pip install newpackage once on any notebook attached to the cluster
正解
Adding /databricks/python/bin/pip install newpackage to the cluster’s bash init script
There is no way to make the newpackage library available on a cluster
全体的な説明
Correct Answer:
✅ Adding /databricks/python/bin/pip install newpackage to the cluster’s bash init script
Explanation:
To make the newpackage library available to all notebooks on a shared cluster, the best approach is to install it via the cluster’s init script.
Why?
Init scripts run automatically when the cluster starts, ensuring the package is installed for all users and notebooks.
This eliminates the need for reinstalling the package in each session.
Works across all users and all projects using the same cluster.
How to Add an Init Script to a Databricks Cluster
Navigate to Clusters in the Databricks workspace.
Select the cluster you want to modify.
Click Edit and scroll down to Advanced Options → Init Scripts.
Add the following command to install newpackage:
Example Init Script (Bash)
#!/bin/bash
/databricks/python/bin/pip install newpackage
Save the changes and restart the cluster.
The package will now be available in all notebooks attached to this cluster.
Why Other Options Are Incorrect?
"Edit the cluster to use the Databricks Runtime for Machine Learning"
Incorrect, because Databricks ML Runtime includes pre-installed ML libraries but does not automatically install custom packages like newpackage.
"Set the runtime-version variable in their Spark session to 'ml'"
Incorrect, because there is no runtime-version variable in Spark that controls package availability.
Databricks runtime selection happens at the cluster level, not in a notebook session.
"Running %pip install newpackage once on any notebook attached to the cluster"
Incorrect, because installing with %pip install only applies to the current session.
Once the cluster restarts, the package will be removed, requiring reinstallation.
"There is no way to make the newpackage library available on a cluster"
Incorrect, because Databricks provides multiple ways to install persistent libraries (e.g., init scripts, cluster libraries, workspace libraries).
Final Answer:
✅ Using an init script (/databricks/python/bin/pip install newpackage) ensures the package is installed persistently on the cluster for all users and notebooks.
ドメイン
Spark ML Basics

問題33
未回答
In Databricks AutoML, which default metric is used to evaluate the performance of regression models?

Mean squared error (MSE)
Mean absolute error (MAE)
正解
R-squared (R2)
Root mean squared error (RMSE)
全体的な説明
Correct Answer: R-squared (R2)
Explanation:
R-squared (R2):
Databricks AutoML uses R-squared (R2) as the default metric for evaluating regression models.
R2 represents the proportion of the variance in the dependent variable that is predictable from the independent variables.
It provides a measure of how well the model fits the data, with higher values indicating a better fit.
Why other options are incorrect:
Mean squared error (MSE):
While MSE is a common regression metric, it's not the default in Databricks AutoML.
Mean absolute error (MAE):
MAE is also a valid regression metric, but R2 is the default.
Root mean squared error (RMSE):
RMSE is another common regression metric, but again, R2 is the default.
Reference: Link
ドメイン
Databricks ML

問題34
未回答
A data scientist is working on a Databricks project involving natural language processing (NLP) tasks. They need to preprocess text data, including tokenization and removing stop words.
What Spark MLlib feature should they use for text preprocessing in a scalable manner?
StringIndexer
正しい選択
Tokenizer
正しい選択
StopWordsRemover
CountVectorizer
None of the Above
全体的な説明
Correct Answer:
Tokenizer and StopWordsRemover
Explanation:
Tokenizer:
Tokenization is the process of splitting text into individual words or tokens.
In Spark MLlib, the Tokenizer class is specifically designed for this purpose. It takes a text column as input and outputs a list of tokens (words).
Example:
from pyspark.ml.feature import Tokenizer
tokenizer = Tokenizer(inputCol="text", outputCol="words")
tokenized_data = tokenizer.transform(text_data)
tokenized_data.show(truncate=False)
StopWordsRemover:
After tokenization, the next step is often to remove stop words (common words like "the", "is", "and" that do not contribute much to the meaning of the text).
The StopWordsRemover class in Spark MLlib is used to remove these stop words from the tokenized text.
Example:
from pyspark.ml.feature import StopWordsRemover
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
filtered_data = remover.transform(tokenized_data)
filtered_data.show(truncate=False)
Why These Are Correct:
Tokenizer is essential for breaking down text into words, which is a fundamental step in NLP.
StopWordsRemover is used to clean the tokenized text by removing unnecessary words, improving the quality of the data for downstream tasks like feature extraction or modeling.
Why Other Options Are Incorrect:
StringIndexer:
The StringIndexer is used to convert categorical text data (like labels) into numerical indices.
It is not used for text preprocessing tasks like tokenization or stop word removal.
Example use case: Converting labels like "cat", "dog", "bird" into 0, 1, 2.
CountVectorizer:
The CountVectorizer is used to convert a collection of text documents into vectors of token counts.
It is used after tokenization and stop word removal, not for preprocessing tasks.
Example use case: Converting tokenized words into a feature vector for machine learning models.
Example Workflow in Databricks:
from pyspark.ml.feature import Tokenizer, StopWordsRemover
# Sample DataFrame
data = spark.createDataFrame([
    (0, "I love Databricks and machine learning"),
    (1, "Stop words are removed in NLP tasks")
], ["id", "text"])
# Tokenization
tokenizer = Tokenizer(inputCol="text", outputCol="words")
tokenized_data = tokenizer.transform(data)
# Stop Words Removal
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
filtered_data = remover.transform(tokenized_data)
# Show results
filtered_data.select("id", "text", "filtered_words").show(truncate=False)
Output:
+---+------------------------------------+----------------------------+
|id |text                                |filtered_words              |
+---+------------------------------------+----------------------------+
|0  |I love Databricks and machine learning|[love, databricks, machine, learning]|
|1  |Stop words are removed in NLP tasks |[stop, words, removed, nlp, tasks]|
+---+------------------------------------+----------------------------+
This workflow demonstrates how to use Tokenizer and StopWordsRemover for text preprocessing in a scalable manner using Spark MLlib in Databricks.

問題35
未回答
What is the purpose of the fmin() function in Hyperopt, and what are its essential arguments?
It logs tuning results to MLflow
It defines the hyperparameter space
正解
It executes a Hyperopt run and searches the hyperparameter space
It parallelizes computations for single-machine ML models
全体的な説明
Correct Answer:
It executes a Hyperopt run and searches the hyperparameter space
Explanation:
The fmin() function in Hyperopt is the core function used to execute a hyperparameter optimization run. It searches the hyperparameter space to find the combination of hyperparameters that minimizes the objective function. The essential arguments for fmin() include:
fn: The objective function to minimize. This function takes a set of hyperparameters as input and returns a loss value.
space: The hyperparameter search space, defined using Hyperopt's hp functions (e.g., hp.uniform, hp.choice).
algo: The search algorithm to use, such as tpe.suggest for Tree-structured Parzen Estimator (TPE) or rand.suggest for random search.
max_evals: The maximum number of evaluations (trials) to perform.
trials: An object (e.g., Trials or SparkTrials) to store the results of each evaluation.
Example:
from hyperopt import fmin, tpe, hp, Trials
# Define the objective function
def objective(params):
    # Model training and evaluation logic
    return loss
# Define the search space
space = {
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'max_depth': hp.choice('max_depth', range(1, 10))
}
# Run hyperparameter optimization
trials = Trials()
best_hyperparams = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,
    trials=trials
)
In this example, fmin() searches the hyperparameter space to find the best combination of hyperparameters that minimize the objective function.
Why Other Options Are Incorrect:
It logs tuning results to MLflow:
While MLflow can be used to log results, fmin() itself does not handle logging. MLflow integration is separate.
It defines the hyperparameter space:
The hyperparameter space is defined using Hyperopt's hp functions, not fmin().
It parallelizes computations for single-machine ML models:
Parallelization is handled by the trials argument (e.g., SparkTrials), not fmin() itself.
Key Takeaway:
The fmin() function in Hyperopt is used to execute a hyperparameter optimization run and search the hyperparameter space to find the best combination of hyperparameters.
ドメイン
Hyperopt

問題36
未回答
Utilizing MLflow Autologging, a data scientist is automatically monitoring their machine learning experiments. Once a series of experiment runs for the experiment_id are completed, the scientist intends to pinpoint the run exhibiting the best root-mean-square error (RMSE). To do so, they have initiated the
following incomplete code snippet:
mlflow._________(experiment_id, order_by = ["metrics.rmse"]) ["run_id"] [0]
What piece of code should the data scientist utilize in the blank space above to successfully complete the code block and identify the run with the best RMSE?
Choose only ONE best answer.
client
正解
search_runs
experiment
identify_run
show_runs
全体的な説明
Correct Answer:
search_runs
Explanation:
The search_runs() function in MLflow is used to retrieve a list of runs for a specific experiment, sorted by a specified metric. To identify the run with the best (lowest) RMSE, the data scientist should use search_runs() with the order_by argument set to ["metrics.rmse"]. This will return the runs sorted by RMSE in ascending order, allowing the first run in the list to be the one with the best RMSE.
Here’s the completed code snippet:
best_run_id = mlflow.search_runs(experiment_id, order_by=["metrics.rmse"])["run_id"][0]
Explanation of the code:
experiment_id: The ID of the experiment containing the runs.
order_by=["metrics.rmse"]: Sorts the runs by the RMSE metric in ascending order (lowest RMSE first).
["run_id"][0]: Retrieves the run_id of the first run in the sorted list, which corresponds to the run with the best RMSE.
Why Other Options Are Incorrect:
client:
The client object is used to interact with the MLflow Tracking Server but does not directly retrieve runs. The correct function is search_runs().
experiment:
The experiment object is used to manage experiments but does not retrieve or sort runs.
identify_run:
There is no identify_run function in MLflow. The correct function is search_runs().
show_runs:
There is no show_runs function in MLflow. The correct function is search_runs().
Key Takeaway:
To identify the run with the best RMSE in MLflow, use the search_runs() function with the order_by argument set to ["metrics.rmse"].
ドメイン
ML workflows

問題37
未回答
Your team is dealing with a dataset containing missing values in multiple features. Which technique, supported by Databricks MLlib, can help you handle missing values effectively during data preprocessing?
正解
Data Imputation
Feature Scaling
Outlier Detection
Feature Selection
全体的な説明
Correct Answer:
Data Imputation
Explanation:
Data Imputation is the technique used to handle missing values in a dataset. Databricks MLlib provides several methods for imputing missing values, such as:
Mean imputation: Replace missing values with the mean of the column.
Median imputation: Replace missing values with the median of the column.
Mode imputation: Replace missing values with the most frequent value in the column (for categorical data).
Custom imputation: Replace missing values with a user-defined value.
Example using MLlib's Imputer:

from pyspark.ml.feature import Imputer
# Create a sample DataFrame with missing values
data = [(1, 10.0), (2, None), (3, 30.0), (4, 40.0)]
columns = ["id", "value"]
df = spark.createDataFrame(data, columns)
# Initialize the Imputer
imputer = Imputer(inputCols=["value"], outputCols=["value_imputed"])
# Fit and transform the DataFrame
imputed_df = imputer.fit(df).transform(df)
imputed_df.show()
Output:

+---+-----+-------------+
| id|value|value_imputed|
+---+-----+-------------+
|  1| 10.0|         10.0|
|  2| null|         26.67|  # Mean of 10.0, 30.0, 40.0
|  3| 30.0|         30.0|
|  4| 40.0|         40.0|
+---+-----+-------------+
In this example, the Imputer replaces missing values in the value column with the mean of the non-missing values.
Why Other Options Are Incorrect:
Feature Scaling:
Feature scaling (e.g., normalization, standardization) is used to transform numerical features to a specific range or distribution but does not handle missing values.
Outlier Detection:
Outlier detection identifies and handles extreme values in the data but does not address missing values.
Feature Selection:
Feature selection involves selecting the most relevant features for modeling but does not handle missing values.
Key Takeaway:
Data Imputation is the technique supported by Databricks MLlib for handling missing values during data preprocessing. It ensures that missing values are replaced with appropriate estimates, making the dataset suitable for machine learning.
ドメイン
Data Preparation

問題38
未回答
A data scientist is trying to use Spark ML to fill in missing values in their PySpark DataFrame 'features_df'. They want to replace the missing values in all numeric columns in 'features_df' with the median value of each corresponding numeric column.
However, the code they have written does not perform the task correctly. Can you identify the reason why the code is not performing the imputation task as intended?
my_imputer = imputer
( strategy = "median",
inputCols = input_columns,
outputCols = output_columns
)
imputed_df = my_imputer.transform(features_df)
Imputing using a median value is not possible.
It does not simultaneously impute both the training and test datasets.
The 'inputCols' and 'outputCols' need to match exactly.
正解
The code fails to fit the imputer to the data to create an 'ImputerModel'.
The 'fit' method needs to be invoked instead of 'transform'
全体的な説明
Correct Answer:
The code fails to fit the imputer to the data to create an 'ImputerModel'.
Explanation:
The code is incorrect because it skips the fit() step, which is necessary to create an ImputerModel based on the input data. The Imputer must first be fitted to the data to calculate the median (or mean) values for the specified columns. Only after fitting can the transform() method be used to apply the imputation.
Here’s the corrected code:
from pyspark.ml.feature import Imputer
# Define input and output columns
input_columns = ["col1", "col2", "col3"]  # Replace with actual numeric column names
output_columns = ["col1_imputed", "col2_imputed", "col3_imputed"]
# Initialize the Imputer
my_imputer = Imputer(
    strategy="median",
    inputCols=input_columns,
    outputCols=output_columns
)
# Fit the Imputer to the data to create an ImputerModel
imputer_model = my_imputer.fit(features_df)
# Transform the DataFrame to impute missing values
imputed_df = imputer_model.transform(features_df)
Key steps:
fit(): Computes the median values for the specified columns and creates an ImputerModel.
transform(): Applies the imputation to the DataFrame using the computed median values.
Why Other Options Are Incorrect:
Imputing using a median value is not possible:
This is incorrect. The Imputer in Spark ML supports both mean and median strategies.
It does not simultaneously impute both the training and test datasets:
While it is good practice to apply the same imputation to both training and test datasets, this is not the reason the code fails. The issue is the missing fit() step.
The 'inputCols' and 'outputCols' need to match exactly:
The inputCols and outputCols do not need to match exactly. The outputCols are the names of the new columns containing the imputed values.
The 'fit' method needs to be invoked instead of 'transform':
This is partially correct. Both fit() and transform() are required, but the primary issue is the absence of the fit() step.
Key Takeaway:
To perform imputation in Spark ML, you must first fit the Imputer to the data to create an ImputerModel, and then use transform() to apply the imputation. Skipping the fit() step will result in an error or incorrect behavior.
ドメイン
ML workflow

問題39
未回答
How does Spark ML tackle a linear regression problem for an extraordinarily large dataset? Which one of the options is correct?
Choose only ONE best answer.
Brute Force Algorithm
Matrix decomposition
Singular value decomposition
Least square method
正解
Gradient descent
全体的な説明
Correct Answer:
✅ Gradient descent
Explanation:
For extraordinarily large datasets, Spark ML tackles linear regression problems using Gradient Descent (GD) instead of the traditional Least Squares Method.
Why Gradient Descent?
The Least Squares Method requires inverting a large matrix, which is computationally expensive and infeasible for big data.
Gradient Descent scales efficiently because it updates model weights iteratively instead of solving an explicit equation.
Spark ML implements Stochastic Gradient Descent (SGD) and L-BFGS (Limited-memory BFGS) for optimization.
Example: Linear Regression in Spark ML Using Gradient Descent
from pyspark.ml.regression import LinearRegression
from pyspark.sql import SparkSession
# Initialize Spark session
spark = SparkSession.builder.appName("LinearRegressionExample").getOrCreate()
# Load data
data = [(1, 2.0, 4.0), (2, 3.0, 6.0), (3, 4.0, 8.0)]
df = spark.createDataFrame(data, ["id", "feature", "label"])
# Train linear regression model using gradient descent
lr = LinearRegression(featuresCol="feature", labelCol="label", solver="gd")  # Gradient Descent
# Fit model
model = lr.fit(df)
print("Coefficients:", model.coefficients)
print("Intercept:", model.intercept)
solver="gd" ensures that gradient descent is used for optimization.
Spark also supports L-BFGS solver, which is an improvement over basic GD.
Why Other Options Are Incorrect?
Brute Force Algorithm
Incorrect, because brute force is not computationally feasible for large datasets.
Spark uses optimization algorithms like GD instead of exhaustive search.
Matrix Decomposition
Incorrect, because matrix decomposition methods (e.g., Cholesky, LU decomposition) require storing large matrices, which is impractical for large-scale ML.
Singular Value Decomposition (SVD)
Incorrect, because SVD is used for dimensionality reduction and PCA, not for training regression models efficiently.
Least Squares Method
Incorrect, because the Least Squares Method requires inverting a large matrix, which is computationally expensive for big data.
Spark avoids direct matrix inversion and uses Gradient Descent instead.
Final Answer:
✅ Spark ML tackles large-scale linear regression problems using Gradient Descent, making it scalable and efficient for big data processing.
ドメイン
Scaling ML Models

問題40
未回答
A senior machine learning engineer is developing a machine learning pipeline. They set up the pipeline to automatically transition a new version of a registered model to the Production stage in the Model Registry once it passes all tests using the MLflow Client API client.
Which operation was used to transition the model to the Production stage?
Client.update_model_stage
正解
client.transition_model_version_stage
client.transition_model_version
client.update_model_version
全体的な説明
Correct Answer:
client.transition_model_version_stage
Explanation:
The transition_model_version_stage method in the MLflow Client API is used to transition a model version to a specific stage (e.g., Staging, Production, Archived). This method allows you to move a model version to the Production stage after it has passed all tests and is ready for deployment.
Example:
from mlflow.tracking import MlflowClient
# Initialize MLflow client
client = MlflowClient()
# Transition model version to Production stage
client.transition_model_version_stage(
    name="my_model",
    version=1,
    stage="Production"
)
In this example, version 1 of the model named my_model is transitioned to the Production stage.
Why Other Options Are Incorrect:
Client.update_model_stage:
This method does not exist in the MLflow Client API.
client.transition_model_version:
This method does not exist in the MLflow Client API. The correct method is transition_model_version_stage.
client.update_model_version:
This method is used to update metadata (e.g., description) of a model version but does not transition it to a new stage.
Key Takeaway:
To transition a model version to the Production stage in the MLflow Model Registry, use the transition_model_version_stage method in the MLflow Client API.
ドメイン
Orchestrating Multi-task ML Workflows

問題41
未回答
What is the recommended approach for managing MLflow runs when using SparkTrials, and why?
Avoid using with mlflow.start_run() to prevent conflicts
Use a single MLflow run for multiple fmin() calls to save resources
正解
Wrap the call to fmin() inside with mlflow.start_run() to ensure separate MLflow main runs
Use a separate MLflow run for each trial to simplify logging
全体的な説明
Correct Answer:
Wrap the call to fmin() inside with mlflow.start_run() to ensure separate MLflow main runs
Explanation:
When using SparkTrials with Hyperopt, it is recommended to wrap the call to fmin() inside a with mlflow.start_run() block. This ensures that each fmin() call is associated with a separate MLflow main run, which helps organize and track the results of hyperparameter tuning experiments. Each trial within the fmin() call will be logged as a nested run under the main run, providing a clear hierarchy and making it easier to analyze the results.
Example:
import mlflow
from hyperopt import fmin, tpe, hp, SparkTrials
# Define the objective function
def objective(params):
    # Model training and evaluation logic
    return loss
# Define the search space
space = {
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'max_depth': hp.choice('max_depth', range(1, 10))
}
# Use SparkTrials for distributed tuning
spark_trials = SparkTrials()
# Wrap fmin() in an MLflow run
with mlflow.start_run():
    best_hyperparams = fmin(
        fn=objective,
        space=space,
        algo=tpe.suggest,
        max_evals=50,
        trials=spark_trials
    )
In this example:
The fmin() call is wrapped inside a with mlflow.start_run() block.
Each trial within fmin() is logged as a nested run under the main run.
Why Other Options Are Incorrect:
Avoid using with mlflow.start_run() to prevent conflicts:
This is incorrect. Using mlflow.start_run() is necessary to organize runs and avoid conflicts between different experiments.
Use a single MLflow run for multiple fmin() calls to save resources:
Using a single run for multiple fmin() calls can lead to confusion and make it difficult to track individual experiments. It is better to use separate runs for each fmin() call.
Use a separate MLflow run for each trial to simplify logging:
While this approach might seem logical, it is not practical because Hyperopt with SparkTrials automatically logs each trial as a nested run under the main run. Manually creating separate runs for each trial would be redundant and inefficient.
Key Takeaway:
To manage MLflow runs effectively when using SparkTrials, wrap the fmin() call inside a with mlflow.start_run() block. This ensures that each fmin() call is associated with a separate main run, and trials are logged as nested runs for better organization and tracking.
ドメイン
SparkTrials

問題42
未回答
After you Instantiate FeatureStoreClient as fs. What will be the format of primary_keys in the blank provided?
fs.create_table
(
name = table_name,
primary_keys = ______________,
schema = airbnb_df.schema,
description = "All Errors are captured in this table"
)
正解
["index"]
"index"
("index")
index
None of the above
全体的な説明
Correct Answer:
✅ ["index"]
Explanation:
In Databricks Feature Store, the primary_keys argument must be provided as a list of column names that uniquely identify each row in the feature table.
Since "index" is the primary key column, it must be passed as a list → ["index"].
Feature Store requires primary keys to be explicitly defined as a list, even if there is only one primary key.
Correct Implementation:
from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()
# Create a Feature Store Table with "index" as the primary key
fs.create_table(
    name="airbnb_features",
    primary_keys=["index"],  # Correct format
    schema=airbnb_df.schema,
    description="All Errors are captured in this table"
)
primary_keys=["index"] ensures that each record in airbnb_features is uniquely identified by the "index" column.
Why Other Options Are Incorrect?
"index"
Incorrect, because a string alone is not a valid format for primary_keys.
It must be wrapped in a list to indicate a collection of keys.
("index")
Incorrect, because ("index") is treated as a string, not a tuple or list.
The correct way to define a tuple would be ("index",) but Feature Store requires a list.
index
Incorrect, because index would be treated as a variable, which is undefined in this context.
None of the above
Incorrect, because ["index"] is the correct answer.
Final Answer:
✅ primary_keys=["index"] is the correct format when creating a Feature Store table in Databricks.
ドメイン
Databricks ML

問題43
未回答
When using SparkTrials, how is parallelism configured, and what is the trade-off associated with it?

It is configured automatically based on the number of Spark executors
It is set to the number of concurrent tasks allowed by the cluster configuration
It is determined by the number of trials in the hyperparameter space
正解
It is an optional argument with a trade-off between speed and adaptivity
全体的な説明
Correct Answer:
✅ It is an optional argument with a trade-off between speed and adaptivity
Explanation:
When using SparkTrials in Hyperopt, parallelism is set manually using the parallelism argument, which determines the number of hyperparameter trials that can run concurrently across Spark executors.
Parallelism Trade-Off:
Higher parallelism (e.g., parallelism=8)
✅ Speeds up tuning by evaluating multiple hyperparameter combinations simultaneously.
❌ Less adaptivity since new trials are launched before previous ones finish, which may lead to suboptimal parameter exploration.
Lower parallelism (e.g., parallelism=1)
✅ More adaptive because trials are evaluated sequentially, allowing better exploration based on past results.
❌ Slower tuning since trials are executed one after another.
Example: Setting Parallelism in SparkTrials
from hyperopt import fmin, tpe, hp, SparkTrials
from pyspark.sql import SparkSession
# Initialize Spark
spark = SparkSession.builder.appName("HyperoptSparkExample").getOrCreate()
# Define hyperparameter space
space = {
    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),
    'num_trees': hp.choice('num_trees', [50, 100, 200])
}
# Define objective function
def objective(params):
    loss = train_model_and_get_loss(params)  # Hypothetical function
    return loss
# Use SparkTrials with parallelism
trials = SparkTrials(parallelism=4)  # Runs 4 trials concurrently
# Execute hyperparameter optimization
best_params = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,  # Bayesian Optimization
    max_evals=50,
    trials=trials
)
print("Best Hyperparameters:", best_params)
parallelism=4 → 4 trials run at the same time.
If parallelism is too high, it reduces Bayesian optimization efficiency by evaluating too many trials in parallel without learning from previous results.
Why Other Options Are Incorrect?
"It is configured automatically based on the number of Spark executors"
Incorrect, because SparkTrials does not automatically determine parallelism based on executors.
Parallelism must be explicitly set using parallelism=n.
"It is set to the number of concurrent tasks allowed by the cluster configuration"
Incorrect, because cluster settings determine task execution limits, but parallelism in SparkTrials is manually configured by the user.
"It is determined by the number of trials in the hyperparameter space"
Incorrect, because the number of trials (max_evals) is separate from parallelism.
parallelism defines how many trials run simultaneously, not the total number of trials.
Final Answer:
✅ Parallelism in SparkTrials is an optional argument, balancing speed (more trials at once) and adaptivity (better exploration).
ドメイン
Hyperopt

問題44
未回答
A machine learning engineer is translating a decision tree from sklearn to Spark ML. During the training process, an error occurs stating that the maxBins parameter should be at least equal to the number of values in each categorical feature. What is the reason behind Spark ML requiring the maxBins parameter to be at least as large as the number of values in each categorical feature?
Choose only ONE best answer.
Spark ML requires more split candidates in the splitting algorithm than single-node implementations
正解
Spark ML requires at least one bin for each category in each categorical feature
Spark ML tests only categorical features in the splitting algorithm
Spark ML tests only numeric features in the splitting algorithm
全体的な説明
Correct Answer:
✅ Spark ML requires at least one bin for each category in each categorical feature
Explanation:
In Spark ML’s Decision Tree implementation, categorical features are binned into discrete bins, and each category must have at least one bin. The maxBins parameter controls how many bins are available for splitting categorical features.
If maxBins is too small (less than the number of unique values in a categorical feature), some categories cannot be represented, causing an error.
Solution: Set maxBins greater than or equal to the number of unique values in any categorical feature.
Example: Setting maxBins Correctly in Spark ML Decision Trees
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.sql import SparkSession
# Initialize Spark
spark = SparkSession.builder.appName("DecisionTreeExample").getOrCreate()
# Sample data with a categorical feature (3 unique categories)
data = [(1, "A", 0), (2, "B", 1), (3, "C", 0), (4, "A", 1)]
df = spark.createDataFrame(data, ["id", "category", "label"])
# Convert categorical feature into numerical index
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="category", outputCol="categoryIndex")
df = indexer.fit(df).transform(df)
# Decision Tree Model
dt = DecisionTreeClassifier(featuresCol="categoryIndex", labelCol="label", maxBins=3)  # Must be >= num categories
# Train the model
model = dt.fit(df)  # No error if maxBins >= num unique categories
If maxBins < 3, Spark throws an error because it cannot assign bins to all three categories.
If maxBins >= 3, Spark successfully bins each category.
Why Other Options Are Incorrect?
"Spark ML requires more split candidates in the splitting algorithm than single-node implementations"
Incorrect, because maxBins is not about split candidates, but about binning categorical features.
"Spark ML tests only categorical features in the splitting algorithm"
Incorrect, because Spark ML supports both categorical and numeric features in Decision Trees.
"Spark ML tests only numeric features in the splitting algorithm"
Incorrect, because Spark ML supports categorical features, and maxBins applies to categorical feature binning, not numeric features.
Final Answer:
✅ Spark ML requires at least one bin for each category in each categorical feature to properly process categorical data in Decision Trees.
ドメイン
Feature Store

問題45
未回答
A data scientist is working on a regression task in Databricks using Spark MLlib. They have computed predictions and true labels stored in the DataFrame regression_preds_df with the following schema:
prediction DOUBLE
label DOUBLE
Which of the following code blocks can be used to compute the mean absolute error (MAE) for the regression model?
mae_evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="label", metricName="mae") mae = mae_evaluator.evaluate(regression_preds_df)
mae_evaluator = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="mae") mae = mae_evaluator.evaluate(regression_preds_df)
mae_evaluator = BinaryClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="mae") mae = mae_evaluator.evaluate(regression_preds_df)
mae_evaluator = RegressionSummarizer(predictionCol="prediction", labelCol="label", metricName="mae") mae = mae_evaluator.evaluate(regression_preds_df)
正解
mae_evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="label", metricName="mae") mae = mae_evaluator.evaluate(regression_preds_df)
全体的な説明
The correct code block to compute the Mean Absolute Error (MAE) for the regression model is:
mae_evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="label", metricName="mae")
mae = mae_evaluator.evaluate(regression_preds_df)
Explanation:
Option A (RegressionEvaluator with metricName="mae") correctly specifies the RegressionEvaluator for the regression task and uses the Mean Absolute Error (MAE) as the evaluation metric.
Option B (MulticlassClassificationEvaluator with metricName="mae") is incorrect. MulticlassClassificationEvaluator is not suitable for regression tasks, and it is used for classification tasks.
Option C (BinaryClassificationEvaluator with metricName="mae") is incorrect. BinaryClassificationEvaluator is intended for binary classification tasks, not regression tasks.
Option D (RegressionSummarizer with metricName="mae") is incorrect. RegressionSummarizer is not the correct evaluator for calculating MAE.
Option E (RegressionEvaluator with metricName="mae") is the correct option, equivalent to Option A. It correctly uses the RegressionEvaluator for regression tasks and specifies the Mean Absolute Error (MAE) as the evaluation metric.
Therefore, the recommended code block is Option E.

問題46
未回答
In a data science project, you encounter a classification task where the decision boundaries are complex and nonlinear. Which Spark ML algorithm is suitable for capturing such intricate patterns?
Linear Regression
正解
Decision Trees
Naive Bayes
Support Vector Machines
全体的な説明
Correct Answer:
✅ Decision Trees
Explanation:
For classification tasks with complex, nonlinear decision boundaries, Decision Trees in Spark ML are a strong choice because:
They handle nonlinear relationships well by recursively splitting the feature space.
They can model nonlinear decision boundaries using hierarchical splits.
They are efficient and scalable in Spark ML for large datasets.
Example: Training a Decision Tree Classifier in Spark ML
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.sql import SparkSession
# Initialize Spark
spark = SparkSession.builder.appName("DecisionTreeExample").getOrCreate()
# Sample training data (features, label)
data = [(1, [0.1, 0.3], 0), (2, [0.4, 0.6], 1), (3, [0.8, 0.9], 1)]
df = spark.createDataFrame(data, ["id", "features", "label"])
# Decision Tree Model
dt = DecisionTreeClassifier(featuresCol="features", labelCol="label")
# Train model
model = dt.fit(df)
# Make predictions
predictions = model.transform(df)
predictions.show()
Why Other Options Are Incorrect?
Linear Regression
Incorrect, because Linear Regression is for regression tasks, not classification.
Linear models struggle with complex nonlinear boundaries.
Naive Bayes
Incorrect, because Naive Bayes assumes feature independence, making it unsuitable for complex decision boundaries.
Works best when features are conditionally independent, such as in text classification.
Support Vector Machines (SVM)
Incorrect, because Spark ML does not natively support SVM for classification.
While SVM can handle nonlinear boundaries using kernel tricks, Spark ML primarily focuses on tree-based models for nonlinear classification.
Final Answer:
✅ Decision Trees are the best choice for capturing complex, nonlinear decision boundaries in Spark ML classification tasks.
ドメイン
Spark ML

問題47
未回答
Your team is working on optimizing Spark performance for a large-scale machine learning project. What technique involves storing intermediate data in memory to speed up iterative algorithms and reduce disk I/O?
Data Shuffling
正解
In-Memory Computation
Disk Caching
Data Replication
全体的な説明
Correct Answer:
✅ In-Memory Computation
Explanation:
In-Memory Computation is the key technique for optimizing Spark performance by storing intermediate data in memory instead of recomputing or fetching from disk repeatedly.
Why is this important?
Many machine learning algorithms (e.g., Gradient Descent, KMeans, Decision Trees) require multiple passes over the data.
Without in-memory computation, Spark would need to recompute transformations from the original dataset or read from disk, slowing down performance.
By persisting data in memory, Spark significantly reduces disk I/O and recomputation overhead.
Example: Using cache() and persist() for In-Memory Computation
from pyspark.sql import SparkSession
# Initialize Spark session
spark = SparkSession.builder.appName("InMemoryComputation").getOrCreate()
# Load DataFrame
df = spark.read.csv("large_dataset.csv", header=True, inferSchema=True)
# Cache DataFrame in memory
df.cache()  # Stores the dataset in memory for faster access
# Perform multiple iterative computations
df.groupBy("category").count().show()
df.groupBy("category").avg("value").show()
cache() stores the dataset in memory, speeding up subsequent operations.
Alternatively, persist(StorageLevel.MEMORY_AND_DISK) can be used for large datasets.
Why Other Options Are Incorrect?
Data Shuffling
Incorrect, because shuffling is a costly operation where Spark redistributes data across partitions.
It is used in joins, aggregations, and repartitioning, but does not store intermediate data in memory.
Disk Caching
Incorrect, because disk caching stores data on disk, not in memory.
This is slower than in-memory computation since it involves disk read/write operations.
Data Replication
Incorrect, because replication is used in distributed storage systems (e.g., HDFS, Delta Lake) to prevent data loss, not for improving iterative computation speed.
Final Answer:
✅ In-Memory Computation is the best technique to speed up Spark ML performance by reducing disk I/O and avoiding recomputation.
ドメイン
Scaling ML Models

問題48
未回答
A data scientist has developed a two-class decision tree classifier using Spark ML and computed the predictions in a Spark DataFrame preds_df with the following schema:

prediction DOUBLE
actual DOUBLE
Which of the following code blocks can be used to compute the accuracy of the model according to the data in preds_df and assign it to the accuracy variable?
accuracy = RegressionEvaluator( predictionCol="prediction", labelCol="actual", metricName="accuracy" )
accuracy = MulticlassClassificationEvaluator( predictionCol="prediction", labelCol="actual", metricName="accuracy" ) accuracy = classification_evaluator.evaluate(preds_df)
classification_evaluator = BinaryClassificationEvaluator( predictionCol="prediction", labelCol="actual", metricName="accuracy")
accuracy = Summarizer( predictionCol="prediction", labelCol="actual", metricName="accuracy" )
正解
classification_evaluator = BinaryClassificationEvaluator( predictionCol="prediction", labelCol="actual", metricName="accuracy" ) accuracy = classification_evaluator.evaluate(preds_df)
全体的な説明
I apologize for any confusion in my previous responses. Let me reevaluate the options:
The correct code block to compute the accuracy of the two-class decision tree classifier according to the data in preds_df and assign it to the accuracy variable is:
E.
classification_evaluator = BinaryClassificationEvaluator(
    rawPredictionCol="prediction",
    labelCol="actual",
    metricName="accuracy"
)
accuracy = classification_evaluator.evaluate(preds_df)
Explanation:
BinaryClassificationEvaluator is suitable for binary classification problems.
The correct parameter for representing the raw prediction column is rawPredictionCol, not predictionCol. In binary classification, raw predictions are often used to calculate metrics.
labelCol specifies the name of the column containing the true labels or actual values.
metricName is set to "accuracy" to calculate accuracy as the evaluation metric.
ドメイン
Databricks ML

問題49
未回答
You are setting up a Databricks cluster for a machine learning project that requires substantial computational power. Which cluster type would be most suitable for this scenario?
Single-node cluster
Standard cluster
正解
Multi-node cluster
Task-specific cluster
全体的な説明
Correct Answer:
✅ Multi-node cluster
Explanation:
For a machine learning project requiring substantial computational power, a multi-node cluster is the most suitable option because it:
Distributes workloads across multiple nodes, enabling parallel processing for large datasets.
Utilizes multiple executors, improving performance for ML model training and inference.
Supports distributed ML algorithms, such as Spark MLlib, XGBoost, and Hyperopt with SparkTrials.
Can be autoscaled to adjust resources dynamically based on demand.
Example: Configuring a Multi-Node Cluster in Databricks
Navigate to: Databricks Clusters → Create Cluster
Select: "Multi-node" cluster
Set Worker Type: Choose GPU or CPU nodes depending on ML workload
Enable Autoscaling: Allows Databricks to dynamically scale based on demand
Select Runtime: Use Databricks Runtime for ML for optimized performance
Attach Libraries: Install MLflow, Hyperopt, and other ML packages
Why Other Options Are Incorrect?
Single-node cluster
Incorrect, because single-node clusters are only suitable for small datasets and prototyping.
They do not scale well for computationally heavy ML tasks.
Standard cluster
Partially correct, but not specific enough.
A multi-node cluster is a type of standard cluster, but explicitly choosing multi-node ensures scalability.
Task-specific cluster
Incorrect, because task-specific clusters are optimized for Databricks Jobs, not general ML workloads.
These clusters are meant for short-lived execution tasks, not continuous ML training workloads.
Final Answer:
✅ A Multi-node cluster is the best choice for a high-computation ML project in Databricks.
ドメイン
Cluster Creation and Management

問題50
未回答
What is the purpose of the feature_store_lookups parameter in AutoML?
To control the duration of an AutoML run.
To list algorithm frameworks that AutoML should not consider.
To specify the path to the directory in the workspace.
正解
To represent features from Feature Store for data augmentation.
全体的な説明
Correct Answer:
✅ To represent features from Feature Store for data augmentation.
Explanation:
The feature_store_lookups parameter in Databricks AutoML is used to augment datasets with additional features stored in the Databricks Feature Store.
Why is this useful?
In machine learning, using enriched feature sets often improves model performance.
Instead of manually joining raw datasets with feature tables, AutoML automatically retrieves relevant features from the Feature Store using feature_store_lookups.
Ensures that feature engineering steps are consistent and reproducible across ML workflows.
Example: Using feature_store_lookups in Databricks AutoML
import databricks.automl
from databricks.feature_store import FeatureLookup
# Define feature lookups from Databricks Feature Store
feature_store_lookups = [
    FeatureLookup(
        table_name="customer_features",
        lookup_key="customer_id",
        feature_names=["age", "income", "customer_segment"]
    )
]
# Run AutoML with Feature Store augmentation
databricks.automl.classify(
    dataset=df,
    target_col="churn_label",
    feature_store_lookups=feature_store_lookups
)
feature_store_lookups ensures that additional customer features (age, income, customer_segment) are automatically joined with df before training.
This enables better model accuracy without manual feature engineering.
Why Other Options Are Incorrect?
"To control the duration of an AutoML run."
Incorrect, because timeout_minutes is used for controlling execution time, not feature_store_lookups.
"To list algorithm frameworks that AutoML should not consider."
Incorrect, because excluded_algorithms controls which models (e.g., xgboost, random_forest) are skipped, not feature_store_lookups.
"To specify the path to the directory in the workspace."
Incorrect, because workspace paths are set using the experiment_dir parameter, not feature_store_lookups.
Final Answer:
✅ feature_store_lookups is used in Databricks AutoML to represent features from the Feature Store for data augmentation.
ドメイン
AutoML

問題51
未回答
You are working with a time series forecasting task, and you want to identify the column(s) that identify the time series for multi-series forecasting. Which parameter should you use?
正解
identity_col
output_database
time_col
frequency
全体的な説明
Correct Answer:
✅ identity_col
Explanation:
In Databricks AutoML for time series forecasting, the identity_col parameter is used to identify the column(s) that uniquely distinguish different time series in a multi-series forecasting problem.
Why is this needed?
In multi-series forecasting, a dataset may contain multiple independent time series (e.g., sales data for different stores, sensor data from multiple devices).
The identity_col ensures that AutoML treats each time series separately while training the model.
Without identity_col, AutoML assumes a single time series in the dataset.
Example: Using identity_col in Databricks AutoML
import databricks.automl
databricks.automl.forecast(
    dataset=df,
    target_col="sales",   # Column to predict
    time_col="date",      # Timestamp column
    identity_col="store_id"  # Identifies unique time series (e.g., multiple stores)
)
time_col="date" → Identifies the time index.
identity_col="store_id" → Each store has its own time series in the dataset.
Why Other Options Are Incorrect?
output_database
Incorrect, because output_database is not a valid AutoML parameter.
If needed, you can specify an output directory using experiment_dir.
time_col
Incorrect, because time_col specifies the timestamp column for forecasting but does not differentiate multiple time series.
Correct usage:
time_col="timestamp"
However, for multi-series forecasting, identity_col is required.
frequency
Incorrect, because frequency defines the granularity of the time series (e.g., daily, hourly) but does not identify separate time series.
Example usage of frequency:
frequency="D"  # Daily frequency
This parameter helps handle missing timestamps, but it does not serve the same purpose as identity_col.
Final Answer:
✅ Use identity_col to identify distinct time series in multi-series forecasting tasks.
ドメイン
AutoML

問題52
未回答
A data scientist is using 3-fold cross-validation and a specific hyperparameter grid for optimizing model hyperparameters via grid search in a classification problem.
The hyperparameter grid is as follows:
Hyperparameter 1 [4, 6, 7]
Hyperparameter 2 [5, 10]
What is the total number of machine learning models that can be trained simultaneously during this process?
Choose only ONE best answer.
2
6
12
正解
18
24
全体的な説明
The total number of machine learning models trained during a 3-fold cross-validation with the given hyperparameter grid is calculated by multiplying the number of hyperparameter combinations by the number of folds.
Hyperparameter combinations:
Hyperparameter 1 has 3 values: [4, 6, 7].
Hyperparameter 2 has 2 values: [5, 10].
Total combinations = 3 × 2 = 6.
Cross-validation folds:
For each hyperparameter combination, 3 models are trained (one per fold).
Total models = 6 combinations × 3 folds = 18.
The term "simultaneously" in the question refers to the total models trained across all folds and hyperparameters, not parallel execution. Thus, the correct answer is 18.
ドメイン
ML workflows

問題53
未回答
What does efficient cluster configuration in Databricks Runtime for Machine Learning enable?
Quick data preprocessing
正解
Optimal resource utilization
Streamlined model deployment
Real-time data visualization
全体的な説明
Correct Answer:
✅ Optimal resource utilization
Explanation:
An efficient cluster configuration in Databricks Runtime for Machine Learning (ML) ensures optimal resource utilization by:
Dynamically allocating compute resources based on workload needs (autoscaling).
Distributing ML workloads efficiently across multiple nodes, improving performance for large datasets.
Optimizing memory and parallel execution, reducing costs while maintaining high performance.
Providing pre-configured ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn) that are optimized for distributed execution.
By optimizing resource allocation, Databricks ML Runtime helps data scientists run ML workflows efficiently without excessive resource consumption.
Why Other Options Are Incorrect?
Quick data preprocessing
Incorrect, because while Databricks can speed up preprocessing, it depends more on data pipeline optimizations (Delta Lake, Spark optimizations) rather than just cluster configuration.
Streamlined model deployment
Incorrect, because model deployment is handled by MLflow Model Serving or MLOps pipelines, not cluster configuration.
Real-time data visualization
Incorrect, because visualization tools (e.g., Databricks notebooks, Power BI, or Tableau) do not rely on cluster configuration for real-time rendering.
Final Answer:
✅ Efficient cluster configuration in Databricks ML Runtime enables optimal resource utilization, improving ML workload performance while minimizing costs.
ドメイン
Databricks ML

問題54
未回答
A data scientist is using MLflow to manage machine learning experiments and versions. They want to update the metadata of an existing model version, such as changing its description or adding tags.
Which MLflow operation should they use?
mlflow.update_model_metadata
mlflow.register_model
正解
mlflow.update_model_version
mlflow.edit_model_version
全体的な説明
Correct Answer:
✅ mlflow.update_model_version
Explanation:
In MLflow Model Registry, the update_model_version() method is used to update metadata of an existing model version, such as:
Changing its description
Adding or modifying tags
Updating other metadata fields
This allows data scientists to manage model documentation and tracking efficiently without re-registering a new version.
Example: Updating a Model Version’s Metadata in MLflow
import mlflow
from mlflow.tracking import MlflowClient
# Initialize MLflow Client
client = MlflowClient()
# Define model details
model_name = "my_model"
model_version = 2  # The model version to update
# Update model description
client.update_model_version(
    name=model_name,
    version=model_version,
    description="Updated model description: Improved accuracy and retrained on new dataset."
)
# Add a new tag to the model version
client.set_model_version_tag(
    name=model_name,
    version=model_version,
    key="dataset_version",
    value="v2.1"
)
print(f"Updated metadata for {model_name} version {model_version}")
update_model_version() updates the model version's description.
set_model_version_tag() adds tags for better tracking.
Why Other Options Are Incorrect?
mlflow.update_model_metadata
Incorrect, because no such function exists in MLflow.
Metadata updates are done using update_model_version() instead.
mlflow.register_model
Incorrect, because registering a model creates a new model version, rather than updating an existing one.
mlflow.edit_model_version
Incorrect, because this function does not exist in MLflow.
The correct function is update_model_version().
Final Answer:
✅ Use mlflow.update_model_version() to modify metadata, descriptions, and tags for an existing MLflow model version.

問題55
未回答
During a classification AutoML run, you notice that the positive class needs to be explicitly defined for calculating metrics like precision and recall. Which parameter should you use for this purpose?

primary_metric
正解
pos_label
max_trials
time_col
全体的な説明
Correct Answer:
✅ pos_label
Explanation:
In Databricks AutoML for classification tasks, the pos_label parameter is used to explicitly define the positive class when calculating metrics like Precision, Recall, and F1-score.
Why is this needed?
By default, AutoML assumes the higher value is the positive class (e.g., 1 in a binary classification task {0,1}).
If the dataset has imbalanced labels or requires a different positive class, you must manually specify the pos_label to ensure correct metric computation.
Example: Setting pos_label in AutoML Classification
import databricks.automl
databricks.automl.classify(
    dataset=df,
    target_col="churn",
    pos_label=1,  # Explicitly setting '1' as the positive class
    primary_metric="f1"
)
Here, pos_label=1 ensures that Precision, Recall, and F1-score calculations treat 1 as the positive class.
This is important for imbalanced datasets, where choosing the correct positive class affects performance evaluation.
Why Other Options Are Incorrect?
primary_metric
Incorrect, because primary_metric selects the main evaluation metric (e.g., accuracy, f1, roc_auc), but it does not define the positive class.
max_trials
Incorrect, because max_trials controls the number of AutoML experiments (e.g., how many models to train), but does not affect classification label definitions.
time_col
Incorrect, because time_col is used only for time-series forecasting, not for classification.
Final Answer:
✅ Use pos_label to explicitly define the positive class in a Databricks AutoML classification run.
ドメイン
AutoML

問題56
未回答
In a distributed computing environment, what is the primary purpose of data partitioning?
Reducing Storage Costs
Enhancing Data Security
正解
Distributing Data Across Nodes
Enabling Compression
全体的な説明
Correct Answer:
✅ Distributing Data Across Nodes
Explanation:
In a distributed computing environment, data partitioning is primarily used to distribute data across multiple nodes to enable parallel processing, improve query performance, and balance compute workload efficiently.
Partitioning splits large datasets into smaller chunks (partitions) so that multiple nodes in a cluster can process them in parallel.
This approach is essential in big data frameworks like Apache Spark, Databricks, and Hadoop, where datasets are too large to fit on a single machine.
Example: Partitioning a DataFrame in PySpark
df = spark.read.csv("large_dataset.csv", header=True, inferSchema=True)
# Repartition the DataFrame into 10 partitions for parallel processing
df = df.repartition(10)
# Check the number of partitions
print(df.rdd.getNumPartitions())
Before processing, data is distributed across 10 partitions, enabling parallel execution on multiple nodes.
This is crucial for Spark transformations like map(), filter(), and join() to work efficiently.
Why Other Options Are Incorrect?
Reducing Storage Costs
Incorrect, because partitioning does not directly reduce storage costs.
Compression and optimized file formats (Parquet, ORC) help with storage, but partitioning is meant for distributing workloads.
Enhancing Data Security
Incorrect, because partitioning is for parallel processing, not security.
Security in distributed environments is managed through encryption, access control, and authentication policies.
Enabling Compression
Incorrect, because compression reduces file size but does not distribute data across nodes.
File formats like Parquet, ORC, and Delta Lake provide built-in compression, but they work independently of partitioning.
Final Answer:
✅ Data partitioning is primarily used to distribute data across nodes in a distributed computing environment, ensuring efficient parallel processing.
ドメイン
Scaling ML Models

問題57
未回答
A team is formulating guidelines on when to apply various metrics for evaluating classification models. They need to decide under what circumstances the F1 score should be favored over accuracy. The F1 score formula is given as follows:
F1 = 2 * (precision * recall) / (precision + recall)
What recommendations should the team incorporate into their guidelines?

The F1 score is more suitable than accuracy when the target variable has more than two categories.

The F1 score is recommended over accuracy when the number of actual positive instances is equal to the number of actual negative instances.
正解
The F1 score should be favored over accuracy when there is a substantial imbalance between the positive and negative classes and minimizing false negatives is important.
The F1 score is recommended over accuracy when the target variable comprises precisely two classes.
The F1 score is preferable over accuracy when correctly identifying true positives and true negatives is equally critical to the business problem.
全体的な説明
Correct Answer:
The F1 score should be favored over accuracy when there is a substantial imbalance between the positive and negative classes and minimizing false negatives is important.
Explanation:
The F1 score is the harmonic mean of precision and recall, making it particularly useful in scenarios where:
Class imbalance exists: Accuracy can be misleading in imbalanced datasets (e.g., 95% negative class, 5% positive class), as a model predicting the majority class will appear highly accurate but fail to capture the minority class. The F1 score balances precision and recall, providing a more robust metric.
False negatives are costly: In cases like fraud detection or disease diagnosis, missing a positive instance (false negative) has severe consequences. The F1 score prioritizes minimizing false negatives by emphasizing recall.
Formula Recap:

Precision: Measures how many predicted positives are actual positives (reduces false positives).
Recall: Measures how many actual positives are correctly predicted (reduces false negatives).
Why Other Options Are Incorrect:
"The F1 score is more suitable than accuracy when the target variable has more than two categories":
The F1 score is primarily for binary classification. For multiclass problems, use metrics like F1-micro/macro or weighted F1.
"The F1 score is recommended when the number of positive and negative instances is equal":
With balanced classes, accuracy can suffice. The F1 score shines when imbalance exists.
"The F1 score is recommended when the target variable has precisely two classes":
While true, this alone doesn’t justify favoring F1 over accuracy. The key is imbalance or costly false negatives.
"The F1 score is preferable when true positives and true negatives are equally critical":
Accuracy already balances true positives/negatives. F1 is better when false negatives/positives are asymmetric in importance.
Key Takeaway:
Prioritize the F1 score when:
Class imbalance is severe (e.g., fraud, rare diseases).
False negatives are costly (e.g., missing a cancer diagnosis).
For balanced datasets or when all errors are equally important, accuracy may suffice.
ドメイン
Spark ML

問題58
未回答
A data engineering team is developing ETL pipelines on a shared Databricks cluster. They need to use a third-party Python library, etl_utils, in their notebooks.
What is the recommended way to make this library available to all team members?

Edit the cluster to use the Databricks Runtime for Data Engineering.
Set the PYTHONPATH variable in the cluster configuration to include the path to etl_utils.
Run %pip install etl_utils once on any notebook attached to the cluster.
正解
Use the dbutils.library.installPyPI("etl_utils") command in the cluster’s initialization script.
There is no way to make the etl_utils library available on a cluster.
全体的な説明
Correct Answer:
Use the dbutils.library.installPyPI("etl_utils") command in the cluster’s initialization script.
Explanation:
To make a third-party Python library (e.g., etl_utils) available to all users on a shared Databricks cluster, the recommended approach is to:
Add the library installation command to the cluster’s init script:

#!/bin/bash
/databricks/python/bin/pip install etl_utils
or use dbutils.library.installPyPI("etl_utils") in a notebook and save it as an init script.
Why this works:
The init script runs every time the cluster starts, ensuring the library is installed globally.
All users attached to the cluster will have access to etl_utils without manual installation.
Why Other Options Are Incorrect:
Edit the cluster to use Databricks Runtime for Data Engineering:
This only changes the runtime environment but doesn’t install custom libraries.
Set PYTHONPATH in cluster configuration:
PYTHONPATH is for module search paths, not installing packages. It won’t install etl_utils.
Run %pip install etl_utils in a notebook:
This installs the library only for the current session. Other users won’t see it unless they rerun the command.
"No way to make it available":
Incorrect. Init scripts or cluster libraries (via UI) can globally install packages.
Key Takeaway:
For team-wide library availability, use:
Cluster init scripts (for automatic installation on startup).
Cluster UI Libraries tab (manual upload/install).
Example init script:
dbutils.library.installPyPI("etl_utils")
dbutils.library.restartPython()  # Optional: Restart Python to apply changes
ドメイン
Databricks ML

問題59
未回答
In which scenario using a single Train-Test Split is better than Cross-Validation?
When the goal is to maximize model performance
When the goal is to ensure model stability and generalization
正解
When computation time and resources are limited
When the dataset is imbalanced
全体的な説明
Correct Answer:
When computation time and resources are limited
Explanation:
A single Train-Test Split is preferable over Cross-Validation in scenarios where computation time and resources are constrained. Here’s why:
Speed and Efficiency:
Train-Test Split: Evaluates the model once (1 train/test cycle), making it faster and less resource-intensive.
Cross-Validation (e.g., k-fold): Requires training the model k times (once per fold), which is computationally expensive for large datasets or complex models.
Use Cases:
Exploratory Analysis: Quick validation of model feasibility.
Large Datasets: When data is abundant, a single split often provides a reliable estimate of performance.
Resource Constraints: Limited hardware (e.g., small clusters) or tight deadlines.
Trade-off:
Pros: Faster, cheaper.
Cons: Less reliable performance estimate (higher variance) compared to cross-validation.
Why Other Options Are Incorrect:
"When the goal is to maximize model performance":
Cross-validation is better for maximizing performance because it uses more data for training and provides robust performance estimates.
"When the goal is to ensure model stability and generalization":
Cross-validation is superior for assessing generalization (via multiple folds) and reducing overfitting.
"When the dataset is imbalanced":
Cross-validation with stratification (e.g., StratifiedKFold) is preferred for imbalanced datasets to maintain class distribution in each fold.
Key Takeaway:
Use a single Train-Test Split when:
Speed/resources are critical.
Data is large enough for a representative split.
Use Cross-Validation when:
Accurate performance estimation is needed.
Data is limited or imbalanced.
Example:
from sklearn.model_selection import train_test_split
# Single split (fast)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
ドメイン
ML workflows

問題60
未回答
How can you create a new catalog in Unity Catalog?
By running spark.sql("CREATE CATALOG IF NOT EXISTS ml").
正解
By running spark.sql("CREATE CATALOG ml").
By running spark.sql("USE CATALOG ml").
By running spark.sql("CREATE SCHEMA ml").
全体的な説明
Correct Answer:
By running spark.sql("CREATE CATALOG ml")
Explanation:
To create a new catalog in Unity Catalog, you must use the CREATE CATALOG SQL command. This command initializes a new top-level catalog for organizing your data assets.
Correct Code:
spark.sql("CREATE CATALOG ml")
Why Other Options Are Incorrect:
CREATE CATALOG IF NOT EXISTS ml
While this syntax is valid, the question asks for the basic creation command. The IF NOT EXISTS clause is optional and used to avoid errors if the catalog already exists.
USE CATALOG ml
This command switches to an existing catalog but does not create one.
CREATE SCHEMA ml
Schemas (databases) are created inside a catalog, not at the catalog level. This command would fail if no catalog is selected.
Key Notes:
Catalogs are the top-level namespace in Unity Catalog (e.g., ml).
Schemas (databases) reside inside catalogs (e.g., ml.default).
Requires admin privileges or CREATE CATALOG permissions.
Example Workflow:
# Create a catalog
spark.sql("CREATE CATALOG ml")
# Create a schema inside the catalog
spark.sql("CREATE SCHEMA ml.default")
# Verify
spark.sql("SHOW CATALOGS").show()
ドメイン
Feature Store

問題61
未回答
A machine learning model has been trained and evaluated successfully using Databricks MLlib. The data scientist now needs to deploy the model for real-time predictions in a production environment.
What steps should they take?
Export the model as a serialized file and deploy it on a separate server.
正解
Use MLflow to package the model and deploy it as a REST API endpoint.
Save the model to a Delta table and query it for predictions.
Schedule a Databricks Job to run the model periodically.
全体的な説明
Correct Answer:
Use MLflow to package the model and deploy it as a REST API endpoint.
Explanation:
The most robust and scalable approach for deploying a Databricks MLlib model in production is to use MLflow to:
Package the Model:
Log the trained model using mlflow.spark.log_model().
MLflow automatically captures dependencies (e.g., PySpark, Python version).
Deploy as a REST API:
Use MLflow Model Serving or deploy to Azure ML/AWS SageMaker for real-time inference.
Example:
import mlflow
from pyspark.ml import PipelineModel
# Log the model
model = PipelineModel.load("path/to/model")
mlflow.spark.log_model(model, "spark-model")
# Deploy (e.g., via Databricks Model Serving UI)
This provides a scalable, low-latency endpoint for predictions.
Why Other Options Are Incorrect:
Export as a serialized file and deploy on a separate server:
Manual deployment is error-prone (dependency mismatches, scaling issues).
Lacks the monitoring and scalability of MLflow.
Save to a Delta table and query for predictions:
Delta tables are for data storage, not model serving.
Real-time predictions require low-latency APIs, not batch queries.
Schedule a Databricks Job for periodic runs:
Jobs are for batch processing, not real-time inference.
Key Benefits of MLflow Deployment:
✅ Dependency Management: Auto-captures Python/Spark versions.
✅ Scalability: Built-in load balancing (e.g., via Databricks Model Serving).
✅ Monitoring: Track latency, errors, and usage metrics.
Example Workflow:
Train → Log model with MLflow.
Deploy → Enable "Model Serving" in Databricks UI or export to cloud platforms.
Call API:
import requests
response = requests.post("https://<model-endpoint>/invocations", json=input_data)
ドメイン
Databricks ML

問題62
未回答
Your machine learning project involves feature selection to improve model efficiency. Which technique, supported by Databricks MLlib, can help you identify the most important features from a dataset?
Principal Component Analysis
Recursive Feature Elimination
正解
Feature Importance Ranking
Feature Scaling
全体的な説明
Correct Answer:
Feature Importance Ranking
Explanation:
Feature Importance Ranking in Databricks MLlib is a technique that helps identify the most influential features in a dataset by quantifying their contribution to model predictions. It is supported by tree-based models (e.g., Random Forest, Gradient-Boosted Trees) in MLlib, which provide a built-in .featureImportances attribute to rank features.
How It Works:
Train a tree-based model (e.g., RandomForestClassifier).
Extract feature importances:
from pyspark.ml.classification import RandomForestClassifier
# Train model
rf = RandomForestClassifier(featuresCol="features", labelCol="label")
model = rf.fit(train_data)
# Get feature importances
importances = model.featureImportances
print("Feature Rankings:", importances)
Select top features based on scores (e.g., highest to lowest).
Why It’s Preferred for Feature Selection:
✅ Model-Agnostic Insights: Directly tied to model performance.
✅ Scalable: Works with distributed Spark DataFrames.
✅ Interpretable: Scores indicate relative importance (e.g., 0.8 = high impact, 0.1 = low impact).
Why Other Options Are Incorrect:
Principal Component Analysis (PCA):
Reduces dimensionality but creates new synthetic features (losing interpretability of original features).
Use case: Feature extraction, not selection.
Recursive Feature Elimination (RFE):
Not natively supported in MLlib (requires custom implementation).
Computationally expensive for large datasets.
Feature Scaling:
Normalizes feature ranges (e.g., StandardScaler) but does not rank or select features.
Key Takeaway:
For feature selection in MLlib, use Feature Importance Ranking from tree-based models to:
Identify top predictors.
Reduce overfitting by removing low-impact features.
Improve model efficiency without losing interpretability.
Example Output:
Feature Rankings: (10,[0,2,5],[0.8,0.15,0.05])
# Feature 0 is most important (score=0.8), Feature 5 is least (score=0.05).
ドメイン
Data Preparation

問題63
未回答
A senior machine learning engineer is implementing model serving using MLflow in a production environment. They want to deploy the model as a Docker container and expose it as a REST API.
What MLflow command or functionality can help in creating a Docker image for the model?
mlflow.build_docker_image
mlflow.dockerize_model
mlflow.create_container
mlflow.serve_model
正解
mlflow.models.build_docker

全体的な説明
Correct Answer:
mlflow.models.build_docker
Explanation:
To deploy an MLflow model as a Docker container with a REST API, use the mlflow.models.build_docker function. This command:
Packages the model and its dependencies into a Docker image.
Exposes a REST API endpoint for real-time predictions.
Steps to Deploy:
Log the model during training:
import mlflow
mlflow.sklearn.log_model(model, "model")
Build the Docker image:
mlflow.models.build_docker(
    model_uri="runs:/<RUN_ID>/model",
    name="mlflow-docker-model",
    env_manager="conda"
)
model_uri: Path to the logged model (e.g., runs:/12345/model).
name: Name of the Docker image.
env_manager: Dependency manager (conda or virtualenv).
Run the container:

docker run -p 5000:8080 mlflow-docker-model
The model will be accessible at http://localhost:5000/invocations.
Why Other Options Are Incorrect:
mlflow.build_docker_image:
Incorrect function name (does not exist). The correct command is mlflow.models.build_docker.
mlflow.dockerize_model:
Not a valid MLflow function.
mlflow.create_container:
Does not exist in MLflow.
mlflow.serve_model:
Launches a local REST server but does not create a Docker image.
Key Benefits of MLflow Docker Deployment:
✅ Consistency: Encapsulates model + environment.
✅ Scalability: Deploy to Kubernetes, AWS ECS, etc.
✅ Low-Latency: Optimized for real-time inference.
Example API Request:
import requests
data = {"inputs": [[1.2, 3.4]]}
response = requests.post("http://localhost:5000/invocations", json=data)
print(response.json())


問題64
未回答
Which of the following Python data types is matched to PySpark's DecimalType(38, 18) when converted in the pandas API on Spark?
float
int
bytes
正解
decimal.Decimal
全体的な説明
Correct Answer:
decimal.Decimal
Explanation:
When using the pandas API on Spark (Koalas), PySpark’s DecimalType(38, 18) (a high-precision decimal type) is mapped to Python’s decimal.Decimal to preserve precision. This avoids floating-point rounding errors common with float.
Why It Matters:
DecimalType(38, 18):
Stores numbers with 38 total digits, 18 of which are after the decimal point.
Used for financial/scientific data where precision is critical.
decimal.Decimal:
Python’s arbitrary-precision decimal type.
Ensures exact decimal representation (unlike float, which is binary floating-point).
Example:
from pyspark.sql import SparkSession
import decimal
# Create a Spark DataFrame with DecimalType
spark = SparkSession.builder.getOrCreate()
df_spark = spark.createDataFrame(
    [(decimal.Decimal("12345678901234567890.123456789012345678"),)],
    schema="value DECIMAL(38, 18)"
)
# Convert to pandas-on-Spark DataFrame
df_koalas = df_spark.to_koalas()
# Verify the Python type
print(type(df_koalas["value"].iloc[0]))  # Output: <class 'decimal.Decimal'>

Why Other Options Are Incorrect:
float:
Loses precision for DecimalType values (e.g., 12345678901234567890.123456789012345678 → rounded).
PySpark uses float for FloatType/DoubleType, not DecimalType.
int:
Cannot represent decimal places.
Used for PySpark’s IntegerType/LongType.
bytes:
Unrelated to numeric types.
PySpark’s BinaryType maps to bytes.
Key Takeaway:
For exact decimal precision in pandas API on Spark:
Use decimal.Decimal for PySpark’s DecimalType.
Avoid float for financial/scientific data to prevent rounding errors.
ドメイン
Pandas API on Spark

問題65
未回答
Your team has trained a machine learning model using Spark ML. When interpreting the model results, what is vital for making informed business decisions?
Training time
Model complexity
正解
Interpreting output and parameters
Number of features
全体的な説明
Correct Answer:
Interpreting output and parameters
Explanation:
To make informed business decisions, the most critical aspect is interpreting the model's output and parameters. This involves:
Understanding Predictions:
What do the model's predictions mean for the business?
Example: A fraud detection model's "probability score" translates to risk levels.
Analyzing Parameters/Features:
Which features drive predictions? (e.g., featureImportances in tree-based models).
Example: In a customer churn model, "usage frequency" might be a top predictor.
Actionable Insights:
Translate model results into business actions (e.g., "Target customers with low engagement to reduce churn").
Why Other Options Are Less Critical:
Training Time:
Relevant for engineering efficiency but doesn’t directly impact business decisions.
Model Complexity:
Affects maintainability but doesn’t explain why a prediction was made.
Number of Features:
Important for performance, but interpretation matters more for decision-making.
Key Steps for Interpretation in Spark ML:
✅ Extract Feature Importance:
model = pipeline.fit(train_data)
importances = model.stages[-1].featureImportances  # For tree-based models
✅ Explain Predictions:
Use SHAP values or MLflow’s model explainability tools.
✅ Validate with Business Context:
Ensure features align with domain knowledge (e.g., "Is high ‘credit utilization’ really predictive of default?").
Example Business Impact:
A bank uses a loan approval model’s featureImportances to discover that "debt-to-income ratio" is the top factor. They adjust their lending policy accordingly.
Key Takeaway:
Interpretation > Metrics for business decisions. Focus on:
What the model predicts.
Why it makes those predictions.
How to act on them.
ドメイン
Spark ML Basics
